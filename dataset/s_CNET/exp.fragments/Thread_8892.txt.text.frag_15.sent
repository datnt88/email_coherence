i don't have the link anymore and when i first saw it i didn't believe it but dvi-i (as all signals) has a standard that it has to meet to be &quot;true dvi-i&quot; otherwise it's just like using analog, on a wider, uglier, port and yes, you would need an oscilloscope to test it out and the image quality difference is almost negligle (if there was a huge difference, dvi-i would've taken off more) one thing i'd like to note is that the more pixels you set on each monitor, the more you'll load up the gfx card (more ram is better in this scinario, you have to realize, even if it's just cloning, it needs twice the ram to do twice the work for twice the image...) so i'm currently running two crt's at 1152x864, so my video card has to generate 2304x864 or 1,990,656 pixels over the 995,328 pixels of 1152x864 while most video cards don't have an issue drawing more pixels having more ram/higher fillrate does help out (if you ran two 1280x1024 displays i'm guessing it'd be slowed down) 
