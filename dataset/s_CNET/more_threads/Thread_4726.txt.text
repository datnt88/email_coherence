i'm getting ready to buy a new video card. in the specs it states the resolutions and the refresh rates ( i.e. 800x600 70hz - 1028x720 75hz etc. )for the card. while looking at my monitors manual i also noticed that it included these same numbers (resolutions-refresh rates)and that the refresh rates for the same settings were different. my question is - which of the two( video card or monitor)determines the refresh rate you see on the screen ? or how does this work ?
it's very simple. whichever is slower and smallest is what you get to use of the two devices. example. at 800x600, your card says it will do 100 hz refresh. your monitor says 75 hz. you'll use 75 or less hz. bob
does the operating system have any effect on this ? because i was using win98 and the refresh rate was 56 hz. i swapped out the hard drive to one with xp on it and the refresh rate jumped up to 75 hz. is this normal ?
os? no. it's all about the drivers. bob
my video card is set to 800x600 and my monitor shows that the refresh rate is 35 khz/ 56 hz. is there any way that i can get a higher refresh rate ? isn't this better for the eyes(less strain )?
just a thought. if i r/c a blank area of the desktop and select properties it brings me to a screen that applies to my vid card. if i go through the tabs on that screen i get to a screen that allows me to select refresh rate. in my case it is set to default which gives me a rate slightly higher than yours. i have tested higher rates and for my use i do not see a diff, perhaps gamers play with this stuff.
