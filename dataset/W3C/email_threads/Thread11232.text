I am strongly supporting SD1 - Short End Tags, primarily with respect to
the useage of XML to exchange databases. In some examples from my desk -
we are exchanging data for engine management system - I was saving 40%
size of the instance by simply shorten the end tags.
Making Perl hacker?s life easier is ok. But if such kind of programs
want meet the requirements of error reporting (WF errors) they still
must track the element names.
There was a proposal to allow Short End Tags in Elements with PCDATA
only. I think this is hard to keep sure.
Therefor, I hereby propose, that Short End Tags are allowed in valid
XML-Documents. In this case, it causes no more effort to read the
document, since there is a validating parser.
On the other hand, the database people need a Database scheme, so that
they will work with valid XML-document primarily.
Regards/Mit freundlichen Gruessen
P.O. Box 30 02 40
D-70442 Stuttgart
Germany
This sounds a bit like "I really want smaller heads on these hammers you're
selling -- I'm using them to turn screws and they're just not very
efficient." Usually if 80% of your files is markup (roughly that, since by
shortening half the tags you say you're saving 40%), there's a bigger
problem going on.
The biggest savings come if you've got a whole lot of little tiny fields.
Little tiny fields seldom have much internal structure; we're usually
talking about integers, dates, social security and phone numbers, etc. In
such cases, you're a whole lot better off sticking them in attributes.
Conceptually, it makes at least as much sense; and attributes are good at
reprenting little chunks of info that each have little internal structure;
and SGML and XML can validate a number of datatypes for attributes (and,
perhaps will add a few), whereas it is not possible even to require that
#PCDATA content *exist*.
To me, the big problem is not that you have to give the GI twice. It's that
you have to give it on every *instance*, which simply doesn't make sense for
RDBs. If you're shipping 100 records with 10 fields each, you don't want to
just get from 2000 GIs to 1000 GIs. You want to get to 10 GIs.
The total costs for different forms, given an n-char GI are:
gi /gi or 5+2n chars per field
gi / or 5+n chars per field
gi/ / or 3+n chars per field
gi=" " or 5+n chars per field
|gi or 1 chars per field.
Maybe I'm an incorrigible CS nerd, but I find O(k) a lot better than O(n),
and O(n) not very much better than O(2n). You can save the 2n vs. n amount
just by shortening GIs, and most modems will compress it away anyway.
Doesn't seem worth it, esp. since we have a known constituency that it will
hurt (the DPH, not the parser writers, of course).
Saving part of the end-tag is just a patch; if we really want to address
this case we should save an awful lot more than that, and it seems to me
that is best done some more radical way if at all.
Steven J. DeRose, Ph.D., Chief Scientist
Inso Electronic Publishing Solutions
(formerly EBT)
The more I look at the arguments, the more convinced I am that
XML is not likely to be a meaningful format for distributing
database-resident bulk data. This shouldn't surprise anyone: it
was never designed to do this. You want to ship bulk data, you
send a Java applet followed by a gzipped datafile, and let the
applet generate XML locally for the display.
///Peter
I do not think this to be the case. You can use XML to mark up
database records for distribution/interchange, in which case,
one would (intuitively) expect that any compression applied would
result in a file little different, because the markup would tend
to form a very regular input pattern (easily compressed).
To test this, I performed another little experiment. I created
a 1024x1024 table of (random, integer, small) numeric data in both
comma delimited, and xml form, and then compressed both files.
The results follow:
Before compression:
foo.txt 11,873,460
foo.xml 17,123,519
Markup adds about 44% to the size of the data.
After compression (using gzip):
foo.txt.gz 5,323,235
foo.xml.gz 5,704,385
markup adds about 7% to the compressed size.
This gives me pause. I now wonder precisely which applications
it will be good for. If all you are doing is sending XML to the
display, HTML works pretty well for that. So what I get from
XML then, is a display format with more complex hyperlinking?
no....
len
Gavin, doesn't this look pretty much like the arguments
in VRML about the binary formats in which it was decided
that modems and gzips did the job about as well with
regards to transmission size. I don't think I buy
the "transmission size will be a hazy memory" argument
because even where complete infrastructure change is
needed (why not dump TCP/IP while at it?), it won't
happen too quick and in all of the places XML should
be able to reach. I think it likely to happen to
the well-heeled in the short term, but that's life.
OTH, reduction of end tagging just isn't an issue with
enough arguments one way or another to convince me
that it requires a lot of time right now. I think it
should be reconsidered in XML 2.0 if there is
sufficient evidence it gives the application vendors
grief (such as, they ignore it and do it anyway).
BTW: On conformance. Anyone looking at that issue
now should look at Mary Brady's site for VRML 2.0
at NIST. Top marks!
len
[shipping bulk data]
No, I don't see that conclusion follows at all. You can build into the
applet whatever smarts you want: all I'm saying is that shipping bulk
data premarked in plain XML (or SGML) is inefficient. Having a
dynamically-generated chunk of JavaScript unwrap and rejig a
dynamically-composed chunk of data in some highly compact (?binary)
format might overcomes the transmission problem. The XML which hits
the browser should be the same as if it has been composed and
transmitted _en clair_.
///Peter
Sure. I think you could define a binary form that would be *smaller*
than anything you can prodice with gzip et al., but that should not
have any effect of the textual representation --- an in particular,
the argument for empty end tags.
Right. Like VRML, even when compressed, at some point, a binary
has become necessary. But an applet? I can see that approach
working in the way a self-contained compress/decompress works.
Umm... the browser? Just what is that? I know what a framework is,
and a framework + plugin = browser. I'm unclear about the dynamically
generated chunk of JavaScript is needed. The XML hitting the
browser is still data to some process, right? What is that process?
What is it's application?
Probably we should take this over to xml-dev. I'm in the weeds
and folks are too busy for that here.
len
