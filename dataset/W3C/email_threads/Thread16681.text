Hi,
FYI - my recent posting to CHI-WEB. It doesn't address the problems
that some people have with text-only/text-rich web sites. The
understand of these needs is still evolving and I was reluctant to bring
it up at CHI-WEB.
Scott
Hi,
I'll try to respond to several messages in this one note.
Someone brought up the issue of "Separate is Unequal". There is no
consistant view of this within the disabled community. Some people have
very strong philosophical points of view on assumption. Other people
are more pragmatic. For example, stairs and ramps are of equal value.
Stairs are more efficient to use by people who can walk while ramps are
more efficient for people who use wheelchairs. One is not necessarily
any better than the other. Nor would it be appropriate to require that
all people use only one, e.g. ramps.
A very, very major concern about multiple versions of a web page is
being sure that all versions have the same updated information. I
believe that this problem could be addressed by using various tools or
technologies and I've been looking at three levels of approaches for
keeping multiple versions of the same web page in sync.
The simplest level is where a standard web page is created but the developer
includes information in the web page which guides an 'extractor'
which develops an alternate version of a web page.
The second level is where the server works with self-configuring
web pages which use conditionals, macros, symbols, etc, for creating
different versions of itself.
The third level is where XML and XSLT is used to create the various
versions of a web page.
The simplest version has the advantage that it can probably be easily
learned by web designers without their needing to be programmers.
The third level takes advantage of the power of XML/XSLT, but its draw
back is that some programming background is almost required. The
self-configuring web pages have some of the simplicity of the first
level, but avoids the complexity of implementing XML/XSLT of the third
level.
Here's a little more detail on the first level of approach.
First, I'm going to be looking at accessibility from the point of:
Can a disabled person use a piece of technology with as much
efficiency, ease of use and accuracy as that experienced by a
non-disabled person using the technology?
rather than:
Can the person with a disability use a piece of technology?
Traditionally, people have looked at accessibility from the second point
of view. I believe the more appropriate way to look at accessibility
is from the first point of view mentioned.
I'm also going to propose that a universal web page be a basic web page
without a lot of frills that can be used by a variety of people. (I'm
not wedded to the name "universal web page", but am open to other
names.)
A universal web page would have a pretty standard liner format like:
TITLE
INTRODUCTION
INDEX OF LINKS TO SECTIONS OF WEB PAGE
SECTION
SECTION
SECTION
Each section of a universal web page would have the same basic format:
Section name
Link to previous section, link to index of sections, link to next section
Section body
In addition, the section body would primarily contain text.
This structure would have several advantages. Little use of frills
means that the universal web page can be used with a variety of browsers.
Also, the bandwidth wouldn't be very high.
The general structure would benefit blind users. The index of
sections at the beginning would present an overview of the web page
which would help understanding the web page. The linear format
would also help with understanding the organization of the web page.
The linear format, the index of links to sections and the ability
to link from section to section provide simple, but fairly complete
navigation of the web page.
The idea of the 'extractor' is that it is a tool used to extract a
universal web page from a standard web page. The tool could be used by
the web page designer or could also be used by a server to automatically
create the 'universal' version of a web page. (The extractor does NOT
live at the user's computer.)
The key issue is that the web page designer includes some directives
to the extractor in the web page in addition to the HTML.
This is going to be getting a little theoretical now. Suppose that
there is a universal web page (UWP) whose design is pretty
accessibile. Now suppose that the web page designer decides that
he/she wants to make it more visually appealing by using tables
to put the various sections of the UWP into a more interesting format.
By using tables, the web page designer has introduced accessibility
problems and created more work for himself/herself. The web page
designer could use CSS, but that can be a lot more work than
using tables. However, the problem with tables and accessibility
is that the web page designer would then have to figure out
what linearization is and then run various tests.
However, suppose that when the web page designer creates the web page,
additional information is stored in the web page to direct the extractor
on how to create a universal web page. For example, the web page designer
could include a special comment to the extractor like:
This article discusses
The comment tells the extractor to extract the text contents of the cell
and make it a section on the UWP.
The combination of the special comments and the extractor avoids the
problem of the web page designer having to figure out how tables will
be linearized.
A number of extractor directives could be specified. For example:
!-- universal-extractor: include
--
(The extractor directives could also be TAGS, but then that complicates
the process.)
I think that the extractor approach could be inplemented for some web
sites by using something like a perl script.
Scott
Hi,
The statement:
"If one follows the guidelines for the end result no matter how that
end result is achieved and if that end result validates and if
propper human judgement is employed, than the accessibility as a
result of that effort is vastly improved."
in a way goes back to the question that I was asked by some CHI-WEB
people, especially in light of Jakob Nielsen's report. How much
is accessibility improved by the guidelines? How is this improvement
measured? Why did Jakob Nielsen's report have its own list of
recommendations?
I also believe that my statement on who serves who is not being
understood within the context it was made.
Scott
I was answering part of the message not all of it. There are a lot of
assumptions being made here and I was addressing one of them.
Accessibility in the broadest senxe is not an absolute target nor is it
absolutely obtainable but a couple of things I know. If one follows the
guidelines for the end result no matter how that end result is achieved
and if that end result validates and if propper human judgement is
employed, than the accessibility as a result of that effort is vastly
improved. This is not rocket science. Creating an accessible end
product under most circumstances confining that accessibility to what
the guidelines document was meant to do no matter how it is achieved
will serve the purpose. A multi-model end is a fair way to achieve
this. We have enough anechdotal andother evidence now to answer all the
questions about this topic going back several years and the original
poster should do some reading to see what questions should really be
asked. Following this thread you end up with my post because the rest
has already been covered in one way or another and As I said, we serve
the technology so that it can serve us. I will go further to say that
if we ever get to the point where the technology truly serves us as the
poser of the question asks, we will become subserviant to the technology
which may not be a bad thing depending on how much benevolence we build
into it. I would not want to live in that world however.
I think this is an excellent question that needs to be addressed by
the WCAG working group; I think it's vitally important that we have
some testable metrics not only to whether or not our guidelines are
being met, but also to what degree that improves (or doesn't improve)
access by our core audiences.
If Nielsen's work teaches us nothing else, it should teach us that
standard empirical methods of research and study can be applied to
accessibility and it's not just pie-in-the-sky hopes and personal
anecdotes. [*]
--Kynn
[*] Yes, I am aware that many organizations, e.g. TRACE, have been
doing research on such matters for years; however, I still see
very little of hard, citable research going into guideline
development, and a lot more weight is given to the dogmatic
desires of markup purists or to a few random, unsupported
assertions by people with or without disabilities.
Kynn Bartlett kynn@idyllmtn.com http://kynn.com
Chief Technologist, Idyll Mountain http://idyllmtn.com
Web Accessibility Expert-for-hire http://kynn.com/resume
January Web Accessibility eCourse http://kynn.com/+d201
Kynn, you will then be as pleased as I am that the WCAG group is working with
a few organisations (notably in the UK) to empirically test the usability of
WCAG 2 in particular, following some research done and published on WCAG 1.0.
It is listed as an issue by the group - you should be able to follow it
there. http://www.w3.org/WAI/GL/wcag20-issues.html#14
cheers
Charles McCN
I think this is an excellent question that needs to be addressed by
the WCAG working group; I think it's vitally important that we have
some testable metrics not only to whether or not our guidelines are
being met, but also to what degree that improves (or doesn't improve)
access by our core audiences.
If Nielsen's work teaches us nothing else, it should teach us that
standard empirical methods of research and study can be applied to
accessibility and it's not just pie-in-the-sky hopes and personal
anecdotes. [*]
--Kynn
Hi,
Some considerations about metrics include how well designed they are and
under what conditions are measurements and interpretations made.
The Heisenberg uncertainty principle of course is always an issue
and needs to be considered when designing measurements and metrics.
The conjecture that there is often a bias that can invalidate them
should not be used as an excuse to use no metrics. That is similar
to saying that no program should be used because often they have bugs
in them. Perfection in metrics or software is rarely achievable.
People learn to accept that something is useful if "good enough".
While it may be psychologically reassuring to lump all metrics together
as a way to dismiss them, the reality of metrics and measurements
is much more complex and subtle.
Scott
it is your arrogance that brings this out and it is the arrogant
responses you make that re-oinforces this but I will not differ with you
on your clarification. I will say though that it would take a lot of
money and a lot of years to come up with metrics and even they might be
worthless depending on how far reaching they are and how much the
technology has changed by then. I'm not against metrics but I have seen
two things come out of metrics that scare me. One is that attempts to
measure often distort what is being measured and thus the results are
distorted and two there is often a biass in the measurements that can
invalidate them.
