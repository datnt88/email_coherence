Folks,
Don Eastlake has provided a location for the draft BOF notes, you can find
them at:
ftp://ftp.pothole.com/pub/xml-dsig/IETF44minutes.txt
My brief summary:
1. DOM-HASH. Do we need it, why not just sign surface strings? Abstract
models and semantics can be confusing, but it permits compound documents.
There may be a crypto fault in the "hashes on hashes" algorithm. [1]
2. Brown Draft: XML and binary, alg-independent, composite docs, adding and
removing parts, internal and external items (XLink), endorsement, multiple
recipients (shared keyed hash), co-signature. Concern about crypto
weaknesses. Again, why not just use S/MIME chunks?
3. IETF or W3C: Some folks are more comfortable with the IETF process and
security expertise, others feel the key is coordination with other XML
activities. Consensus from W3C workshop needs to be reported back to IETF on
W3C's plans.
[1] Dan Connolly later provided me with the reference that led him to raise
this concern:
"Alas, the method is not sound. Recall that the probabilistic guarantee
is valid only if the strings being fingerprinted are independent of the
magic number. But fingerprints themselves are dependent on the
magic number, so the probabilistic guarantee is invalid whenever
fingerprints are fingerprinted. The Vesta group was soon debugging
an unexpected collision."
excerpted from
t.i3.html
Copyright (C) 1994, Digital Equipment Corp.
Joseph Reagle Jr. W3C: http://www.w3.org/People/Reagle/
Policy Analyst Personal: http://web.mit.edu/reagle/www/
It should be noted that the purpose of an IETF BoF is to determine if there
is sufficient community interest and a reasonable starting point for the
formation of an IETF Working Group. Thus detailed technical discussion of
proposals really isn't the point unless it is to show that no approache is
likely to enable advancement toward the advertised goal of the working
group it is proposed to set up.
While creation of an IETF working group is never guaranteed, the BoF
clearly showed enough interest and support. At this point, really, the
only question is what the workshop outcome is going to be.
See additional comments below preceeded by ###.
Thanks,
Donald
Donald E. Eastlake, 3rd
17 Skyline Drive, Hawthorne, NY 10532 USA
home: 65 Shindegan Hill Road, RR#1, Carmel, NY 10512 USA
"Joseph M. Reagle Jr. (W3C)" reagle@w3.org on 04/02/99 02:42:49 PM
Subject: IETF Signed-XML BOF Notes
Folks,
Don Eastlake has provided a location for the draft BOF notes, you can find
them at:
ftp://ftp.pothole.com/pub/xml-dsig/IETF44minutes.txt
My brief summary:
1. DOM-HASH. Do we need it, why not just sign surface strings? Abstract
models and semantics can be confusing, but it permits compound documents.
There may be a crypto fault in the "hashes on hashes" algorithm. [1]
### The main thing with which there was no disent was that
cannonicalization is necessary, for the reasons cited in the
minutes. There was criticism by ekr (Eric Riscola) that for
digital signing the recursive nature of the DOM HASH proposal
is not needed (as it would be for efficient tree comparison)
and is slower than just feeding a similarly defined ordered
byte stream for the entire structure to be signed into a
single hash function.
### On the hashes of hashes question, I don't claim to be an
expert in this area. But the modern definition of a strong
hash function is one for which it is computationally
infeasible to find an input string that will hash to a given
hash value AND computationally infeasible to find two input
strings that will hash to the same value. The "Fingerprint"
function used in the reference evidently does not meet this
modern definition because it states that it is easy to come
up with two inputs that have the same hash if you know the
"magic number". (The modern definition assumes you know all
of the interior details of the hash function.)
### Furthermore, PKCS#7, as I recall, normally uses hashes
of hashes. This is, it hashes the data to be signed as well
as any authenticated attributes and then signes the hash of
those hashes. Taking the hash of hashes is a common
modern technique even for systems that have received much
close examinateion so I tend to believe that it is
well accepted for modern hash functions like SHA-1.
2. Brown Draft: XML and binary, alg-independent, composite docs, adding and
removing parts, internal and external items (XLink), endorsement, multiple
recipients (shared keyed hash), co-signature. Concern about crypto
weaknesses. Again, why not just use S/MIME chunks?
### Although ekr said he had concern about crypto weakness
the only specific he gave was an incorrect
criticism where he said that the giving of the hash function
before a block to be signed was not sufficient for one-pass
processing becasue HMAC requires not just the function but
also the key to prefix. But that is beside the point since
the data is not signed directly, its hash is signed. So
there is no flaw here (and if there were, it would be purely
an efficiency flaw, not a crypto flaw).
### On "why not just use S/MIME chunks?" I think an XML
signature standard that isn't XML encoded misses the point.
3. IETF or W3C: Some folks are more comfortable with the IETF process and
security expertise, others feel the key is coordination with other XML
activities. Consensus from W3C workshop needs to be reported back to IETF
on
W3C's plans.
### It seems clear that there would be wider participation
in an IETF working group. The BoF had 157 attendees that
signed the attendance list. The last time I counted the
workshop was just over thirty including W3C staff. That's
fewer than the number of people at the BoF who said they
would actively work on an IETF XML DSIG standards effort.
The IETF is a more open process and is less expensive to
participate in.
[1] Dan Connolly later provided me with the reference that led him to raise
this concern:
"Alas, the method is not sound. Recall that the probabilistic guarantee
is valid only if the strings being fingerprinted are independent of the
magic number. But fingerprints themselves are dependent on the
magic number, so the probabilistic guarantee is invalid whenever
fingerprints are fingerprinted. The Vesta group was soon debugging
an unexpected collision."
excerpted from
in
t.i3.html
Copyright (C) 1994, Digital Equipment Corp.
Joseph Reagle Jr. W3C: http://www.w3.org/People/Reagle/
Policy Analyst Personal: http://web.mit.edu/reagle/www/
There may be agreement that IN SOME APPLICATIONS
the ABILITY TO canonicalize is a requirement.
There is intransigent objection to any requirement that
EVERY signed document be canonicalized.
I know that various people have said 'of course that
will be an option', however I now see the requirement
that the functionality be available turning into a requirement
the functionality be employed.
The reason is that I just do not believe that semantically
neutral transformations are possible in practice. However
good the spec looked I would distrust the implementations.
Moreover I don't believe that there is sufficient knowledge
to construct a formal proof of correctness that demonstrates
that an XML cannonicalisation process is semantically
neutral. XML is not defined using a formal method which
is one obstacle, even if it were however XML is not a
syntax but a meta-syntax, the only proofs I have seen in
that domain which were convincing involved category theory.
The requirement that electronic commerce systems
be formally verified is quite realistic. Proofs relating to
substantially larger systems exist. I find the idea that
digital signatures will be reliably used in any environment
which does not preserve the integrity of messages
considerably less plausible. That does not mean that I
don't expect people to try.
I want to sign the bits on the wire. If people want to use
broken networks, the spec should provide them with the
tools. I do not however agree that those of us with networks
which do not mangle messages should be _required_ to
perform any transformation which is not fully specified
using formal methods and proven to be semantically neutral
using formal methods.
I would like to see a mechanism for signing the bits on the
wire as a phase 1 deliverable and defer canonicalisation
until phase 2, I think that the task will prove somewhat more
complex than some anticipate.
Phill
I agree. And following up on Phillip's comments, I think what will make
sense is if in the signature one can express the semantic depth (surface
string, canonical XML, DOM, etc.) one wishes to achieve in the signature. If
you just want to sign the bits, then you can. Sometimes this might be
implied by the hash algorithm, but it should be explicit otherwise.
Registration closed with 40 folks registered which was the cap.
Fair enough. I personally am not terribly vested in this having to happen at
the W3C. What I do want is that it be a well integrated piece of the whole
XML frabric. We'll have to see what people say on that note. Regardless of
where it happens, I think it should have coordination requirements whereby
it is synchronized and reviewd by the W3C XML-link and Sytax Working Groups
at least.
Joseph Reagle Jr. W3C: http://www.w3.org/People/Reagle/
Policy Analyst Personal: http://web.mit.edu/reagle/www/
Do I need formal methods to assess the neutrality of a canonicalizer? This
is probably true if I want to define a "general purpose" canonicalizer for a
given syntax (i.e. DER for ASN1). However, in most circumstances, people
have considered canonicalization in the confines of well-defined business
frameworks. There, it is very simple to ensure neutrality of the
canonicalizer. This because, from a business standpoint, XML is secondary.
It is just a syntax (or to be more precise a ML that could be used to define
such a syntax) - Adopting some XML DTD instead of ASN1 is just a matter of
convenience. The framework defines the semantics and dictate to the rules.
In such circumstances, a canonicalizer has many benefits and among others
relieves the business logic from the requirement of maintaining a copy of
the bitstream to ensure verifiability of the signature.
The previous comment does not preclude the definition of "general purpose"
canonicalizers such as DOM-HASH and X-HASH (respectively proposed by IBM and
GlobeSet). It shall be a matter of the different business frameworks to
decide if these algorithms are sufficient and semantically neutral from
their respective standpoint. In fact, they have been proven satisfactory in
several circumstances.
But you are correct, canonicalization shall not be a REQUIREMENT - It shall
be possible to sign at the bitstream level. (Though one may argue that the
simple fact to define what should be signed is a matter of
canonicalization.)
Sincerely,
Richard D. Brown
From: w3c-xml-sig-ws-request@w3.org
[mailto:w3c-xml-sig-ws-request@w3.org]On Behalf Of Phillip
hallam-Baker
Sent: Monday, April 05, 1999 1:35 PM
Subject: Re: IETF Signed-XML BOF Notes
You do if you have a requirement to asses an entire system
using formal methods and the canonicalizer is required.
I don't necessarily expect many people to be formally
validating their software but experience indicates that
the set of software systems which have in practice been
thoroughly debugged closely resembles the set for which
formal validation is possible in principle.
[Of course using formal methods may well in practice rule
out building systems in which XML documents are atomized,
stored in a database as parsed trees and reconstituted and
so the communities needing canonicalization and those
needing formal methods appear to be disjoint.]
Thanks, that is all I am asking for.
Phil,
From: "Phillip hallam-Baker" pbaker@verisign.com
Message-ID: 003301be7f93$177d24b0$42060a0a@pbaker-pc2.verisign.com
Date: Mon, 5 Apr 1999 11:34:45 -0700
Yes, the meeting and the minutes and Joesph Reagle's condensation of
them and my partial re-expansion were all a bit incomplete.
The IETF Draft Charter proposes two documents because it really
proposed to standardize two very different things:
(1) how to sign things using an XML encoding regardless of what is
being signed. IE, something that is, to the first approximation, an
XML verison of PKCS#7. This certainly includes singing a unmodified
pile of bits (well, really an octet stream) even if that pile happens,
on closer inspection to by XML.
(2) how to sign XML regardless of the means of siging. For most
cases, this would involve some canonicalization.
The case of 1 &amp; 2 togethjer exists of course, an XML encoded signature
of XML, and is actually a fairly important case, but isn't the only
one. (And, of course, non-XML signing of non-XML exists but isn't in
scope.)
The canonicalization rules DEFINE the semantic content for that
particular canonicalization. Whatever is trashed by the
canonicalization, for example the information as to whethet the
original was in UTF-7, UTF-8, or UTF-16, is by definition
insignificant or you should not have chosen that canonicalizer.
Since I do not believe there can be a unviersal statement for all
systems of what the true semantic content of some XML is, I think you
are asking the wrong question. For any particular specification of
what the semantic content is and a particular canonicalizer, I believe
you could often prove or disprove that the semantic content is
preserved by that canonicalizer.
You are just begging the question. What is "the integrity of
messages"? If you want "the integirty of octet streams" that's
certainly trivial to get. But I think the position that the integrity
of an XML "message" is not changed if it is mapped in an algorithm
one-to-one reversible way between UTF-8 and UTF-16, for example, is
quite reasonable.
And there are cases in protocols where a portion of an XML document is
moved between documents and the like where, for example, the namespace
prefix used to denote a particular namespace was changed and it is
desireable to canonicalize this aspect as well as character encoding,
etc.
If you want to use only feature (1) from above, an XML encoded
signature of a pile of bits, feel free to do so.
I can't conceive of any reason why these tasks can not proceed in
parallel. Both are needed now. I think we should try to reduce the
cases, like IOTP v1.0, where protocols have to adopt their own XML
DSIG procedures because accepted standard XML encoded signatures and
XML canonicalizations are not in place.
Thanks,
Donald
Donald E. Eastlake 3rd +1 914-276-2668 dee3@torque.pothole.com
65 Shindegan Hill Rd, RR#1 +1 914-784-7913(w) dee3@us.ibm.com
Carmel, NY 10512 USA
