I don't think the answer (especially in a general set of guidelines) to
interface problems is to allow keyboard control specifically. I think the
answer is to allow all controls to be exposed to third party technology.
However, for any UA which has a keyboard interface it seems like a
priority 1 that the keyboard be able to operate all controls.
(ie it is more important that the controls are exposed in general, but
only just. Keyboard is the single most important specific device)
Charles McCathieNevile
to follow up on what Charles McCathieNevile said:
Sorry for possibly muddying what seems like a simple question,
but I a just beginning to realize that what we are looking at in
the command area is not just the ability to bind a set of
commands to different tokens that go with different devices, but
that there needs to be a sliding scale of _protocols_ available
for commanding.
Over the last few days I have had the pleasure of watching Gregg
demonstrate the EZ Access modes for touch-screen kiosks that have
been developed at Trace. One of the modes is adaptive for people
with poor fine motor control or a tremor. This is a typing
emulator where there is a two-step transaction for each symbol
recorded. The semi-controlled hand waves past the touch screen
until a sensitive area is brushed and a letter gains the focus
because a touch is sensed. Then, if this letter is intended, it
is committed by pressing the big green diamond button. One of
the traits of the latter is a generous guardband around it. You
don't activate anything else by mistake when going for it.
The touch screen can be used by people with motor control that is
not so fine as the grid of sensitive spots in the alphabet typing
screen, because of the two-phased commit, where a letter is not
committed by a screen touch alone.
The conventional mouse command modality is a stateful protocol
as well, because mouse motion is relative and accumulated by
the screen cursor. That is what breaks when the user is blind.
Not the ability to manipulate the mouse, but the ability to
process the state feedback or orientation given by the cursor.
Al
Could you clarify your statements. You seem to be saying that built-in
keyboard commands are not all that important as long as the function is
exposed to 3rd party assistive technology.
One of our goals is universal design and to reduce the need for 3rd party
assistive technology.
Jon
Jon Gunderson, Ph.D., ATP
Coordinator of Assistive Communication and Information Technology
Division of Rehabilitation - Education Services
University of Illinois at Urbana/Champaign
1207 S. Oak Street
Champaign, IL 61820
Voice: 217-244-5870
E-mail: jongund@uiuc.edu
WWW:http://www.staff.uiuc.edu/~jongund
No, not quite. Keyboard control is probably the most important feature to
implement natively. But providing 3rd party technology access to those
controls (in our hypothetical API - in this case probably via the OS) is
slightly more important. Both are serious priority 1 items, and in
practical terms both are crucial. The document as it stands does not
appear to give that much importance to exposing the controls beyond
keyboard accessibility. I suspect that in practice, providing keyboard
access does expose the functions to the OS and therefore 3rd party
technology, but this relies on assumptions about the OS.
Charles McCathieNevile
