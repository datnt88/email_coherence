I was hoping to polish this proposal a little more before floating it
externally, but alas, with the meeting on Monday, time did not permit. I
hope that I have at least stated my perspective well enough to stimulate
discussion.
Problem statement:
The existing HTTP authentication model does not allow authentication realms
to be distributed across servers. To protect user credentials. HTTP
browsers associate each realm with a single IP address, and will not pass
user credentials to multiple servers even if they claim to belong to the
same realm. This security measure, built into the browser, has created the
undesirable side effect of requiring users to re-type their user names and
passwords for each protected server within a multiple server site.
Abstract: A proposal is made to:
Allow the distribution of protection realms across servers
Protect user credentials from "imposter" servers which claim to belong
to a realm but do no
Pass user credentials securely to content servers which identify
themselves as members of those realms
To re-use user credentials throughout protection realms so that users
are challenged for user name and password only once within the context
of a single session.
Create secure, trusted relationships between servers
Centralize authentication, authorization, and directory services for one
or multiple websites
Centralize directory security
Simplify or eliminate directory services on distributed content servers
Make it scalable
I propose that this new authentication scheme be named "remote
authentication".
Theory of operation:
When access is first attempted to a content page which is protected by
remote authentication, the browser is redirected to the "remote
authentication" server for that realm. This redirection should be done via
SSL for security. The user is then challenged for user name and password
by basic authentication. The server then encrypts the user name and
password with a secret (symmetric) key and returns the user name and
password to the browser where they are cached for the session. The browser
is then re-directed to the original content server and the browser passes
the encrypted user name and password to the server. The content server,
which shares the same secret key as the authentication server, is able to
decrypt the user name and password.
When the browser is challenged by another server which claims to be in the
same realm, the user credentials are served. If the server is trusted, it
will share the same secret key as the authentication server and the user
name and password will be decrypted. If the server is an imposter,
decryption will fail and the user credentials remain secure.
The way I see it, the browser will associate each realm with an encrypted
user name and password, which are simply opaque strings. In addition, I
would like the browser to allow the server to cache an additional string of
arbitrary length to pass state to the other servers in the realm. It could
be used to store additional authentication and authorization semantics,
such as an expiration time for the authentication, authorization
information such as a list of groups to which the individual belongs, an
index to identify the identity of the encryption key, etc. This
information could be signed with an MD5 MAC or encrypted or just plain
cleartext.
I think that we could do the web a great big favor if we eliminated the
need to replicate directories to content servers. The authentication piece
is easy, but if we can solve the authorization part I believe that we could
simplify the common registration puzzle at the same time.
Thank you for your support,
-e
I think that the spec for "domain" is broken -- it specifies a list of URIs,
but doesn't say that these can be _prefixes_ of URIs that may also use the
same credentials. Without that, it is pretty uselss, IMHO.
From: Scott Lawrence[SMTP:lawrence@agranat.com]
Sent: Friday, December 05, 1997 10:53 AM
Subject: Re: Proposal for new HTTP 1.1 authentication scheme
Digest authentication already includes a mechanism (the 'domain'
attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to
specify that credentials may be used on multiple servers, and through the
'digest' attribute allows for mutual authentication.
There is also the model of Kerberos to consider - developing a
ticket-based authentication scheme (with the advantages and problems of
any third-party mechanism) would be another area to explore.
Could the spec be fixed without interoperability trouble emerging?
(Query to digest implementers???).
- Jim
Most of the suggestions by Paul and Dave seem to be clarifications
of the original intent. They should not cause problems.
The one significant change is Paul's suggested change of the algorithm
for calculating the "entity-digest". If implementations exist they
will be incompatible. I don't think I would describe this as
fixing a "bug" in the entity-digest algorithm. It might be an
improvement though. On the other hand, if I recall correctly it
was Paul who wrote the entity-digest algorithm, so he may have a
right to call it a bug.
John Franks
john@math.nwu.edu
I still feel my one objection about proxy-added headers is substantive
and unresolved. Briefly, an origin server might omit headers that get
figured into the entity-digest calculation. A proxy might subsequently
add those headers. The client sees a message *with* the headers,
calculates an entity-digest that figures them in, and gets a different
answer from what the origin server calculated.
Dave Kristol
I agree that there is an issue here. The current spec says the
proxy MUST not add these headers. If I recall you suggested the
MUST be changed to SHOULD. I am not sure how this helps beyond
making the proxy technically "legal." It doesn't materially affect
the problem.
What should a proxy do in this situation? It seems it must either
not add headers or break the entity-digest.
John Franks
john@math.nwu.edu
Ummm... I think my "MUST - SHOULD" had to do with a proxy's changing
the content of headers. I think I see the words to which you're
referring (end of p.13), and they mention Content-Length explicitly but
don't mention Date. And there's a potential problem with
Content-Length: suppose a proxy eats chunked data and wants to create a
complete entity *with* Content-Length. Is it hereby forced to forward
the entity as "chunked" because it's forbidden to add Content-Length?
I agree it's a dilemma. An option is to require that clients send
Content-Length and (perhaps) not Date, and forbid proxies to add either
within this context.
Dave Kristol
Sorry, my mistake.
...snip..
Here is what the spec says:
The entity-info elements incorporate the values of the URI used
to request the entity as well as the associated entity headers
Content-Type, Content-Length, Content-Encoding, Last-Modified,
and Expires. These headers are all end-to-end headers (see
section 13.5.1 of [2]) which must not be modified by proxy
caches. The "entity-body" is as specified by section 10.13 of [2]
or RFC 1864. The content length MUST always be included. The
HTTP/1.1 spec requires that content length is well defined in all
messages, whether or not there is a Content-Length header.
I was remembering "which must not be modified by proxy caches" as
"which MUST NOT be modified by proxy caches."
I guess I don't see a problem with this. On the question of length it
says the content length must be used in the digest even if there is no
Content-Length header. This seems fine and should cause no problem if
a proxy unchunks a chunked response. The server has to calculate the
MD5 digest of the entity so it will not be much harder to calculate
the length. I guess the proxy better get the length right or the
client better do its own length calculation and not trust the proxy.
As for Date, I guess the only problem is servers with no clock.
They don't send a Date header. Draft-v11-rev01 says
A received message that does not have a Date header field MUST be
assigned one by the recipient if the message will be cached by that
recipient or gatewayed via a protocol which requires a Date.
So it seems that it is fine for the proxy to forward the dateless
response as long as it does not cache the entity. It is unlikely
that an authenticated response should be cached anyway.
Maybe there are problems I don't understand.
John Franks
john@math.nwu.edu
Studying the specification some more I see there seems to be
some ambiguity about the meaning of Content-length. Here are
some quotes:
7.1 Entity Header Fields
Entity-header fields define optional metainformation about the entity-
body or, if no body is present, about the resource identified by the
request.
entity-header = ...
Content-Length ; Section 14.14
7.2.2 Length
The length of an entity-body is the length of the message-body after any
transfer codings have been removed...
But later we have
14.14 Content-Length
The Content-Length entity-header field indicates the size of the
message-body, in decimal number of OCTETs, sent to the recipient...
These seem inconsistent. If Content-Length means the length after
transfer encodings have been applied then it is hop-by-hop and not
end-to-end. It also cannot be an entity header as described in
7.1. There probably is also a need for a header meaning entity-length.
Personally I would like to see Content-Length remain an entity header.
All the other Content-* headers are entity headers and apply to the
entity before transfer encoding.
One way to do this would be to introduce a new "Transfer-Length"
header with the stipulation that its default value is the
Content-Length. The Content-Length would be defined as it is now in
section 7, i.e. the entity length. Thus the Transfer-Length header
would only be needed when the message length and entity length
differed. This would give us consistent terminology (Content-* for
entity, Transfer-* for message). It would also not break any current
of which I am aware. At present the only widely deployed TE is
chunked and it needs neither header. If new TEs arise which need
to have the message length specified they would have to use
Transfer-length (or both).
I see no alternative other than rewriting the specification to make
Content-length a hop-by-hop general header and not an entity header.
The authentication specification would also need to be modified
since it is not possible to put Authentication-Info in a chunked
trailer as it is currently defined if Content-length is the length
of the chunked message.
John Franks
Content-Length certainly has been a thorn in my side for a long time,
from the very beginning. Trying to rationalize the contradictory
definitions for Content-Length in HEAD vs GET, and the fact that servers
used it to indicate message length while browsers ignored it except
for measuring the size of a POST, hasn't worked very well. We have
skated by so long as the only transfer encoding is chunked, but John
is right in that the basic abstractions break down when considering
digests or transfer codings in general.
That is a reasonable solution. My only concern would be for proxies,
but I think they'd be better off in the long run with a clear definition.
The one exception to the above is that Transfer-Length would default
to zero for responses to HEAD requests, 204, and 304.
....Roy
I think proxies should be ok. If they understand a new TE which requires
Transfer-length then they should also understand Transfer-length. If
they don't understand the TE they have to reject it. Proxies are not
supposed to touch digests so that shouldn't be a problem.
Yes, you are right. Indeed, any request or response should have
Transfer-length 0 if and only if it has an empty message body. And
an empty messge body should imply Transfer-length 0 without the header
being present.
John Franks
john@math.nwu.edu
I am quite confused here. We introduced CHUNKed transfer encoding
because it is difficult for some servers to know the content length
prior to beginning to send data.
Exactly HOW would a server know transfer-length before sending
data? I can define a reasonable use of content-length in the
trailer of a chunk-encoded transfer ... since content length is
the entity length, it could serve as a double check of the
receipt of the chunk-encoded entity. But clearly, transfer
length couldn't appear in the trailer as the length wouldn't
be known until after the trailer was complete. It makes no
sense to me for a server which knows the length of the transfer
encoded entity to ever use transfer encoding.
Sounds like protocol cruft to me. What am I missing?
Dave Morris
I'm all for an editorial note about "Content-Length" that explains
its difficulty in serving both as an entity length and a transfer
length. In fact, this kind of confusion is, I believe, one of the
reasons why content-length was deprecated for use in mail!
So we're saddled with a legacy, and need some clear warnings that
"Content-Length".
It's less clear what "Transfer-Length" would buy us, since we've
survived for so long without it. It sounds like a good conceptual
aid ("we shoulda done it that way") but not a necessary protocol
addition ("we need to do it this way now").
Unfortunately, this seems (to me, personally) to be the only
way out. I believe that most implementations treat it this
way, in any case.
This seems acceptable to me; what do others think?
We will need two interoperable digest implementations (two clients, two servers)
that have been tested as interworking in conjunction with a
1.1 proxy that does rewriting of the transfer encoding. Is there any
hope of setting up such a test during the weekly testing?
Larry
