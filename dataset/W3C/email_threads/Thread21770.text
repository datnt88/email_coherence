Hi Edd, Dave,
We're looking into making sure #rdfig discussions have an archive at
W3C. Probably the easiest thing to do is to agree a schedule for us to
grab a tar.gz from each of your sites. In theory, the chump is completely
derrivable; in practice, it'd be a pain to rebuild the site that way.
Edd, could you re-remind me the url and schedule for your .tar.gz; Dave, do
you have / could you have a similar setup, so a w3c crontab could grab both
at the same time? Or perhaps the full logs are so much larger than the
chump logs that we'd need to do things different. Hmm, thinking out loud here!
What do you folks think?
Also I wanted to say thanks for running these invaluable services for the Interest
Group; your efforts are much appreciated. Please take this offsite logging enquiry as
an expression of the value that people find in the logs, rather than as an lack of
confidence in your sysadmin skills, btw...!
Let me know what you think would be best to do,
cheers,
Dan
* Dan Brickley danbri@w3.org [2003-01-03 15:20-0500]
Hi, pardon my butting in...
I just wanted to note that grabbing regular .tar.gz's would use
much more bandwidth than some other more incremental approach.
It would be much more efficient (but slightly more work) to
http-crawl each site and just keep a copy of everything we see,
if that's ok with you.
(I don't know how much more work it is; I think it's fairly easy
to do stuff like this with wget, though I always need to rtfm)
Gerald Oskoboiny http://www.w3.org/People/Gerald/
World Wide Web Consortium (W3C) http://www.w3.org/
I'm fine with a crawler, you're right about the bandwidth requirements
-- especially as each day there are at most 4 files which change in the
chump hierarchy (front page, archived day, month, year.)
A simple Perl script could mirror the chump stuff quite happily.
-- Edd
* Edd Dumbill edd@usefulinc.com [2003-01-03 20:35+0000]
Yep, Gerald's right. I guess I figured the laziest, simplest thing
was just grabbing the single tar.gz, but probably best to get the 4 pages
instead.
Dan
| Managing Editor, XML.com, XMLhack.com -- Chair, XML Europe 2003
| I PGP sign my email; more info at http://heddley.com/edd/pgp.html
Just in case I though I would toss this out there... Is the data for the
4 pages available as RDF? If so, I would be willing to contribute a
script that would crawl the pages and keep an InformationStore (or
TripleStore) up-to-date so that the RDF is archived. And could also
contribute code for generating an HTML view of the RDF etc.
Daniel Krech, http://eikeon.com/
Redfoot.net, http://redfoot.net/
RDFLib.net, http://rdflib.net/
No, it's XML.
-- Edd
* Edd Dumbill edd@usefulinc.com [2003-01-03 23:50+0000]
I think Libby has an XSLT that turns it into RDF, as used by
Libby, if I remember right, do you have an URL for your code?
Dan
Actually, no I just get it from the rdfig.xmlhack.com site as RDF (RSS).
see
(though I only harvest new files normally)
Attached is my (very old) XSLT file - but it isn't the one used now I
don't think - I think Edd made it much better.
cheers
Libby
I will have an archive of the rdfig chumps at http://eikeon.com/news/
after making a tweak to the code that displays RSS (to key off of
chump:contributedAt if there is no dc:date) and if there was a
rdfs:seeAlso to the static RSS URL from
I have made the tweak to the display code and manually loaded in today's
RSS... and can add some code for calculating the URLs so that I do not
need an rdfs:seeAlso to discover them.
Daniel Krech, http://eikeon.com/
Redfoot.net, http://redfoot.net/
RDFLib.net, http://rdflib.net/
