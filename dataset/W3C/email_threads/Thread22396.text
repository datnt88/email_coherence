I'm wondering if there might be a way to have the best of both worlds.
Some applications would use DOM to work with possibly invalid XML
documents but still treat the documents as XML. Other applications
might use DOM in a manner such that DOM ensures that the document is
always valid. In this latter case the implementor of the DOM
interface(s) might walk the document through a series of invalid states,
but externally the document will only be seen in its valid form. In
this posting I suggest why we might want to do this, how it might be
done, and what in part it might look like.
The application that can work with an invalid XML document might be able
to use DOM facilities to test the validity of the document, but the
application would be responsible for leaving the document in a valid
state. These kinds of applications might be editors that can take human
coded documents. I'm guessing that this class of application will
always require human interaction in order to make a document valid. If
the document is invalid, there may not be enough information in the
document to correct the problem by automatic means.
The application that works only with valid XML documents would rely on
the DOM facilities to enforce validity. Through these facilities DOM
could only load valid documents and the application can only create or
change documents in ways that leave them valid. Such a DOM facility
would centralize knowledge of what it means to be valid XML and of how
to validate against a DTD. It would not be possible for an application
using these facilities to pass off an XML document to another application
unless that document were valid. This reduces the intelligence required
of receiving applications and improves the robustness of the system.
Robustness would be centralized in the component of one vendor instead of
being distributed across the components of many vendors.
The challenge is in creating DOM interfaces that enable us to satisfy
both needs. It seems to me that one way to accomplish this is to ensure
that the DOM XML interfaces are complete and independent of the DOM core
interfaces. Two kinds of servers could be created: one kind would expose
both the DOM core and the DOM XML interfaces, and the other kind would
only expose the DOM XML interfaces.
In the first server, the implementation of the DOM XML interfaces would
not be able to keep state information for the document element outside
of the DOM core. This way, whenever a client changes the document
through the core, the XML interfaces will operate on the document
containing those changes. The client would be required to bring the
document into a valid state before using the XML interfaces, since the
XML interfaces would have to throw exceptions upon encountering an
invalid underlying document. The one exception might be a DOM XML
operation that tests the validity of the underlying document and that
may provide the client with information about how the document is
invalid. Once the client brings the document into a valid state, the
client might simplify many of its manipulation chores by working directly
through the XML interfaces. The client might only use the core when it
first loads a raw document and when it imports documents into the
current document.
In the second server, the client interacts with the document only through
interfaces that ensure the document's validity. The core interfaces
would not be available. The XML interfaces would throw exceptions upon
detecting an invalid underlying document. It becomes impossible for a
client to create an invalid document through these interfaces. As an
extra benefit, we completely free the server from constraints on
implementation. The server could retain the document using DOM core, or
the server could do something completely different. The implementation
may be a relational database or some hyperlinked data structure. This
frees the server to create especially efficient document access.
Gavin mentioned a performance issue involved with interfaces that always
ensure the validity of the underlying document. He said that it would
probably be too big a hit to always require that every operation check
the document's validity. There are two points I'd like to make along
this line. The first is that to perform the check on every operation
may not be as big a hit as we might expect. The server knows that the
document is valid prior to the operation, and it has control over the
operation itself, so the server need only focus on creating a valid
change to the document. There is no need for the server to "check"
anything other than the client's new contribution.
The second point about the performance problem is that we may not need
to perform any kind of validity "check" on a per-operation basis. Even
assuming that the document is not constrained by multi-user concurrency
issues, transaction notation could be used to solve the problem. The
server would only validate on transaction boundaries. Moreover, the
server could cache knowledge of all operations performed during the
transaction and upon reaching a transaction boundary validate only the
deltas applied to the document. By validating deltas we retain the
efficiency of the minimal checking we could have done on a per-operation
basis. Where validation is necessarily resource-intensive, with
transactions we reduce the frequency of using these resources.
However, I'd like to make another point: I think an XML interface that
always leaves the document in a valid state will require that changes
be made through transactions. I could only find one feature of the
current XML standard that would require this: #REQUIRED IDREFs. It
seems to me that the only way to create a cyclic chain of required
IDREFs and end up with a valid document is to create all of the IDREFs
all in one transaction. If the first element you wish to add requires
a reference to another element, and that other element cannot exist
without an IDREF to the first element (possibly indirectly through a
series of other elements), then the only single operation that yields
a valid document is an operation that creates all of the elements that
exist in the cyclic chain all at once. Having transactions in these
interfaces will also future-proof us against unanticipated extensions
to the XML standard, where such extensions affect our ability to
transform one valid document into another valid document using only
primitive operations.
(Note: There is a way to get around the #REQUIRED IDREF chain problem
without transactions, but it requires that we produce a document that
is *semantically* invalid, although it would be valid by the XML
definition of validity. We could point the IDREF to the incorrect
element momentarily and then later change it to point to the correct
element. This introduces the possibility for client error and would
be entirely unacceptable in a multi-user system or even a multi-thread
system, since different users or threads may end up working with false
information.)
So, it seems to me that we can have our cake and eat it too. We can
have DOM interfaces that will function on well-formed but invalid XML
documents (for example), and we can have DOM interfaces that only
operate on valid XML documents. The first use of DOM will allow us
to create editor-like applications, and the second use of DOM will
allow us to create robust distributed applications, where responsibility
for ensuring the integrity of documents can be centrally maintained.
Moreover, by introducing transactions into the DOM XML interfaces, we
minimize the penalties of validity checking and ensure that DOM can
evolve gracefully in step with changes in the XML specification.
Joe Lapp (Java Apps Developer/Consultant)
Unite for Java! - http://www.javalobby.org
jlapp@acm.org
Overall, it seems like you're asking the DOM to do way too much; it is
intended to allow standard access to common functionality, not dictate
functionality that is at or beyond the state of the art. Again, I see all
this as an interesting PRODUCT idea that could be layered on top of the DOM
and would probably find a significant customer base. But putting it in the
DOM means that we more or less insist that DOM-compliant applications
(including browsers, publishing applications, etc.) implement all that
stuff that is sure to be of interest only to a fairly small subset of DOM
consumers.
Also, it's not clear to me the extent to which XML consumers will be
concerned about always maintaining the validity of their documents. In
XML, DTD's are optional, and even when they are in use I suspect that they
will be in flux most of the time, so insisting on maintaining validity all
the time will seriously inconvenience most users. So I don't think that the
behavior you describe should be the default even if acceptance,
implementation, performance, etc. were not at issue. There will clearly be
a significant demand for "robust distributed applications, where
responsibility for ensuring the integrity of documents can be centrally
maintained", but I see them as non-trivial applications of the DOM, not
something that the DOM should expect to see in a compliant implementation.
I'm intrigued by the transaction processing ideas you put forth. We will
be considering such features down the road (Level 3 maybe?). It may well be
useful to consider your idea of a "robust distributed validating XML
application" built on the DOM as a "reference application" that the DOM
should support in principle, and make sure that the ultimate DOM
transaction model supports the kind of design you propose. But the DOM is
just learning how to walk; your ideas seem to require it to run, so I think
we'll just have to revisit them when the little guy is more steady on his
feet ;~).
Thanks for such a detailed suggestion,
Mike Champion
It is asking DOM to do a lot. However, I think there is a need to have
standard interfaces for manipulating XML documents that enforce validity.
The existence of such a need has two dependencies: a need to centrally
enforce validity, and a need for the mechanism to be a standard.
You suggest that most applications will be working with well-formed but
not valid XML documents. I think that will be true, especially considering
that a well-formed document must contain a DTD. Most applications will be
consumers of XML documents, where the consumer must assume that the
document is valid in order to use it. However, I also think that there
will be many applications producing XML documents, although I'm guessing
that few of them will change existing documents (they will produced new
documents from scratch). Any time two applications need to communicate,
they may choose to do so using an extensible future-proof protocol. One
application will have to create an XML document that is valid with respect
to the receiving application's DTD (otherwise the receiving application
will choke on the document). The DTD serves as a contract, and the
application that creates a document will need to create the document with
respect to the contract. Furthermore, the application may want to ship a
DOCTYPE section with the XML document so that the document references the
DTD on which it was generated. A recipient might be able to then decide
whether it can handle documents of that particular DTD.
I also think the interface to the XML document ought to be a standard.
That way different vendor implementation can be interchanged. Moreover,
it creates opportunity for document-clients to be connected with different
kinds of document sources. Some of those document sources might represent
the document using DOM core or XML, while some will have special document
engines. By having a standard interface to XML documents (which make the
documents look like XML and not something more flexible), we create an
industry for a variety of XML document sources.
In many cases I do expect DTDs to be in flux, but in many cases I also
expect to see them so rigid that it takes hundreds of thousands of dollars
and a lot of time to get one changed. I'm referring to DTD standards for
electronic commerce and application interchange. I'd guess that it will
take many people mulling over DTDs for a long time to solidfy a particular
DTD standard, and then afterwards it would take a lot to get one changed.
However, I wonder if the only real benefit of a validating XML layer is
really just the fact that it validates. I'm not familiar enough with HTML
to tackle this question properly, but here's the question: Is it possible
to create a DOM document that can be expressed as a proper HTML document
but that cannot be expressed as a well-formed XML document? Are there any
constraints that are specific to XML that cannot be applied to documents
in general, but that are not embodied in the definition of XML document
validity? Or is this one of the challenges of the DOM group: to create a
modelling language such that every document expressed in that language has
some well-formed (or proper) expression in all markup languages?
I guess I am getting a bit ahead of things. I would like a better
understanding of what DOM is now and where it is going. You have
partitioned the effort into various stages, and I'm trying to get a grip
on the roles that have been assigned to the stages. I'm starting to
understand what the DOM core means to accomplish and what it means to
leave to other extensions. Thanks for your help in this regard.
Joe Lapp (Java Apps Developer/Consultant)
Unite for Java! - http://www.javalobby.org
jlapp@acm.org
It's funny ... I had a discussion at lunch in which I became convinced that
having standard validation methods in the DOM *would* be a good idea,
especially in a distributed XML application in which "validity" could only
be determined by some server-side process that saw the big picture ...
Anyway, I think there are a lot of good ideas here, but we're so deep in
the mud building foundations at the moment that it's hard to think about
the upper stories. I *can* see the utility of a VALIDXML interface a
couple of levels up (Level 3 or 4), and I hope you can bear with us until
we can give this the consideration it deserves.
In the meantime, part of the work to validate the lower level specs is to
think through what it would take to build higher-level applications on top
of the Level 1 DOM. To the extent that one can point to flaws in Level 1
that would make such a DOM application (or a Level n extension) difficult,
we can try to fix the underpinnings now. So the more real
design/prototyping work people can do with Level 1, the better Levels 2, 3
... will be.
Mike
