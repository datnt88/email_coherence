Hi all,
To get the ball rolling again on the features of HTTP/1.1,
I'd like to get a firm definition of the packetized transfer
encoding. The packet CTE will be used to safely delimit the
size of dynamically generated response messages and (if the
server is known to be 1.1 or above) requests with content.
The last time I posted my notes regarding the format, I referred
to it as being basically the same as that proposed by Dan Connolly
way back in 9409271503.AA27488@austin2.hal.com on www-talk.
I'd like to change that now, before we get products out on the
street that use the sucker.
After investigating the needs of the security/authentication folks
regarding the desire to sign the content of a message after it
is generated, Phill Hallam-Baker has proposed adding a footer to
the format that would contain post-generation Entity-Header fields.
Also, I've been playing around with various formats and have
found that the optimum for most transfers uses a simple one-byte
prefix to encode the length of each packet, with a zero byte
indicating end-of-packets.
So, without further ado, here's the format:
Content-Transfer-Encoding: packet
Entity-Body = *( packet ) NUL
footer
CRLF
NUL = octet 0
packet = packet-size packet-data
packet-size = OCTET, excluding octet 0, representing
the unsigned integer (1-255)
packet-data = packet-size(OCTET)
footer = *( Entity-Header )
Note that the footer is terminated by an empty line (just like the
headers) and is optional. The semantics of a footer are as if the
given Entity-Header fields are part of the message headers.
The un-packetizer algorithm is fairly simple.
length := 0
packet-size := read-byte
while (packet-size  0) do
read input until amt-read == packet-size
write buffer to BODY
length := length + packet-size
packet-size := read-byte
line := read-line
while (line is not empty) do
process Entity-Header
append header to HEADERS
replace Content-Length value with length.
In other words, the result looks like a normal message, with
the Content-Length computed by the unpacketizer. In actuality,
it is often faster to read packet-size + 1 bytes, slurping in
the next packet-size as part of the prior packet read.
So, is that enough detail for discussion? I will try to get
the updated 1.0 spec done this week, with the 1.1 spec soon
after that.
....Roy T. Fielding Department of ICS, University of California, Irvine USA
Visiting Scholar, MIT/LCS + World-Wide Web Consortium
(fielding@w3.org) (fielding@ics.uci.edu)
The advantages that the one-byte packet-size has is that it takes up
a minimal amount of space in the stream of bits to the transfer and
is trivial for any system to produce or consume. Allowing larger
packets means we have to use decimal (with the additional CRLF delimiters)
or hope that everyone remembers to read the number in network byte order.
I acknowledge a small space improvement (see below). I wouldn't rank
it as "onorous". I think "network byte order" is a red herring -- you
would be converting a decimal number to binary.
That is an "or", as in we could send the number using a binary integer
if the integer was restricted to network-byte-order interpretation, but
implementations are notorious for screwing that up in spite of the specs.
Here is a comparison of the two for selected message sizes. (Please
That is a useful comparison for data transfer overhead (thanks),
but what we really need is a comparison of processing overhead,
taking into account the vagaries of TCP socket reads/writes.
Keep it coming ....
.......Roy
I very much prefer the design Dan originally posted
Content-Transfer-Encoding: packet
15
fifteen bytes
where the 1 5 CR LF indicate "read exactly fifteen octets"
(the example happening to have a CR and LF at the end of the printable
characters), at which point the receiver (either client or server)
looks for another ASCII byte count, always terminated by CR/LF.
I do not see the problem with using ASCII numerals terminated
by CR/LF. Though I expect quite a few real transfers using such a
method to be effectively unreadable, it's likely that the "packets"
will often be large enough to hold the entire "file", in which case
the transfers will again be humanly readable. I expect this latter
to happen frequently in a development scenario, and keeping the
protocol "eyeballable" to developers only helps. Simplify life
for the developers and you get a more solid product and get it
to market quicker. CPU time is less expensive than man hours.
I disagree. I see no justification for not using it.
I like this idea; just pick another name. I hope that
Dan's "packet" CTE will see the light of day (even though he may
himself have abandonned it by now), and maybe reserving that name
will help. Maybe call yours "pack8bit"?
Another variation might be to call all of them "packet",
and require a parameter for the prefix: 8bit, 16bit, or ASCII.
That's an optimization that works on some systems;
not on others. Keep the protocol free from platform specifics.
Thanks for the contribution!
Rick Troth troth@ua1vm.ua.edu , Houston, Texas, USA
That's an optimization that works on some systems;
not on others. Keep the protocol free from platform specifics.
Although I strongly favor the use of ASCII encodings of "packet"
lengths (perhaps we could use a term besides "packet", which will
cause untold confusion for years to come), I agree that it might
be nice to use an encoding that occupies a fixed number of bytes.
It's not exactly "platform-specific" to optimize things in this
way; it's basically a tradeoff between these two algorithms:
[*** Anti-paranoia warning: I'm writing these examples using UNIX
*** APIs for concreteness, NOT because this is the the only API
*** I care about. So don't flame me about that, please.]
Naive algorithm:
while (not done)
fgets(length, sizeof(length), stream);
n = decode(length);
fread(buffer, n, 1, stream);
end
Clever algorithm:
fgets(length, sizeof(length), stream);
n = decode(length)
while (not done)
fread(buffer, n+sizeof(length), 1, stream);
n = decode(&amp;buffer[n]);
end
The clever algorithm uses M+1 instead of 2*M reads from the
input stream. This is especially important if your code
doesn't want to read past the end of the input stream (cf.
the NCSA httpd server), in which case you would write this
using read() system calls instead of using stdio buffering.
Note also that if we use a simple ASCII encoding, you need
to parse the input stream to figure out where the end of
the "packet" length string is. If we use an encoding that
takes a fixed number of columns, then you don't need to parse
anything, you just grab a bunch of bytes.
Since I also don't see any reason to limit the "packet" size
to just 256 bytes, or to any small value, I recommend that the
length field be fairly large. For example, suppose that the
standard says "use 6 digits + CR + LF". Then this leaves
the following data nicely aligned (even for 64-bit machines),
and allows almost a million bytes per "packet". Better yet,
use a hexadecimal encoding; this allows  16 million bytes, and
is much easier to encode and decode.
Example:
0 0 0 0 0 8 CR LF
eight bytes of data
1 0 0 0 0 0 CR LF
1,048,576 bytes of data
0 0 0 0 0 0 CR LF
-Jeff
We could call it "chunky" if you like... I prefer that over "chunks",
though I'm sure we could have no end of amusement talking about
blowing chunks across the network. ;-)
Good point -- I prefer that as well (in fact, knowing the size of
the size ahead of time was one of the main reasons to do a 256chunk).
Hmmm, fixed hex sounds reasonable, but 6 digits is a bit overboard.
And why would we need the CRLF if the size is fixed? It does not
improve readability (aside from the first chunk).
How about four hex bytes:
0 0 0 8
eight bytes of data
F F F F
65,535 bytes of data
0 0 0 0 CR LF
CR LF
I would still prefer to see some hard data on the performance
differences. Unlike the rest of HTTP/1.1, this has to be right
the first time its distributed in beta code.
....Roy T. Fielding Department of ICS, University of California, Irvine USA
Visiting Scholar, MIT/LCS + World-Wide Web Consortium
(fielding@w3.org) (fielding@ics.uci.edu)
No, there will be only one packetized 8bit-clean CTE that will be
required for HTTP/1.1 compliance. That is why we are having this
discussion now, instead of after a full draft is produced.
Folks, I am getting a little tired of this line of response
without any thought behind it. There is absolutely nothing
platform-specific about preferring 1 read over two reads.
If you have a technical disagreement, fine, but the next person
who cries "platform specific" better damn well include at least
one concrete example wherein the protocol prefers one platform over
another, and not just because of differring TCP implementation bugs.
....Roy T. Fielding Department of ICS, University of California, Irvine USA
Visiting Scholar, MIT/LCS + World-Wide Web Consortium
(fielding@w3.org) (fielding@ics.uci.edu)
I became a convert to the 'boundary' method for delimiting otherwise
unbounded data, as is done with multipart/* messages. E.g.,
content-transfer-encoding: bounded-binary; boundary="xxxxxx"
would signal that the actual data started with
--xxxxxx
and ended with
--xxxxxx--
This is simple for both the sender and recipient; the packetized
encodings seem messy. I'd worry that we'd need an
'accept-transfer-encoding' to allow negotiation of CTE, though.
Hmmm, fixed hex sounds reasonable, but 6 digits is a bit overboard.
Well, I was assuming that 4-byte or 8-byte alignment is a Good Thing
for fast processing, and that we would use CRLF. With 2 bytes taken
up by CRLF, that leaves either 2 hex digits or 6 hex digits.
To quote Gordon Bell and Bill Strecker, "There is only one mistake that
can be made in computer design that is difficult to recover from -- not
having enough address bits for memory addressing and memory
management."[1] That seems to apply to network protocols, too: look
at the problems with TCP sequence number space and window size, and
with NFS request size and file offset size.
4 hex digits *might* be enough, but since (as you've observed several
times) we won't have a chance to fix this once it is widely
implemented, I'd vote for 6 or 8 hex digits, just to be safe.
And why would we need the CRLF if the size is fixed? It does not
improve readability (aside from the first chunk).
I suppose that's open to interpretation. Anyway, if you buy the
argument that 6 hex digits are worth having, and you buy the argument
that the fields should 4-byte aligned, then the CRLF doesn't really
cost anything.
-Jeff
[1] C. G. Bell and W. D. Strecker, "Computer structures: What have we
learned from the PDP-11?", Proc. Third Annual Symposium on Computer
Architecture, Pittsburgh, PA, pp 1-14. January, 1976.
In fact, the "--xxxxxx--" does not necessarily end the multipart
body, it is just the end of the interesting stuff. Without additional
contraints on accuracy, multipart is worthless as a size delimiter.
In addition, it requires the receiver to scan the entire message
body for the delimiter, which has proven to be contrary to the design
goals of HTTP.
....Roy T. Fielding Department of ICS, University of California, Irvine USA
Visiting Scholar, MIT/LCS + World-Wide Web Consortium
(fielding@w3.org) (fielding@ics.uci.edu)
4 things: 1) I do try to put some thought behind a response
before pressing the SEND key. It is unfortunate that I *consistently*
remember/realize/recognize more pieces *after* pressing SEND for which
I find I have to follow-up. Thus, I remembered my real objection
after pressing SEND, *and* realized that I really LIKE your proposal.
2) it is immensely convenient that MIME is "plain text".
It is unfortunate (but perhaps correctable) that this characteristic
is not documented, supported, and exploited. CTE packet (as you've
proposed) would encourage yet more MIME streams that are valid MIME
and yet NOT "plain text". Although I like your proposed mechanism
for CTE packet, I do NOT like the idea of more non-plaintext
cluttering up MIME. Would that the trend would reverse itself.
If someone could propose a "packet" content transfer encoding
that fit well with a "MIME should be plain text" objective, I'd very
much like to see it. Since I cannot think of one (Dan's was good),
I'll accept yours if the rest of the world does, but I will ask for
some additional wording. (see below)
3) There's nothing broken in the TCP stack I'm using.
But I'm not reading directly from TCP. The TCP stream is being
fed to another stage, another process, in the pipeline of my HTTPD.
In this context, I simply don't have a read(,,size) construct available.
To that extent, "reading one byte beyond" is a platform-specific
optimization. THIS DOES NOT MAKE YOUR PROPOSAL A SHOW STOPPER,
it just means that I won't be able to do it as efficiently as others.
The *optimization* you recommend is what's platform specific,
not the CTE itself.
Really, this has nothing directly to do with the platform (VM)
but with the VHLL (REXX) and its supporting tools (CMS Pipelines).
I'm getting a lot of efficiency where I can hand-off the byte counting
to system level routines. If the whole thing is plain text, that
makes my work a complete cake walk. Where things are binary,
I can still get it done, it's just not as elegant.
4) I suggest (for the sake of argument) that we recommend
CTE packet explicitly for on-the-wire transfer and INCLUDE WORDING
that as it is taken off-the-wire the CTE packet should be immediately
processed and removed. Other than that, this is a good idea.
I regret that I didn't recognize my pleasure in it at first, but I
think I've outlined the reasons why it struck me wrong initially.
Using just one byte clearly avoids the network byte order
problem. Using a "zero means end" signal works just fine.
Excellent proposal, Roy.
So in summary: I'd like to keep off-the-wire MIME in "plain
text" (ie: with local canonicalizations and all printable characters)
form whenever possible. I'm willing to accept on-the-wire MIME in
"binary" forms if we can be clear about the difference, can explain
the canonicalization issue, and can encourage the use of the plain text
forms where feasible.
Rick Troth troth@ua1vm.ua.edu , Houston, Texas, USA
