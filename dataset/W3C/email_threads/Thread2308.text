from minutes, HTTP Working Group, IETF June 96, Montreal
|Aug 1: (Leach) draft on sticky headers, short names for headers, and
Can short names for headers (Good Idea) be compatible with existing practice in
HTTP/1.x or must it wait until 2.x? Does allowing aliases for the names of the
header fields alter the general message parsing algorithm or message semantics
as specified in 3.1 of HTTP/1.1?
-marc
I'm not sure. I have been thinking about it some, and have the following
preliminary observations:
1. It could only be used if you knew you were talking to a 1.2
application.
2. A 1.2 proxy talking to a 1.1 or earlier application would have to
convert.
3. Signatures or hashes would have to be computed on the canonical, full
name, form.
I haven't seen any showstoppers yet to using it in 1.2.
Yes, it does -- changing existing header field names (i.e., short names or
tokenizing) can only be done in HTTP/2.x. Besides, it doesn't make any
sense to introduce short names now when the GOAL should be a multiplexed
and tokenized grammar for HTTP/2.0, and that can be accomplished in six
months if people don't waste their time on minor tweaks that are equally
incompatible with 1.x.
...Roy T. Fielding
Department of Information &amp; Computer Science (fielding@ics.uci.edu)
rom the Montreal minutes suggestion for a new charter's milestones:
I suggest dropping this as a product of the working group. Although it
is certainly possible to compress the protocol through two adjacent
communicating parties by the addition of stateful interaction (what is
meant by sticky headers and context identifiers) and tokenizing protocol
elements (a small part of which is covered by short names for headers),
it is not a trivial task and needs to be extremely careful with regard
to proxy-proxy communication with requests from multiple user agents
being interleaved. This falls into the category of "research" and
should be investigated outside the standards arena.
In other words, I don't think this task can be accomplished within the
lifetime of this WG. Furthermore, even if it were accomplished, our long
term (within a year) plans for HTTP would end up replacing it by the
multiplexing and tokenizing of HTTP/2.x. Therefore, I think it would be
better to focus within the WG on things that can be done before the
next IETF meeting, and encourage other efforts to focus on the research
work that already needs doing for multiplexing and tokenizing of HTTP/2.x.
...Roy T. Fielding
Department of Information &amp; Computer Science (fielding@ics.uci.edu)
Header compression has been in TCP for quite a while, so I don't think
that the topic is "research". It is true that good careful engineering
will be required, but that's different that research.
HTTP is not TCP. Just about everything ever planned for HTTP has been
done before in something else. The research question is what happens
when you add that functionality to the existing HTTP system. Does it work
as intended? Does the improvement justify the added complexity?
Those are research questions (and yes, careful engineering does involve
research whenever the exact same thing has not been done before). I like
research (obviously) -- I just don't like research in the name of
standardization.
Why don't you wait until the draft comes out? I promised it by the end
of the month, and it is well under way. If it is so hard to do, it
should be obvious at that point. A draft will also enable experimental
implementations, to gauge how much improvement it might make.
That does not justify a milestone on the WG charter -- we can do all that
outside the committee structure.
If the improvement looks good, we would be very interested in deploying
such a scheme fairly quickly.
You can deploy such a scheme any time you want -- you just can't call it
HTTP. Note that the Upgrade header field exists for that very purpose.
Once you have answered the research questions, a specification of it can
start through the standards cycle, along with any other performance
improvements at an equivalent level of maturity.
Even if HTTP/2.x could be finished within a year, deployment will take
much longer becuase of the installed base of 1.x. But HTTP/2.x seems
much more like research than any 1.x enhancement under discussion.
HTTP/2.0 (consisting of just HTTP/1.1 in a tokenized form and encapsulated
in a multiplexed stream) can be done in six months. We already have
specifications (albeit incomplete) for both, and more than one test
implementation showing primary gains from multiplexing. As such, it has
the exact same risk factor, cost, and implementation time as any incompatible
change to the protocol. The difference is in the payoff.
Even so, I would not have this WG try to standardize HTTP/2.0 -- it is
still research. In fact, I would encourage any vendor to begin work
on it immediately, since 99% of the effort is just enabling the internals
of the application (client or server) to deal with any form of a
multiplexed, persistent connection; once that is done, converting it to
the final "standard" form is easy.
Rapid incremental evolution, as opposed to revolution, has usually been
more successful at getting technology to the user quickly. It's just
part of the sociology of large distributed systems.
Allow me to refresh people's memory. A group of people involved in the
development of the Web met at the Chicago WWW2 conference (October 94,
before this WG existed) to discuss the future of HTTP and the plan of
attack for improving its security, performance, etc. Henrik and I
volunteered to rewrite the HTTP specification, since it was already a
year out-of-date and it was very clear that new people entering the Web
project did not understand the full capabilities of the protocol, and we
needed a shared understanding in order to avoid breaking its good qualities
when advanced to the next generation. Simon Spero and Dave Raggett
(with Andy Norman) volunteered to work on a binary version of HTTP
[I can't remember if it included multiplexing at that time or not -- I do
know multiplexing was present soon afterword]. A few months later, we
put together the HTTP BOF at San Jose which began the HTTP-WG.
Small improvements in the protocol were to be done in HTTP/1.x in parallel
with the large performance change in HTTPng, for precisely the reason you
give: Rapid incremental evolution. The difference, however, is that
evolution within the HTTP/1.x family would retain the existing message
format and field names to maximize compatibility (reducing complexity
of the implementations) and maintain a "safe harbor" for those unable or
unwilling to sacrifice simplicity for speed.
Adding persistent connections was one of those small improvements -- removing
the dependence on a single transaction per TCP connection was a basic step
toward supporting multi-layered proxy configurations efficiently and a
better understanding of long-lived, multiplexed connections. And yet, it
took over six months of prodding before we could get people to experiment
with it -- before we could do the basic tests necessary to determine if
it was even worthwhile adding to the proposed standard. In hindsight, I now
regret that we even bothered to do that much -- it alone added over eight
months to the delay in getting HTTP/1.1 finished (and, BTW, I was mighty
pissed-off when the same person who insisted on it being in HTTP/1.1 later
complained about how long it took to get draft 00 to the WG).
Almost two years later, the performance problems of HTTP remain the same.
So does the solution. The only thing that is lacking is the PEOPLE dedicated
to writing a complete specification of the solution that we already know
will work. This doesn't have to happen inside the WG -- it just has to
happen.
In the mean time, the notion that anyone can write a complete specification
of something that hasn't been tried before, and then get this WG to agree
that adding statefulness to an otherwise stateless protocol is a good thing,
all in less than 4 months, is utterly ludicrous. Attempting to do that
within this WG will only starve-out the other products of the WG, such as
a real specification of content negotiation, because people like me will
have to spend hours generating long-winded messages just to explain why
it is the wrong way to proceed instead of doing something that would
contribute toward accomplishing our other goals.
...Roy T. Fielding
Department of Information &amp; Computer Science (fielding@ics.uci.edu)
There's some concern in the community that "HTTP-WG will close, what
will happen to HTTP-NG". The standard answer of "Another Working
Group" hasn't been all that reassuring, without there being much
evidence of such.
So, I'd like to strongly suggest that those who are interested in
starting a HTTP-NG working group ("binary format HTTP, without MIME
message headers") get together, create a charter, a list of
milestones, find document editor(s) and a chair and begin NOW to lay
the groundwork for starting HTTP-NG review now, if things are far
enough along.
I'd suggest that the charter of HTTP-NG-WG be narrowed to concern
itself only with the transport issues (tokenization, multiplexing,
etc.) rather than an open ended list of HTTP enhancements.
Having a draft charter and a WG proposal will remove some of the myths
surrounding HTTP-NG.
I don't recall if there's a public HTTP-NG mailing list; creating one
so that all HTTP-NG discussion can move there (rather than here on
HTTP-WG) would be a good first step.
Larry
swop the wg for an ng on this list and your set.
Cause maybe (maybe) | In my mind I'm going to Carolina
you're gonna be the one that saves me | - back in Chapel Hill May 16th.
And after all | Email address remains unchanged
You're my firewall - | ........First in Usenet.........
