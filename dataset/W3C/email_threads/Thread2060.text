The goal of my proposal is to
(1) Provide a means for new servers &amp; browsers to correctly
handle existing (unmodified) Web data in various character
set encodings.
(2) Not to break the current servers and browsers (anymore than
they are already) with regards to handling these code sets.
The proposal does not try to fix things that are broken in existing
clients/servers.
I agree with Larry Masinter masinter@parc.xerox.com
that we should replace the
with the
request header in this proposal. Larry, thanks for the update.
Here are my replies to the thoughtful comments of:
Daniel W. Connolly connolly@hal.com
Ken Itakura itakura@jrdv04.enet.dec-j.co.jp
Daniel |7.1.1. The charset parameter
Daniel |
Daniel | [...]
Daniel |
Daniel | The default character set, which
Daniel | must be assumed in the absence of a charset parameter, is US-ASCII.
Daniel
Daniel This conflicts somewhat with your proposal. However, the RFC goes on
Daniel to say...
Daniel
Daniel | The specification for any future subtypes of "text" must specify
Daniel | whether or not they will also utilize a "charset" parameter, and may
Daniel | possibly restrict its values as well.
Daniel
Daniel I wonder if changing the default from "US-ASCII" to
Daniel "implementation-dependent" can be considered "restricting the values"
Daniel of the charset parameter.
I agree that if the charset parameter is not specified, the default
***should*** be US-ASCII (or ISO8859-1, if it's been changed).
Unfortunately, since charset was reserved for future use, Japanese servers
had no choice but to serve non-Latin files without a charset parameter!!
Why don't we enforce the default for servers using a future version of the
HTTP protocol? ...and let current versions be "implementation dependent"
in order to preserve backwards compatibility?
Daniel I suppose the relevant scenario is where an info provider serves up
Daniel an ISO2022-JP document with a plain old:
Daniel Content-Type: text/plain
Daniel header. I gather that this is current practice.
Yes, this is the current practice (for text/html too). Additionally, some
files are sent in SJIS and EUC code set encodings with the same headers.
Daniel That intent is already mucked up somewhat by the fact that normal html
Daniel documents are allowed to have bytes 127, which are normally
Daniel interpreted as per ISO8559-1. So we already have the situation where a
Daniel conforming HTTP client, say on a DOS box might retreive a text/html
Daniel document and pass it over to a conforming MIME user agent, which would
Daniel then blast it to the screen. The user would lose, cuz the bytes 127
Daniel would get all fouled up.
Yes, this situation is broken for current browsers/servers and I do not
propose to fix it. Using my proposal, a new DOS browser would send:
and a new server would send back:
and the new DOS browser should convert it to PC 850 for rendering.
Daniel But... back to the case of ISO2022-JP encoded data tagged as plain
Daniel would muck things up. So where do we assign fault?
Daniel My vote is to assign fault at the information provider for serving up
Daniel -JP encoded data without tagging it as such.
We are not trying to fix existing browsers/servers.
If a new charset-enable server slaps the wrong charset header (or fails
to slap on a header for non-Latin1) to a new charset-enabled browser, it
is the server's fault.
Daniel So all those information providers serving up ISO2022-JP data without
Daniel tagging it as such are violating the protocol. This doesn't prevent
Daniel NetScape and other vendors from hacking in some heuristic way to
Daniel handle such a protocol violation. But the spec shouldn't condone this
Daniel behaviour.
Unfortunately, the spec is lagging behind the implementations. The spec
did not provide a means for the existing servers to resolve this problem.
Pragmatically, I cannot introduce a server or client product that breaks
established conventions.
As mentioned above, can't this be handled with HTTP versioning?
HTTP V1.0 &amp;&amp; no charset paramter == implementation defined
HTTP V3.0(?) &amp;&amp; no charset parameter == IS08859-1
Daniel Ok... so now let's suppose all the information providers agree to
Daniel clean up their act. Somehow, they have to get their HTTP servers to
Daniel tag -JP documents as such.
Daniel
Daniel How do they do this? File extension mappings? It's not relavent to the
Daniel HTML or HTTP specs,
Daniel but I think the overall proposal is incomplete
Daniel until we have a workable proposal for how to enhance the major httpd
Daniel implementations to correctly label non-ISO8559-1 documents.
Yes, I explicitly left this out my proposal, but you're right, we need
to discuss the implications.
Ken - Before encouragement to label correctly for non-ISO8859-1, we must
Ken give servers the way to know what they should label it. Otherwise,
Ken nobody can blame the server that distribute illegal information.
Ken
Ken The third one has the difficult problem. For the situation for the mail
Ken may simple, since the user knows he knows what encoding he use now, so
Ken he can specify the correct label before sending it. (The user who doesn't
Ken know about encoding at all must not use default encoding.) But for the
Ken situation for the web documents is difficult. I think the file extension
Ken mapping nor the classification by the directory structure is not suitable.
Ken My current Idea is 'server default' + 'directory default' + 'mapping file'.
Ken But I myself don't like my idea. Does anyone have more elegant idea?
Initally, I assume most web data will be configured on a directory or file
basis. I imagine most files will be configured by what directory they live
in. This should be relatively easy extension for existing servers and
how they parse their config files.
Files like Japanese .fj newsgroups (in ISO2022-JP) are already organized by
directories. So are a lot of Japanese Web pages.
A web site with versions of the same files in different encodings
(e.g., SJIS, EUC and JIS) or languages (e.g., English and Japanese) could
create separately rooted trees with the equivalent files in each tree.
The top page could say click here for SJIS/EUC/JIS or English/Japanese.
File-by-file basis would be supported too, but I'd expect this to be
used infrequently. Besides, this would be a Web server administrator's
nightmare to maintain the configuration database.
I don't like the idea of new extensions, although the current server
software probably could support this. I think the data should really
identify itself and not rely upon extensions. Also, we don't want
to make people rename their files. For example, how are you going
to rename the news archives?
Ultimately, I'd like the content itself to specify the encoding.
One idea, is for a HTML charset tag that would take precedence over the
MIME header:
THE REST OF THE DOCUMENT GOES HERE
Daniel Then web clients will start seeing:
Daniel
Only new charset-enabled clients will see this.
Daniel Many of them will balk at this and punt to "save to file?" mode.
Daniel
Daniel Is that a bad thing? For standard NCSA, Mosaic 2.4, no, because
Daniel it can't do any reasonable rendering of these documents anyway.
Daniel
Daniel But what about the multi-localized version of Mosaic? Does it handle
Daniel deploying the enhanced version?
Daniel
Daniel parameter unless the client advertises support for it. I think that
Daniel will cause more trouble than its worth (see the above scenario of
Daniel untagged -JP documents being passed from HTTP clients to MIME user
Daniel agents on a DOS box.)
Why is this more trouble? It's broken now and remains broken. In either
case it would ignore the charset information and guess at the
encoding (for most clients the guess would be 8859-1).
But the purpose of NOT returning the charset parameter, has to do with
not breaking the client parsing of the MIME Content-Type. If the server
always slapped charset on, current clients would parse the header:
not just 'text/html' string and would fail to read Latin1 files!!!!!
To be backwards compatible, the servers should not send the charset
parameter to old browsers.
Daniel variations or just latin1? That is, when a client says:
Daniel
Daniel Accept: text/html
Daniel
Daniel is it implying acceptance of all variations of html, or just latin1?
Daniel
Daniel To be precise, if a client only groks latin1, and it says accept:
Daniel text/html, and the server sends ISO2022-JP encoded text, and the user
Daniel loses, is the fault in the client for not supporting ISO2022-JP, or at
Daniel the server for giving something the client didn't ask for?
Daniel
Daniel so the client didn't advertise support for -JP data.
Daniel
Daniel But "giving somethign the client didn't ask for" is _not_ an HTTP
Daniel protocol viloation (at least not if you ask me; the ink still isn't
Daniel dry on the HTTP 1.0 RFC though...). It's something that the client
Daniel should be prepared for.
As you put it "It's something that the client should be prepared for."
I'm still assuming that
dictates if the server sends back the charset parameter. An old browser
should continue to get the 2022-JP data untagged. A new charset-enabled
browser should get tagged 2022-JP data even if it only advertised 8859-1.
Daniel not returning latin1 data. So the client does know that it's not
Daniel getting latin1 data. It has the responsibility to interpret the
Daniel charset correctly, or save the data to a file or report "sorry, I
Daniel don't grok this data" to the user. If it blindly blasts ISO2022-JP
Daniel tagged data to an ASCII/Latin1 context, then it's broken.
I agree. I've purposely read EUC and JIS pages on my Mac (SJIS), so that I
could save the source and look (grok) at it later. (not a usual user...)
I'm glad you bring up this point, so we can consider the implications.
But what the client does in this situation should be implementation
dependent and not part of this proposal.
Daniel Does this mean that charset negociation is completely unnecessary?
Daniel No. It's not necessary in any of the above scenarios, but it would be
Daniel necessary in the case where information can be provided in, for
Daniel example, unicode UCS-2, UTF-8, UTF-7, or ISO2022-JP, but the client
Daniel only groks UTF-8.
Daniel
Daniel In that case, something like:
Daniel
Daniel Accept-Charset: ISO8859-1, ISO2022-JP
Daniel
Daniel or perhaps
Daniel
Daniel
Daniel I'm not convinced of the need for the generality of the latter syntax.
Daniel Besides: we ought to allow preferences to be specified ala:
Daniel
Daniel Accept-Charset: ISO8859-1; q=1
Daniel Accept-Charset: Unicode-UCS-2; q=1
Daniel Accept-Charset: Unicode-UTF-8; q=0.5
Daniel Accept-Charset: Unicode-UTF-7; q=0.4
Daniel Accept-Charset: ISO2022-JP; q=0.2
Daniel
Daniel which says "if you've got latin1 or UCS2, I like that just fine. If
Daniel you have UTF-8, UTF-7, or -JP, I'll take it, but I won't like it as
Daniel much."
Ken I want to add one more thing about this issue. We could have the document
Ken which uses multiple charset in future. We must define the way to label
Ken such a document.
Ken It can be like ...
Ken Is this OK?
I'd rather leave this as a possible future direction. Multilingual has
had a lot of heated discussions. If we can agree on a means to support
the existing mono-lingual mono-encoded Web data, that will allow us
to create products to fill an immediate need. Can we phrase something that
leaves this open and discuss this in another thread?
Regards,
Bob
Netscape Communications Corp. 501 E. Middlefield Mtn View, CA 94041
I too, agree with Dan: if the data uses anything other than Latin1, it
should be tagged as such.. We need to force people to start using
tags, one way or another, and this seems a reasonable line to draw.
I would also like to note (and I am sure everyone agrees), that this is
all a temporary solution. Things like:
are obviously a poor solution at best. Larry's "conversion server"
idea is obviously preferrable.
As Dan noted, you cannot do it this way. SGML (even if you play with
the declaration) *cannot* handle it. The charset tag idea is also not a
winner...
given the distribution channels in Japan where everyone gets such
clients (magazines, Nifty, NTT), updating should not be hard. It will
certainly be easier than supporting broken software for years to
come....
I have several things to say regarding the character set issue
which reflect the reality of Web technology and how it applies to
the HTTP standards process.
First, regarding standards:
As Dan said, the "official" standard will always lag behind existing
practice -- that is by design. Although some of the drafts we produce
will include things that have not-yet-been-implemented, they will not
be submitted for RFC consideration until we have at least two independent,
working implementations of everything that is contained in the final
draft. Note that this is also why we work on several versions of the
protocol at the same time -- good ideas that are not implemented get
shoved off to a later version.
Second, regarding character sets:
A character set defines the table of codes used to associate small
groups of bits within a document to their individual semantics
(in most cases, a common symbol to be manipulated and/or displayed).
It does not define the format of the overall document, nor does it
have any effect on the language(s) used within the document (other than
the incidental one that some languages cannot be represented using
the symbols defined by some character sets). Some interesting discussion
of languages and character set issues can be found in the Internet-Draft
draft-ietf-mailext-lang-char-00.txt .
Character set names for use in Internet protocols are registered with
IANA and listed in STD 2 (RFC 1700). This is what MIME uses, and what
HTTP will use. The current list includes:
US-ASCII
ISO-8859-1 ISO-8859-2 ISO-8859-3
ISO-8859-4 ISO-8859-5 ISO-8859-6
ISO-8859-7 ISO-8859-8 ISO-8859-9
Note that if your favorite character set is not listed in the above,
someone needs to get off their duff and have it registered by IANA.
There are many others that have been listed in related RFCs, e.g.
UNICODE-1-1 UNICODE-1-1-UTF-8 UNICODE-1-1-UTF-7
ISO-2022-JP ISO-2022-JP-2
ISO-2022-KR
There is also a separate list of character set names in STD 2 that is
not yet used by MIME. These are names approved for Internet documentation
(but not necessarily Internet protocols). The registry is at
ftp://ftp.isi.edu/in-notes/iana/assignments/character-sets .
Note that none of the above is specific to HTTP, nor do we have any
intention of making it specific to HTTP.
Third, regarding media types:
A media type is an association between a (usually) large number of
bits and a document format. Most document formats have a default
character set which defines how the bits are grouped into meaningful
symbols. Some media types allow the character set to be defined
within the document itself. Some media types which do not have such
a capability (including those called text/* by MIME) are provided with
character sets other than the default. The IANA registry is at:
ftp://ftp.isi.edu/in-notes/iana/assignments/media-types .
The character set is ALWAYS a feature of the media type, regardless
of whether or not the charset parameter is present. MIME defines the
default charset of all text/* types as US-ASCII. HTTP defines the default
charset of all text/* types as ISO-8859-1. In both cases, the default
document containing bits intended to encode UTF-7 characters with the
header
Content-type: text/html
is ridiculous. If you want to send a UTF-7 encoded document, it must
be sent with (case-insensitive)
Like character sets, this is not an issue for discussion under HTTP.
HTTP simply uses what has already been defined for other Internet
protocols. Changing the default for text/* was a touchy issue, but
that reflects the reality of Web technology being better than SMTP
and has little effect on accessing older protocols through HTTP.
Changing the default to "implementation dependent" would make the
specification as broken as these clients.
Fourth, regarding applications that can't handle parameters on media types:
Fix them. Let's face it folks, we can't allow broken implementations
to limit the extensibility of the protocol. Browsers that cannot parse
parameters will never be able to handle character sets other than
ISO-8859-1. Nor will they be able to handle future versions of HTML
(which will be indicated by a level parameter). Thus, it makes perfect
sense for them to have to treat the response as application/octet-stream
(the default behavior) if the content-type is unknown.
Finally, regarding an Accept-charset or Accept-parameter header:
I believe it is a mistake to continue loading down the request syntax
with content negotiation information which is pointless 99.9999999%
of the time. On my system, a complete accept-charset listing would
add an additional 361 bytes to every request, and that's with a small
set of X fonts. The likelihood of me ever needing that information
is nil. To be useful, there would need to be a substantial set of
documents that are available in multiple character sets.
In the rare case where parallel sets of documents exist, it makes more
sense to provide a means for automated redirection via URCs and allow
the client to determine which is the "best" of available options.
A non-automated equivalent is the "click here for our Unicode version".
Sure, it's ugly, but nowhere near as ugly as 2 million clients trying
to ask ahead-of-time for every possible format and every possible
character set and every possible language accepted by the client for
the billion documents which are only available in a single
format/language/character set.
Having said that, I do expect that an Accept-charset header will appear
in HTTP/1.1. However, it will be used as all Accept-* headers should
be used -- only when the client wants to specifically restrict the
result to a set of options different than the default (as is the case for
in-line images today).
Accept-charset: UNICODE-1-1-UTF-8, iso-8859-*
would indicate that only UNICODE-1-1-UTF-8 and the iso-8859 set is
allowed as a response to this request.
BTW, Accept-parameter is not useful; charset is the only parameter
shared by multiple media types. We could invent some new parameters,
but that only makes the problem worse. Also, a quality attribute is
only useful if we add it to the content-negotiation algorithm --
something I would like to avoid (like the plague that it has become).
......Roy Fielding ICS Grad Student, University of California, Irvine USA
I think the image/* types might share information about color &amp; size,
just as the text/* types would share information about character sets.
Is such a definition available for image/*? Personally, I'd rather avoid
the whole issue, because the next thing someone is going to ask for is:
Accept-font: YIKES
Pre-emptive content negotiation just doesn't work. Perhaps we should try
to define a matrix of common client characteristics/behavior, and just
provide a header for that. E.g. one of
Agent-profile: graphic, color=24, xwin
Agent-profile: graphic, grey=8, mwin, unicode
Agent-profile: graphic, bw, xwin
Agent-profile: text
Agent-profile: braille
or maybe not. I don't see any good way to go about it.
......Roy Fielding ICS Grad Student, University of California, Irvine USA
I've been patiently watching the discussion of Accept-Parameter, et
al., and waiting for someone to mention my extensions proposal
(http://www.research.att.com/~dmk/extend.html) as a possible approach.
Since no one did, I will.
I hate to see HTTP cluttered with a bunch of rarely-used headers. At
the risk of using more characters (Roy Fielding's caution noted),
Accept-Parameter could be rendered (from the client):
Extension: HTTP/charset required=oneof; set=UNICODE-1-1-UTF-8
Extension: HTTP/charset required=oneof; set=iso-8859-*
The server would either respond with a similar header, like
Extension: HTTP/charset set=UNICODE-1-1-UTF-8
to identify what was used, or it would return a failure, because neither
of the required sets was supported.
An Extensions:-unaware server would ignore the header and return none,
and the client would have to fend for itself, as it does now.
Roy Fielding said:
The matrix approach is interesting (rather than negotiate each little
item). My Extensions: proposal includes a negotiation algorithm. I
think it would be better to settle on a single algorithm than have a
different one for each thing to be negotiated. Perhaps the one I've
proposed is unsatisfactory. I'm open to improvements. Nevertheless, I
think there will be things in HTTP that will demand negotiation.
Dave Kristol
