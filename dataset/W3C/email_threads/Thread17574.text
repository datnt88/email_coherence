The following URL is a draft of the new UA charter.
Please do not publish this URL to anyone and should only be used by UA
group members at this time. Please review and comment on the charter to
the group. There will be some time for discussion of the charter at the
next telecon wednesday.
Two goals in the draft that are important:
Publication of second working draft in september
Publication of recommendation in December/January
Charter process:
Currently the WG (you) and the WAI staff are reviewing the charter and will
probably make some changes. The charter will then be send to the W3C
director for possible revisions and approval.
Jon Gunderson, Ph.D., ATP
Coordinator of Assistive Communication and Information Technology
Division of Rehabilitation - Education Services
University of Illinois at Urbana/Champaign
1207 S. Oak Street
Champaign, IL 61820
Voice: 217-244-5870
E-mail: jongund@uiuc.edu
WWW:http://www.staff.uiuc.edu/~jongund
Some comments:

2.2 Criteria for success
Criteria for success of user agent WG include:
Adoption of guidelines by user agent developers
Inproved [sic] access to the WWW by persons with disabilities
How do we measure success? Items 2 "improved access to the WWW by PWD" is
really vague and unmeasurable.
Also:

13.0 Level of involvement of Team
5% Ian Jacobs
5% Daniel Dardailler
5% Judy Brewer
In my opinion, 5% of someone's time is unrealistic for meaningful input. If
meaningful input is not expected, then change to reflect that.
Charles Oppermann
Program Manager, Active Accessibility, Microsoft Corporation
"A computer on every desk and in every home, usable by everyone!"
I agree that measuring "improved access" is vague. The charter also says
that we will "Evaluate the usability of accessibility features" under 2.1
scope of work items. What if instead we say something like, "Establish a
mechanism for having users with disabilities test the usability of user
agent features." I think it would be important to get feedback formally or
informally from users who are not directly connected with the WAI activities.
And then, under "Criteria for success of user agent WG include:" say
something like,
-Adoption of guidelines by user agent developers
-Documented use of user agent accessibility features by persons with
disabilities with or without assistive technology.
Actually I don't know if documenting feature use is beyond our scope, but
it seems like it would be important to determine whether or not certain
features were being used successfully.
Kitch
Charles::
There are some techniques people have used to measure this. At
the Federal WWW Consortium Seminar on Universal Access last
Tuesday, Educational Testing Service (the people who brought you
the SAT) briefed what they have done as a part of quality
improvement project for the accessibility of their web
publishing. They have an approach to consolidating user
evaluations that seems to make some sense. There is also some
work out of Cork, I hear.
I suppose the questions are: Do we want to listen to actual
users? If so, how?
Kitch::
This is one area where it will be good to coordinate with the
Evaluation and Repair Interest Group. The WAI may or may not be
organizing user testing, but somebody is going to be doing it
somewhere, and we probably want to make it part of our plan to
find out what they are learning and use that knowledge here.
Al
Kitch::
What is to be tested? Existing browsers, mockups, paper walkthroughs? Who
will run these tests?
It should be expected that the browser developer perform usability testing
of their own product (we do, I am sure the others do too), which is
inclusive of the intended user population. When we hit design issues we
can't resolve ourselves, we turn to outside research. If we can help explore
an issue through quick changes in our code, we are more than happy to do so.
Al::
The WAI setting up usability testing is not the answer. Facilitating the
exchange of information between the groups doing this work is a better use
of resources.
Perhaps we can say in 2.1: "Identify and develop resources to support the
implementation of the user agent guidelines by developers. This includes
industry, governmental, and university groups who will participate in the
exchange of information and research findings relevant to guideline
implementation."
If we need to include "evaluation" in 2.1, perhaps it should be based on
6.2, where it says the guidelines are:
"... consensus-based, technically sound, and reflect the most current
technology."
This implies some analysis/evaluation led to the adoption of given
guideline.
Where we don't have any findings (or where they are unclear) it would be
of value to have a "hit list" of design issues in need of research, which
are not presently addressed by any group.
Mark
