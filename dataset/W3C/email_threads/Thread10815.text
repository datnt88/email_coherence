I feel I should apologize in advance here; I am keenly aware of the fact that
the 7-bit-centric-ness of contemporary computer systems is redolent of a lot
of really bad Western attitudes, so arguing against complete i18n generality
makes me really uncomfortable. And in fact some of the things that Henry
&amp; Gavin &amp; Martin have been saying are very important.
But, let's review the design principles. XML has to be *easy to write
programs for* - I draw your attention to
that anyone competent ought to be able to cook up an XML processor in a
few days. For this reason, we are probably going to discard a bunch of pieces
of SGML that are really potentially very valuable, such as abstract syntax
and SHORTREF and LINK - simply because we are happy with the trade-off
of increased ease of implementation for loss of functional richness. So even
though I may end up agreeing with Gavin, for the moment I'm going to argue
the minimalist position, because someone needs to.
My informal interpretation of our clause 4 ("It shall be easy to write
programs which process XML documents") has been more or less, "I can
do it with flex and yacc".
Thus, what we need to do is avoid the extremes of (a) retreating into a
7-bit grass hut, and (b) trying to build something handles all the arcana
of bit-patterns/glyphs/charsets/encodings. Rather, we want something that
hits the right trade-off along the spectrum.
Having said all that:
I would feel somewhat strange about not supporting native language
markup, particularly as we're going to have to use a variant concrete
syntax to support native language content. It seems to me that the
most reasonable thing to do would be to decide upon a syntax that
used ISO 10646 for both data and markup...
Well, data for sure. The problem is, if we keep markup in 7-bit land,
I'm pretty darn sure I can build an efficient parsing/validating system
out of flex &amp; yacc in a week or so (because I have), and utf8 data, for
example, won't break anything. Now, I'm not sure I *can't* do this if
we let markup out of the ASCII box, but it's a question that we have to
think about carefully, because we're in danger of compromising a central
design goal.
you'd also require all content to be in
UTF8, and many users have no way of creating such data. In the best
cases, producing such data usually involves a conversion somwhere.
Hmm, for us Westerners there's no problem, because the standard SGML
repertoire is utf8 as it sits (right?). You're right that your average
Japanese editing system doesn't emit UTF8... but I still think that it
might be reasonable to say that "in XML, it must be utf8" - so the
conversion gets applied on the way in. Question: conversions such as
{S,N,EUC}JIS - UTF8 and Big5 - UTF8 look very easy in principle - are they,
in practice? And does software exist?
Again, we had the same discussion in HTML-WG. There are many good
reasons for selecting a single document character set, and then just
looking upon SJIS and whatnot as encodings.
I think there are many good reasons - overwhelmingly good - for the SGML case.
For XML, we should think about sweeping all the charset/encoding subtleties
under the rug: it's UTF8 and that's all there is to it. Once again, if
it's not crystal clear - we want anyone who wants to write a program to
process XML to be able to go to one place and look at one simple set of
rules, and we want the program, once written, to have a very high degree
of probability of processing any XML doc in the world *as it sits*. If we
get this, and as a consequence there is some proportion of documents that
XML cannot address, that's a fair price to pay.
E.g., a trade-off such as "If you *REALLY* *MUST* have the documents
in the repository in native Shift-JIS, and can't do conversions in &amp; out,
then you have to use real SGML, because it has the machinery for that."
Once again, I'm *really* not a monoglot cave-dwelling bigot.
Cheers, Tim Bray
tbray@textuality.com http://www.textuality.com/ +1-604-488-1167
syntax to support native language content. It seems to me that the
most reasonable thing to do would be to decide upon a syntax that
used ISO 10646 for both data and markup...
If you don't restrict yourself to pure ASCII, you can still write a
minimal parser in a week, or less, using flex and yacc. Yes, I have
done this, and no, it is not difficult at all (in fact, I used a
variant of the TEI grammars).
you'd also require all content to be in
UTF8, and many users have no way of creating such data. In the best
cases, producing such data usually involves a conversion somwhere.
You are getting closer to what I am thinking, and indeed, what you
propose would be one scenario in what I have in mind. There are 2
problems here: 1) that some people confuse coded character sets and
encodings, and 2) it depends on what you see an SGML parser
manipulating.
I imagine a system that has a parser which deals in a single
normalised form (internal encoding). Whatever the data is actually
encoded in get's converted into this normalised form, and if we have a
single document character set, this conversion can be done blindly. I
recommend that the syntax use ISO 10646 because it covers a great
number of languages. Note that I do not believe it wise to prescribe
what the *internal* representation is, nor the number of possible
encodings and XML parser should handle.
Note that the tables for writing converters from SJIS, EUC, etc. to
UNICODE are publically available, and it is easy to use them to
automatically generate converters for what you wish to handle.
UTF8 doesn't solve the worlds problems. I think we can fix the
character repertoire, but fixing the encoding is arbitrary, and
prescribes certain implementation details. It also complicates usage.
[Gavin:]
some people confuse coded character sets and encodings
Yeah. Me, for instance. I apologize for getting this mixed up (once
again) in an earlier comment.
Let's review the basics for us slower members of the class.
* The character set allowed in markup or data (e.g., 10646 BMP) and
the character encoding put on the wire or in a disk file (e.g., UTF-8)
are completely different issues.
* We can specify 10646 as the character set (in the SGML sense) for
all XML documents while still allowing a much more limited encoding
(such as 7-bit ASCII) for the transfer of a particular document
instance, assuming that information about which encoding is being used
is conveyed when the transmission is established.
* Notwithstanding the last point, we can (if we want) go farther than
saying that 10646 is the XML character set (in the SGML sense) and
also say that UTF-8 is the only (or recommended, or expected, or
default) encoding that we shall/should/expect to find when we open a
file containing an XML document or receive a byte stream during an
HTTP session.
Right?
Jon
From: Gavin Nicol gtn@ebt.com
Date: Tue, 10 Sep 1996 19:55:42 GMT
I agree that fixing the character repertoire on ISO 10646 is the right
way to go. (There appears to be consensus on this.) How much
flexibility to allow with encodings is a difficult issue.
Going with UTF-8 makes a lot of sense in the Unix world. Within a few
years I expect to see most Unix platforms supporting UTF-8. But the
Microsoft Windows world is going with UCS-2 (or UTF-16 which is an
extension of UCS-2); system tools (such as Notepad) on Windows NT
already support UCS-2, and I would expect Windows 9x to follow suit
eventually.
However, although many platforms do not currently have support for
either UTF-8 or UCS-2/UTF-16, my impression is that most of them are
planning support for one or the other. This suggests the possibility
of allowing just these two encodings for XML.
If we do this, we then have to face the issue of how an XML system
will know which encoding is being used. I think there's a easy
solution to this. The Unicode standard recommends that files start
with a byte order mark (0xFEFF); since the character 0xFFFE is not a
legal character this allows a Unicode system to determine whether
big-endian or little-endian byte order is being used. Fortunately
neither 0xFF nor 0xFE can start the UTF-8 representation of a
character (actually I don't think they're legal anywhere). So XML
could require that a byte order mark be used whenever UCS-2/UTF-16 is
being used, and assume UTF-8 when the mark is not present.
Restricting XML to these two encodings would, as Gavin says, be
somewhat arbitrary and would indeed complicate usage at least in the
short term for Japanese and European markets. I'm very sympathetic to
Gavin's point of view here, but what's the alternative?
- Allow any encoding at all? That's an implementation nightmare and
users lose the guarantee that any XML system will be able to handle
any XML document.
- If not, what encodings do we allow? If we allow SJIS amd EUC,
shouldn't we also support the Chinese, Taiwanese, Korean encodings as
well, and all the ISO 8859-[123456789] for the Europeans. There are
many other encodings that have active user communities.
And if we have lots of possible encodings, we certainly can't
auto-detect which are being used. The possibilities I can think of
are:
a) rely on some external mechanism to specify the encoding
b) use ISO 2022 escape sequences
c) require each XML file to have a MIME header with a charset
parameter
d) use an attribute in a formal system identifier
Whichever way we go, there's an enormous step up in complexity over
the UTF-8/UCS-2 route.
James
Good. One down.
I will tell you exactly what will happen if we do this:
1) People who use ASCII, will pretend they are using UTF-8
2) People who do not use ASCII (SJIS, EUC, JOHAB etc.) will
*ignore* this requirement and implement systems that handle the
encodings they use every day.
As you, and many other must also be aware, UNICODE doesn't solve the
worlds problems either, just 95% of the more common ones ;-) We have
to think of XML being used to deliver things like the classical
buddhist texts, dead languages, etc.
I think it would be quite hard to guarantee that all XML systems will
be able to meaningfully intrepret any arbitrary XML document
anyway. We are defining a data syntax, and parsers, but not an entire
set of application semantics.
In addition, allowing any encoding to be used is not the nightmare you
make it out to be (it's not particularly pleasant either, and in fact, I
argued very strongly for UTF-8 to become the lingua franca of the WWW
a year and a half ago). I have no problem with recommending that
UTF-8 or UTF-16 be used for XML, and I will even applaude that
stance, but I cannot stand by and watch them become the *only* choice.
The problem of unlimited encodings can be approched in one of two
ways:
1) Limit the encoding an application can accept. For browsers,
content labelling and negotiation are sufficient to handle this.
2) Make the application open-ended by using dynamic link libraries,
scripting languages, or whatever, so that encoding conversion
modules can be downloaded.
I am very much in favor of (2), and have a prototype for a langauge
that could be used to accomplish this (and Rick also has an idea for a
language that would be quite interesting here).
MIME content labelling. This is why I have been fighting for close to
2 years to get client and servers to actually accept, generate, and
use the charset parameter on the HTML MIME type:
I also proposed a data storage format that could be used to combine
such meta-data and the actual data together in a single file.
God forbid....
Right. This is pretty much what my *.mim file type did: it just has a
group of MIME headers followed by CRLF, then the data.
A nice way to go...
It depends. Actually *using* UTF-8/UTF-16 in a meaningfull way in an
application would be just as difficult as any other encoding, and in
the case of UTF-16, maybe even more so. Having only these two
encodings also prescribes certain implementation decisions.
Supporting a wide range of encodings is certainly less work that
supporting a wide range of languages in a browser.
Also: an internationally standardized SGML way to go. FSIs have been approved
for the SGML Extended Facilities in the HyTime TC and will be included in the
revised 8879.
Charles F. Goldfarb * Information Management Consulting * +1(408)867-5553
13075 Paramount Drive * Saratoga CA 95070 * USA
International Standards Editor * ISO 8879 SGML * ISO/IEC 10744 HyTime
Prentice-Hall Series Editor * CFG Series on Open Information Management
Can you expound on this a bit? What character encodings do you
currently use, for what texts, that won't fit into Unicode? Do you
really have encodings that can't even be handled by putting the
characters you need into the private use area of the BMP? If you do,
I'd really like to know more about it.
Some of the Chinese encodings have thousands of characters not in
Unicode. Then we also have live languages that still do not even have
a computerised form yet.
I'm having trouble thinking of serious applications that meet the
standard you appear to be setting, i.e. that do not restrict their
character sets in any way.
It's not so much a matter of the application being unrestricted, as
the *standard* giving enough freedom of movement to allow applications
to be built that would generally fall outsude the mainstream use (for
which UNICODE will generally suffice). As I noted, we can fix the
coded character set to ISO 10646, and we have the BMP, which provides
us with enoguh room to accomodate these other applications, provided
we do not restrict the list of acceptable encodings.
As for a serious application: something I have long wanted to do is to
build a system in which characters could be registered, along with
references to common code points (something like TEI WSD) so that one
could build "virtual" character repertoires and "virtual" coded
character sets. The system would also have glyph server technology so
that an application (or perhaps an OS) could retrieve the glyph images
as appropriate. You can kind of fake this at the moment with SDATA
entities.
C compilers and other language processors do not accept source code in
arbitrary coded character sets; nor do editors and word processors, nor
do Web browsers.
This is just a limitation in the implementations. They could if they
wished.
The internationalized versions of Mosaic I have seen and heard about
do accept more than one coded character set, but they are *not*
extensible, in the sense of allowing run-time additions to their
capabilities by the end-user. They are extensible in the sense of
allowing programmers of sufficient skill to recompile them after
tinkering with the character-handling code.
Sure. In the future, I expect JAVA to provide a nice way for people to
actually support *arbitrary* encodings, and at some point, a way to
render arbitrary character sets. I have been doing some work on this
on the side.
Eventually, I would like to see coded character sets as such
disappear.
than to tell them "To handle your unusual writing systems in XML, recode
the lexical scanner, recompile, and invoke the XML parser."
In a well designed system, this is unecessary.
I should note that I have never said that I wish to *require* that all
XML parsers be open-ended. I have no problem at all with seeing Latin
1 XML systems, and SJIS XML systems, though I expect most will
actually be UNICODE based.
If one really, really needs arbitrary coded character sets, why not
use Real SGML?
Because it's not necessary to do so.
[Tim writes]
Whichever way we go on this, here's a question. Suppose we have a mechanism
for flagging the encoding of some text, and the top-level entity of some
XML doc has been thus flagged as being in encoding X. Is it then a
requirement that all external entities referenced from the top level also
be in encoding X, or do we need to re-evaluate, for example by
checking for James' proposed 0xfffe?
I have been through this so many times in various forums...
If you have a document entity in say SJIS, I can see no good reason
at all for requiring that all external entities also be in shift-jis.
For example, say I have a document entity on an NT machine in JAPAN.
It uses SHIFT-JIS. It includes an entity from CHINA in BIG5, and another
entity from Thailand in TIS, and another from Europe in ISO-8859-1.
Provided we have a single document character set, and some way of
indicating the encoding (preferrably via the MIME type, or via FSI
attributes), then there is no problem at all, and in fact, there
is a great deal of benefit in such a system.
On Thu, 12 Sep 1996 22:11:25 -0400 Todd Bauman said:
Even live languages. I've got some of these documents, and I would
hate to see XML disallow the character encodings I need to use.
Gavin's right,
Can you expound on this a bit? What character encodings do you
currently use, for what texts, that won't fit into Unicode? Do you
really have encodings that can't even be handled by putting the
characters you need into the private use area of the BMP? If you do,
I'd really like to know more about it.
I stand corrected. I do use the private use area for this. I simply
was thinking about all of the odd nonstandard 8-bit character
encodings and matching fonts that I have to employ to get these languages through
existing tools. Of course one problem with the private use area is that
its private.
using UTF-8 as a default and / or suggested encoding and including it
in a reference implementation is one thing. Prohibiting the use of
other character encodings is too restrictive. Whether through MIME and
/ or through FSI's, XML has to be extensible in this regard.
I'm having trouble thinking of serious applications that meet the
standard you appear to be setting, i.e. that do not restrict their
character sets in any way.  Its not that I want to have an
unrestricted character set its that I want to have a way to inform
others that I am employing a particular character set encoding.
Specifying 1 or 2 such encodings such as UTF-8 and / or UTF-16 is
to restrictive.
Its not that I want an unrestricted number of character sets, I just
want to be free to use different encodings of that set, and I
standard way to inform others that I am doing this. Specifying one
or two encodings is to restrictive.
1. Many people like the encodings that they currently use, have the
tools to work with them, and won't be changing anytime to soon.
2. UTF-8 / UTF-16 are terribly inefficient encodings for a large
number of languages. They require 2 or 3 bytes per character when an
alternate encoding would require only one. UTF-8 is particularly
offensive with its blatant western bias. No one is going to use
these inefficient encodings when they have large amounts of information
to store / transmit and they are paying for the bandwidth.
Moreover, many of the languages that UTF-8 bloats in size by two or
three times are those used by countries that have access to the worst computer and
communications technology.
C compilers and other language processors do not accept source code in
arbitrary coded character sets; nor do editors and word processors, nor
do Web browsers. Emacs does pretty well, on X, with character sets
represented by fonts in the X library. I don't have high hopes for any
users who need it to handle EBCDIC all of a sudden. The
internationalized versions of Mosaic I have seen and heard about do
accept more than one coded character set, but they are *not* extensible,
in the sense of allowing run-time additions to their capabilities by the
end-user. They are extensible in the sense of allowing programmers of
sufficient skill to recompile them after tinkering with the
character-handling code.
I would say that this is a poor design.
I don't want end-users to be able to add support for encodings, only
programmers. But I would like -
1. The code that needs to be changed should be
isolated from the parser and the rest of the application.
2. When I'm done I can still claim that I have an XML application.
3. I can communicate to other software that I am using an alternate
encoding for my information.
4. The parser - application API is isolated from any encoding changes
I make.
On the whole, it seems to me simpler to tell users "To handle your
unusual writing systems in XML, translate your documents into Unicode
(using the private-use area if you need to) and invoke the XML parser"
than to tell them "To handle your unusual writing systems in XML, recode
the lexical scanner, recompile, and invoke the XML parser."
? Even with a style sheet? Perhaps you and Gavin have higher hopes for
'meaningful interpretation' than I do in the first place, but I am
having trouble imagining *any* level of interpretation that won't become
a lot more complex if the parser must adjust at run time to
character sets unknown and unimagined at compile time.
I am not a DSSSL expert (nor really even a amateur) so I cannot
attest to its capabilities. I was simply referring to the way ISO
10646 decomposes characters. This makes the mapping from code point
to glyph non-trivial. Multiple ISO 10646 characters may need to be
combined to get the composite that is actually displayed. This is
further complicated by languages such as Arabic in with glyphs change
depending on there proximity to other characters. Browsers capable
of doing this correctly for all languages are difficult and
will not exist for a while (if ever). There is simply no
commercial market for supporting languages like Burmese (which is one
of those languages that is not yet in ISO 10646). As soon as
the font mess is straightened out it will of course be possible to
do this rendering at the server, create the correct glyphs, map them into the private use area,
send a custom font and at least get the browser to display it.
It seems to me that allowing arbitrary coded character sets really
pushes us over a line between something simple and something that may
possibly still be tractable but is surely no longer simple. If Unicode
is not enough, then a finite and small set of alternate coded character
sets can be defined as legal input. Allowing arbitrary parse-time
extension is not the way to keep XML simple to implement.
I always make the distinction between the parser, the entity manager
and the storage manager. The parser sees only UCS-4. It is the storage manager
that needs to be concerned with character encoding, not the parser. I
just want a way to add a storage manager to XML to
support other encodings, and have a standard way to record in a
data stream (possibly outside of SGML) that a specific encoding is being used.
If one really, really needs arbitrary coded character sets, why not
use Real SGML?
1. Due to product availability / price considerations.
2. Due to the increased performance of XML software over its more
feature laden counterpart.
B. Todd Bauman
Graduate Student
University of Maryland, Baltimore County
I was thinking the same thing. For better or for worse, most HTML markup is
still done by hand, and I would hope that XML would be useful and compelling
to the more advanced members of the HTML user community (even those who are
not now familiar with SGML).
Perhaps I could turn this around and ask: is ease of implementation really
as crucial as we have been treating it? I agree that it should be possible
to implement XML much more _efficiently_ than SGML, but I am skeptical that
a language that is "fanatically" easy-to-implement but difficult to write
data for will achieve more success than an author-centered language with a
few free, fast reference implementations.
Note though, that many of the SHORTTAG features actually make SGML harder to
write by confusing authors: "Why do I have to use quotes here, but not
here?" So I am not advocating that we preserve all (or even most) of the
keystroke saving devices in SGML.
Paul Prescod
I think that a major goal of XML should be "reliable interoperability". I do
not think that "support for whatever you need to do" should be a major goal.
If people start creating standards-compliant SJIS XML documents on the Web
and standards-compliang XML client software cannot read it, then we have a
PROBLEM, in my opinion.
When I have heard SGML "sold" I often hear "SGML is completely platform
independant". People presume that this means that they can take any SGML
document from any platform and use it on any other platform. Well, SGML is
platform independant, but SGML documents aren't really. I feel that it is
time to make the language where the documents are truly platform
independent. SGML will always continue to be the flexible language that
supports idiosyncratic needs at the possible expense of document portability.
One last question: Isn't it reasonable to expect that most local encodings
could be translated into XML by the HTTP server on transmission? If so,
there is no loss of convenience in requiring the on-the-wire format to be
standardized in the same way that you would expect FTP keywords to always be
ASCII and IP packets to have a certain byte-ordering.
I don't know what we'll decide in the end, Gavin, but isn't it nice that
we're talking about Unicode _as a minimum_? "We've come a long way, Baby."
Paul Prescod
Prof. Hsieh from Academica Sinica is presenting a paper on handling
variant Han ideographs and missing characters at the China Korea Japan
Document Processing (CJK DOCP) meeting in a weeks time. I don't know the
details yet, but I think it may build on Prof Eiji Matsuoka's comments
about using standard national ideograph books in CJK to reference in SDATA
entities for missing characters.
(CJK DOCP has a liason with WG8 through Dr Komachi of Japanese Business
Machines Association.) I am hosting this meeting this year, and I will
send out details to this list.
UTF-8 is particularly offensive with its blatant western bias.
As far as I know, (apart from Wired, which didn't even credit who these
outraged Asians were as far as I can remember) the difficulties people
have with Unicode tends to be not so much this encoding or that, but rather
that for Han ideographs it follows radical order and not the order in
any existing national character set order. But I suppose if it had it
would be criticised for being "Japanese" or "Taiwanese" or "PRC" biased!
Also, the round-trip rule and unification makes it impossible to specify
other variants in the code. However, when SGML is added, who cares? All
the extra information can be contained in markup: it is just a matter of
setting agreed semantics of the markup.
Unicode seems better than anything before, especially w.r.t. making a
character catalog. (People may be away of PRC's standard GB13000 which
adds all the extra Han ideographs from ISO 10646 onto the previous
national standard: retaining backwards compatibility with previous
character set.)
And the issue of user-defined characters is irrelevent unless there is a
mechanism for accessing their glyphs over the Web. Until there is, there is
no point trying to allow for them in XML.
Rick Jelliffe http://www.allette.com.au/allette/ricko
Allette Systems http://www.allette.com.au
On Mon, 16 Sep 1996 06:22:28 -0400 Rick Jelliffe said:
This seems to represent a third requirement for XML's i18n support; is
that a plausible interpretation? (The first two I see are 1 handling a
suitably large character repertoire, 2 allowing non-Ascii characters in
names.) Is the requirement met by ensuring XML DTDs have a well-known
method for indicating language (and geographic language variant?) in
content, or will more information be needed than is carried by a LANG
attribute of the sort we know from TEI, from DocBook, or from the HTML
i18n proposals?
A fourth requirement? Or should this be outside the scope of XML?
-C. M. Sperberg-McQueen
[In response to Paul Papresco]
I agree, but it is a problem which will not be solved by saying "all
XML documents *must* be in either UTF-8 or UTF-16". Such a statement
*will* be ignored.
This is precisely why I don't think it's such a big deal. If you send
notification to the server that you can only accept UTF-8, ASCII, or
Latin 1, and the server send you a SJIS document, the server is
broken. It should either convert the doducment to UTF-8 before sending
it to you, *or* send an error message.
NOTE 1: In the future, language like JAVA will also give applicatons
the option of downloading translators.
NOTE 2: One of the primary reasons for requiring a single document
character set is precisely such cases: the server can
*blindly* (ie. without parsing) convert the XML data, which
can be made *fast*.
I guess you can actually remember me arguing the hard minimalist path
some time ago... as weel as arguing for support for the TEI subset ;-)
I disagree. I would like to see the ability to use user-defined
characters so that it would spur interest in font/glyph servers.
If I had more time, I'd write the software myself...
Both the need for a standard set of attributes/elements for I18N
and the issue of glyph servers etc. fall outside the scope of this
WG.
On Mon, 16 Sep 1996 19:24:04 GMT Gavin Nicol said:
They may, though I haven't seen that prescribed for us anywhere.
The possible need for a new attribute type (declared value) with a
particular meaning, and the possible need for a new defaulting
mechanism, on the other hand, are clearly within the scope of this WG.
It seems to me that a new declared value for language/locale flags and a
new keyword for attribute-value inheritance from enclosing elements may
be worthwhile, even without reference to Asian languages. If such an
attribute type also makes it feasible to handle the problems Rick
Jelliffe referred to, then all the more reason to provide one.
-C. M. Sperberg-McQueen
