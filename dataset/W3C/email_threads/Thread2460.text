Hi all, sorry if I am not following correct protocol in mailing to this
group. First a bit of background - Our company develops WinGate, a fairly
popular proxy server for NT/95, and so we have a lot of vested interest in
the HTTP/1.1 protocol.
Reading through the proposed standard again today, and some of the issues
raised on http://www.w3.org/Protocols/HTTP/Issues/Overview.html I would just
like to raise a small suggestion with regard to chunking. I have another
with regard to real-time streamed media sent through HTTP, but depending on
the response to this, that may form the entity body of another posting.
To my understanding, chunking was included in HTTP/1.1 for one reason only -
that being where the server could not know beforehand, the size of the
entity it was sending, most likely because it was being created by some
other process (e.g CGI etc). So, Chunking was added to allow the client to
KNOW when it had received the entire entity.
However, this adds a couple of problems/questions:
1. The chunk size may be arbitrary, and the resulting latency introduced at
the server (since chunking at the server requires some form of buffering)
may cause performance degradation in applications such as streamed media
through HTTP. Is there any recommendation on how to chunk the data?
2. Any intermediary (proxy) must continually monitor the transfer through
itself. Is it allowed to re-chunk the data, and if so under what guidelines.
So, I have a proposal to achieve the same desired objective as chunking.
Proposal:
Addition of a new option to specify that transmission of the entity is
terminated by a certain sequence of octets (like an end tag) transmitted by
the server before closing the TCP connection. The receiver would then know
if had received the whole entity by examining the last packet received
before the connection was closed. If the end tag was there, then it got the
lot, else it didn't - simple.
This method provides no less information to the client than the current
chunking method, since there is no progress information in chunking anyway
(one does not know how many chunks are coming through), but is MUCH less
intrusive into the entity body data, and would therefore be much more
robust. A proxy for instance could simply pass the data as it received it,
and if talking to an HTTP/1.0 client would simply strip out the end tag.
This would involve holding back the last n bytes (length of end tag) from
every packet relayed through itself until it received the next packet, which
it would hold back the last n bytes of. When notified of a close, if would
look at those n bytes, and if they were the tag, it would discard them, and
close the client connection, else it would send them to the client and let
the client deal with the problem of not having received all the data.
For an HTTP/1.1 client, it would simply pass everything blindly. If the
entity were cachable, it could be written directly to file as received, and
the end tag removed at the end of the session. Even if the end tag were
sent through to an HTTP/1.0 client, chances are it would not cause problems,
unlike the current chunking system.
A proposal for the end tag could be say a 4 byte magic number. So there
would be a 1 in 4 billion chance of there being a natural conflict caused by
a premature close on the connection with the last 4 bytes of the last packet
being the magic number. If this were thought too risky, simply increasing
the length would serve to render this event less likely.
I believe this method would simplify client, server and intermediary
implementations, achieving the same objective, and thereby increase software
reliability, reducing the opportunities for bugs.
Regards
Adrien de Croy
Adrien de Croy - adrien@qbik.com. Qbik New Zealand Limited, Auckland, New
Zealand
See our pages and learn about WinGate at http://www.qbik.com/
Hi
- From: Adrien de Croy [mailto:adrien@qbik.com]
- Sent: Thursday, January 29, 1998 11:37 PM
- Subject: Re: HTTP/1.1 : Chunking
-
-
- 4.2 Message headers
-
- The allowing of multiple headers if their values form a
- single list seems a
- redundant complexity. Unless you are catering for people
- debugging by hand
[...snip...]
-
- Personally I would remove allowing this from the spec. I
- don't believe any
- client software would implement it (unless they are
- programming in Pascal or
- something with a string length limitation - UGH!).
-
Does the spec allow you to collapse multiple headers into
a single line? If so, you can put them in your name/value
list just the same.
It does, and that is definitely what I will be doing, but the thing is that
the spec allows it, which means implementors must check for it.
-
- Proxy Authentication.
-
- There are some problems with the proposed method where it
- relates to chained
- proxies, or a "use proxy" response. Basically it may be
- necessary for a
- client to include many Proxy-Authorization fields in a
- request, since it is
- possible that only the client holds the credentials of the
- proxies involved.
- Otherwise ALL agents in the chain (including the end server)
- MUST support
- persistent connections.
-
- If it needs multiple authorisation fields, how can each
- proxy tell which one
- is intended for it. It would have to try them all for a
- match, strip it
- out, and pass the request on.
-
Huh? (put 305 aside for the moment)
If the client is going through multiple chained (auth requesting )
proxies then it must include credentials for each of them.
As a side affect, each credential is readable by all
proxies in front of it.
The proxy knows which credential set is for it
by looking for the realm value that it generated.
OK.
As a former proxy implementor, I am happy to agree that
the proxy-auth system is a bit ugly and could use some work.
A proposal to include more specific realm info (for the chained
case) was rejected because it could cause backward compat problems.
Are these backward compatibility problems real - how many implementations
would need to significantly alter their behavior due to a change here, given
that proxy authentication was not even a part of HTTP/1.0
- Multiple byte-range requests?
-
- Normally I guess these would only be made by caches, as clients would
- generally always need the end of a file, rather than chunks
- out of the
- middle of it. Are overlapping byte ranges meant to be
Not really. If a client is doing pipelined range requests,
it may very well, retreive bits of an entity at a time,
ie bytes
1) 1-100 of entity A
2) 1-100 of entity B
3) 1-100 of entity C
4) 101-200 of entity A
5) 101-200 of entity B
6) 101-200 of entity C
to affect a sort of multiplex... This is critical
for user experience to perform well with only 1
or two connections..
OK
It would if the presence of ETag was mandatory, and there was some
definition of the content such that the ETag could be compared by the
recipient to determine precedence.
further, be careful saying that "since the protocol is changing"..
No, the protocol isnt changing necessarily.
We are bound that all changes must be backward compatible.
Additionally, we are past the time for new features to be added
in the protocol. (at least in v1.1). You might look to the extensions
working group for a mechanism to plug in new features...
that list is ietf-http-ext@w3.org
Hmmm, OK.
As for backward compatibility, that is pretty subjective. In software
development, it is more easy (read more reliable) to implement a simple
protocol than one which is backwardly compatible and complex.
Cheers
Adrien
Josh Cohen josh@microsoft.com
Program Manager - Internet Technologies
Adrien de Croy - adrien@qbik.com. Qbik New Zealand Limited, Auckland, New
Zealand
See our pages and learn about WinGate at http://www.qbik.com/
In reality, it is easier and more reliable to deploy a protocol that
is backwards compatible. HTTP/1.0 was almost as complex as HTTP/1.1 --
the only big difference is that HTTP/1.0 was unable to accomplish what
it tried to do, whereas HTTP/1.1 is barely sufficient without making
incompatible changes.
Keep in mind that HTTP is intended for many more applications than
just the one that you are working on today. Even with all of its
apparent complexity, it is still possible to write a simple HTTP server
in just a few hours, and a simple HTTP client in a few days. The
complexity is only needed by complex applications, such as caching,
but failure to account for that complexity results in failed systems.
Kia ora,
....Roy
