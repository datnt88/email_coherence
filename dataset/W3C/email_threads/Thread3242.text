I think that some people will talk about it here.
So I think you have already read this entry of Mark Pilgrim.
about http://www.w3.org/TR/2002/WD-xhtml2-20021211/
and http://www.w3.org/TR/2002/WD-xhtml2-20021211/mod-text.html#s_textmodule
I remind people that:
1. XHTML 2.0 is still a WD
2. You send comments about this XHTML version to the mailing
list. Specs are also made by the public... if this public send
comments to the mailing list. The process of W3C has to reply to all
comments sent on the mailing list when they address specific issues.
"Public discussion of XHTML takes place on
www-html@w3.org (archive). To subscribe send
an email to www-html-request@w3.org with the
word subscribe in the subject line.
Please report errors in this document to
www-html-editor@w3.org (archive)."
I hope that Mark Pilgrim will send his comments to the www-html
mailing list too.
Thanks.
Karl Dubost / W3C - Conformance Manager
--- Be Strict To Be Cool! ---
Mr. Dubost, Why would you post this profanity riddled piece of
self-indulgence to a public mailing list?
Irrespective of the (microscopic)logic of the "argument", Mr. Pilgrim's
post is self-indulgent and immature; it should not be spread any wider
than Mr. Pilgrim's already apparently substantial following. If people
want to subscribe, perhaps out of amusement, to watch his lack of
self-control and verbal tantrums that's one thing. To have someone,
particularly a person representing the W3C actually post references to
Mr. Pilgrims emotional screed and further spread the noise is
inappropriate in the extreme.
...edN
I would like to thank Karl for posting [a link to] Mark Pilgrim's
thoughts on this list. While I am not particularly excited about Mark's
choice of words, I think he voices important concerns shared by many
people.
Indeed, W3C is having a hard time delivering on its promise of a
"future proof" markup, especially given the amount of new "things"
being introduced into the problem domain. Does the gap between W3C and
reality grow with every new draft? If yes, what can W3C evangelists do
about it? These are important questions worth discussing on this list
and elsewhere.
Thank you,
Alex.
HTTP performance - Web Polygraph benchmark
www.measurement-factory.com | HTTP compliance+ - Co-Advisor test suite
I think they are doing just fine. All the existing documents that have
been marked up semantically can be easily/automatically translated to
newer standards (either by rewriting the document itself, or within the
user agent). With documents using non-semantic markup, font tags, etc,
it would be vastly more difficult to understand the meaning of these in
terms of "new" semantics. I think perhaps what we need are more readily
available tools for upgrading documents to newer standards, perhaps even
as part of the working groups deliverables along with the specification.
And as far as the "future" is concerned... I think the web as we know it
today is just the tip of the iceberg. I think the decisions we make
today are going to be increasingly difficult to change in the future, as
these standards become vastly even more entrenched than they are
already. I am more concerned with the longer term outlook, even say 25
to 50 years down the road. Nobody is forcing you to upgrade to XHTML
2! I am, for one, pleased to see a fresh start using all that we have
learned. In the long term I think we will have a cleaner and more rich
base of documents to work with, and will look back and be glad we made
the transition now.
Chris Hubick
You are making an implicit assumption that a "document" is something
static that can be easily modified. I bet that most moderately-complex
sites today are not implemented using static files but are generated
using scripts/programs. Thus, in most interesting cases we are talking
about modifying _programs_ (server- or client-side), not "documents".
This kind of modification is not easy to automate at all and "tools"
will be of little help.
You are also probably making an implicit assumption that the majority
of existing documents have been marked up semantically (otherwise, it
would make little sense to justify W3C actions by talking about these
documents). I doubt that is true.
Based on my personal experience, it takes a lot of resources to keep
up-to-date with W3C, and it should not be this way if W3C cares about
small, resource-limited content "publishers".
What does "future proof" stand for? IMO, it means that if I use _any_
standard published by W3C, I should not be pummeled for not switching
to newer standards. I may miss on some cool new features, of course,
but I should not receive complaints that my site is not "standard" or
not "compliant". So far, the discussions on this list and other public
comments of Web authors make me think that W3C markup is not "future
proof". Moreover, there is no sign of the markup becoming more future
proof with every new standard published (see Mark Pilgrim comments,
for example).
I agree that we should try to look 10-25 years ahead. Unfortunately, I
do not see a _transition_ path. Most changes require a _switch_
instead of a smooth transition (unless you are willing to support N
standards at once, which is even more overhead for small guys). This
factor alone can jeopardize our best intentions and create a gap that
25 years of development will not cure. Think Cobol and Fortran, for
example.
We should not just imagine how things should work and then
write/enforce a standard describing our imaginary ideal! For example,
most people would agree that communism is great in its final
"everybody is free and happy" stage, but we all know what happens if
you try to jump into that stage without a proper transition. The
question is whether there is a transition path that ends in Markup
Heaven, and what that path is.
Thanks,
Alex.
HTTP performance - Web Polygraph benchmark
www.measurement-factory.com | HTTP compliance+ - Co-Advisor test suite
all of the above - PolyBox appliance
