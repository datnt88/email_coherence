Problem:
When a document is composite (text &amp; images) it is not nice to have to wait
for the whole document to load before display, but until the size of the images
is known this is tricky.
Non-Solutions:
1) Opening up multiple TCP/IP sessions. This is a kludge.
Solutions:
1) HTML+ allows the size of an image to be given in the text. We could imagine
some sort of "intall" utility to set up such info.
2) Using MGET we could imagine sending a resume of the images (size etc) before
starting the download. This could be sent in the message header
"image/gif; width=200; height=300; colours=256". Again this would
require some sort of install perhaps - or the server could
be intelligent and "know" about gifs, jpegs etc.
This is where I think that MGET is not quite enough, we would also need an
MHEAD.
I think that we need to enrich the content types to add in extra information
also.
Phill.
Very true. But it is kinda breathtaking and effective from the *client*
end.
This is exactly one solution being given by the same maker of this browser in
their "extensions to HTML" document. But I don't like this too much, as I
think it crosses the HTML structure-presentation boundary too far and isn't
trustable (the HTML author or install procedure could be wrong, the image
size could change over time, etc.)
I like this idea much more. I think the server could be configured to
recognize image files and return coordinates like width, height, and
color pretty easily - and it could cache that information for popular
images.
Brian
A general problem that I see in the proposals until now is that we have
no guarantee that the images are in fact on the same server as the main
document. Often this is _not_ the case and then it doesn't help to keep
the connection open nor is it easy for the server to get the size of
the image.
I think a general solution must be based on at least two connections.
First the main document gets retrived. If the client is text-based then
fine - no more connections are made. If not then the client can sort
the requests for inline images and make simultaneously (multi-threaded)
connenctions to the servers involved. These can then be multipart, MGET
or whatever solution we come up with.
This might not seem very elegant but I think it is necessary in order
to keep the flexibility and backward compatibility which in my opinion
is required.
Another solution could be that the client has a special header-line saying:
keep the connection open - I will tell yon when to close
but then I think it's another protocol than the HTTP we are heading for.
-- cheers --
Henrik Frystyk
The following assumes that the client gets the html document and
then sends the server a list of images to send next, reusing the
same connection.
I like the idea of being able to get the image sizes in advance of
the data, but would also like to be able to interleave the image
data streams. This way users see all of the images start to appear
concurrently, rather than one by one.
One simple idea is to use the segmented encoding approach and include
the stream number with the segment length. The initial info on image
size/type would specify the stream number for each image.
If this sounds too difficult, then at least the server could sort
the images by size and send the small ones first.
The following assumes that the client gets the html document and
then sends the server a list of images to send next, reusing the
same connection.
I like the idea of being able to get the image sizes in advance of
the data, but would also like to be able to interleave the image
data streams. This way users see all of the images start to appear
concurrently, rather than one by one.
One simple idea is to use the segmented encoding approach and include
the stream number with the segment length. The initial info on image
size/type would specify the stream number for each image.
If this sounds too difficult, then at least the server could sort
the images by size and send the small ones first.
Dave Raggett
Both excellent suggestions. I think the latter will result in better
performance on average or lower-end machines.
Brian
Actually, I'd dispute this. I bet we could get one of our web-crawler
authors to add to his crawling algorithm a measure of the ratio of
inlined-images-on-same-site to inlined-images-off-site, and that it
would probably be something on the order of 20-1, if not 100-1. We can't
guarantee it, and we certainly shouldn't set up a protocol that would
make inlining off-site images difficult or impossible, but forgoing
optimizations because of it is a bad choice, I think.
Right, this would be great, and I don't see how it contradicts other
proposals made here, it's just another parallel action.
Brian
