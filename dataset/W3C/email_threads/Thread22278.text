The new version of DOM (July 20) does not include a section for XML APIs. The old version (April 1998) does include a section for XML APIs and the XML APIs use some obsoleted classes (NoteIterator..) Do you know if a new version of XML APIs is coming soon or should we use NodeList instead of NodeIterator? Thanks for your time.
The new version of DOM (July 20) does not include a section for XML APIs.
The old version (April 1998) does include a section for XML APIs and the
XML APIs use some obsoleted classes (NoteIterator..) Do you know if a new
version of XML APIs is coming soon or should we use NodeList instead of
NodeIterator? Thanks for your time.

The XML APIs are now part of the "Core". There is a caveat in certain
interfaces that HTML-only products don't need to implement them (e.g.,
EntityReference).
NodeIterator was removed from the Level 1 spec; something like it will
return in Level 2. In the meantime, use NodeList.
Mike Champion
It's actually rather easy to implement TreeIterator using the various
parent, child, and sibling attributes of Node. It is also trivial to
implement a NodeIterator for the children of a node.
The thing that's difficult is to get an instance of the correct iterator
class from a NodeList; a generic iterator that just keeps track of an index
and uses "item" is likely to be extremely inefficient. However, I believe
there are very few places where you can't use navigation (and hence an
iterator) instead of using a NodeList.
The bottom line is that if you're working in a language that lets you define
your own classes (i.e. not JavaScript), you can simply implement your own
iterators and use them where they're appropriate. This will have the
additional advantage that if iterators _do_ come back, you won't be stuck
with whatever baggage the spec includes that your application doesn't need.
Stephen R. Savitzky Chief Software Scientist, Ricoh Silicon Valley, Inc.,
steve@rsv.ricoh.com California Research Center
home: steve@starport.com URL: http://www.starport.com/people/steve/
In such a way as to be robust against arbitrary adds, deletes, and
moves in the tree? Enlighten us, if you would.
John Cowanhttp://www.ccil.org/~cowancowan@ccil.org
You tollerday donsk? N. You tolkatiff scowegian? Nn.
You spigotty anglease? Nnn. You phonio saxo? Nnnn.
Clear all so! 'Tis a Jute.... (Finnegans Wake 16.5)
This points out _precisely_ the problem that the people working on the DOM
spec seem to have. You want node lists, iterators, and so on to do ``the
right thing'' in all possible circumstances -- deletions, additions, moves,
multithreading, and so on. The problem is that ``the right thing'' varies
with the use you're putting the thing to.
Nodelists want to be ``live'' when you're executing Javascript inside a
browser and you want to start executing the script while the document is
loading. Never mind that using getNextSibling would be more appropriate.
Under _all_ other circumstances, nodelists that behave like static arrays
are more useful. Under _all_ circumstances, node arrays are more efficient,
more predictable, easier to understand, and easier to implement.
Simple iterators that keep track of the current node and perform a
depth-first left-to-right traversal are useful, efficient, and predictable
when used for tree traversal, searching, and so on. They behave
_predictably_ when the tree is modified during an iteration; whether this
behavior is ``correct'' or not depends entirely on what the programmer has
been led to expect. Iterators that keep track of both the current node and
the ``next'' node in the traversal work better for deletion, but have to be
more careful about insertion. It's perfectly acceptable, in my opinion, to
state that the behavior of an iterator or a nodelist is undefined under
certain circumstances, or to provide multiple iterator types for multiple
purposes.
At this point I'm probably going to stop hoping that iterators get back into
the DOM at some point, and instead count on programmers to design
appropriate classes of their own that don't depend on whatever bizarre
single-application-specific behavior somebody decides to throw into the spec
because they think they know better than I do what my application wants to
do with a parse tree.
Stephen R. Savitzky Chief Software Scientist, Ricoh Silicon Valley, Inc.,
steve@rsv.ricoh.com California Research Center
home: steve@starport.com URL: http://www.starport.com/people/steve/
I'm not sure I understand the meaning of "correct" vs. "predictable"
here. If the implementation is understood to be single-threaded, then
all behaviors are "predictable" no matter how bizarre, since none of
the node operations do arbitrary computation. But even under a single-
threaded interpretation, various "obvious identities" do not hold:
(leftSibling(rightSibling(node)) = node,
unless node is a lastChild;
(rightSibling(leftSibling(node)) = node,
unless node is a firstChild;
(firstChild(parent(node)) = node,
if node is a firstChild;
(lastChild(parent(node)) = node,
if node is a lastChild;
(rightSibling(firstChild(node)) = 2nd child of node
provided there is one,
the algorithm "go to firstChild if there is one, otherwise
to rightSibling if there is one, otherwise to parent's
(grandparent's (great-grandparents (etc.))) rightSibling
until you reach the starting point" walks the descendants.
etc. etc. etc. All of these are false under the DOM.
The difficulty is that the DOM is not a parse tree according to the
meaning of the act; it is a tree-like API which provides access to
an underlying implementation that may or may not look like a parse tree.
John Cowanhttp://www.ccil.org/~cowancowan@ccil.org
You tollerday donsk? N. You tolkatiff scowegian? Nn.
You spigotty anglease? Nnn. You phonio saxo? Nnnn.
Clear all so! 'Tis a Jute.... (Finnegans Wake 16.5)
(NOTE: The casual reader will find the main point of this message in the
last two paragraphs.)
``Predictable'' means that it does what a programmer of average competence
would expect it to do given a quick perusal of the specification.
`` ``Correct'' '' (note that the _original_ was in quotes) means that it
does what the application writer requires it to do in order to make the
application work correctly.
I do not understand your point. They certainly hold as far as I can tell;
the classic algorithm for traversing a tree is:
traverse(node) {
visit(node);
if (node.firstChild != null) traverse(node.firstChild);
if (node.nextSibling != null) traverse(node.nextSibling);
That had darned-well _better_ work in the DOM. It's also trivial to
transform this into an iterator with "node" as its instance variable:
class depthFirst {
Node node;
Node toNext() {
if (node.firstChild != null) {
node = node.firstChild; return node;
} else {
while (node.nextSibling == null) {
node = node.parent;
if (node == null) return null;
node = node.nextSibling;
return node;
A competent programmer can easily look at this code, or a well-written
specification of it, and predict that it will behave oddly if the document
gets modified out from under it. It's even completely straightforward to
accurately predict exactly _how_ it will behave (for example, if I delete
the node, the iterator will return null on the next call to toNext).
Piling additional complication into the specification in order to ensure
that every node in the tree will continue to be visited no matter what gets
done between calls to "toNext", which I believe is what the last spec that
included iterators attempted to do, is WRONG, because it makes the simple
implementation impossible and because it becomes too complicated for a
programmer looking at the spec to guess how it's going to behave. It also
becomes next to impossible to specify _correctly_, because English is such a
lousy programming language (and predicate calculus is almost as bad).
If it quacks like a duck and waddles like a duck otherwise behaves like a
duck, then you can't tell that it isn't a duck. (I think it was Alan Kay
who said that.) The API is designed to have an obvious model that looks
like a parse tree. Any programmer, looking at that API, will ``see'' the
parse tree in her mind's eye and be able to make intuitive and accurate
predictions about how it will behave.
Moreover, any programmer who wants to implement parse trees in a portable
and standardized way will take a superficial look at the DOM, and decide to
use it as the API for their parse trees. They will then discover that, in
the details of the specification, the intuitive view of the DOM as the API
for tree-structured documents is WRONG, and that a great deal of non-obvious
machinery has to be added in order to make it work.
To the extent that the ``obvious'' (perhaps it would be better to use
``natural'') model fails to fit an actual implementation, the specification
is a failure. Things like live nodelists and the children of
EntityReference nodes (it would be much simpler if they didn't have any)
violate the natural model, and hence are bugs in the specification. One is
constantly having to say ``yes, this thing quacks like a duck and waddles
like a duck and otherwise fits the API of a duck, but in order to meet the
taste restrictions it has to be a turkey''.
I'm going to go a little further, and define ``natural model.'' The natural
model of an interface is a class in which all attributes are represented by
instance variables, and no other instance variables are present. In other
words, there are no ``hidden variables'' (in the quantum-mechanical sense).
There is a similar concept in mathematics.
My claim is that any specification for which the natural model is not a
valid implementation is either incomplete (hidden variables have to be
added) or incorrect (it will lead both users and implementors astray). It
is acceptable for a specification to be incomplete (e.g. a specification for
an iterator or a map _should_ leave out the implementation details), but it
is simply _not acceptable_ for the natural model or an extension of it not
to be a correct implementation of the specification. It's simply asking for
trouble.
Stephen R. Savitzky Chief Software Scientist, Ricoh Silicon Valley, Inc.,
steve@rsv.ricoh.com California Research Center
home: steve@starport.com URL: http://www.starport.com/people/steve/
The trouble with that algorithm is that it is recursive. It will
blow up if the tree is sufficiently deep. Indeed, in
languages that cannot be relied on to do tail recursion, like
Java, it will blow up if the tree is merely sufficiently wide.
Furthermore, if there is any end-of-node processing to do, such as
emitting an end tag indication, then the algorithm is no longer
even partly tail recursive and will blow up on both depth and
width even in safe-tail-recursion languages.
The algorithm I use in DOMParser, therefore, is non-recursive:
traverse(Node node) {
Node currentNode = node;
while (currentNode != null) {
visit(currentNode);
// Move down to first child
Node nextNode = currentNode.getFirstChild();
if (nextNode != null) {
currentNode = nextNode;
continue;
// No child nodes, so walk tree
while (currentNode != null) {
revisit(currentNode)// do end-of-node processing, if any
// Move to sibling if possible.
nextNode = currentNode.getNextSibling();
if (nextNode != null) {
currentNode = nextNode;
break;
// Move up
if (currentNode = node)
currentNode = null;
else
currentNode = currentNode.getParentNode();
Because of the reliability of this algorithm vis-a-vis the recursive
one, I believe it should be the standard way of walking DOM trees,
and therefore it is essential that DOM implementations make the
structural access methods fast.
John Cowanhttp://www.ccil.org/~cowancowan@ccil.org
You tollerday donsk? N. You tolkatiff scowegian? Nn.
You spigotty anglease? Nnn. You phonio saxo? Nnnn.
Clear all so! 'Tis a Jute.... (Finnegans Wake 16.5)
John Cowan wrote,
Even with a remove() operation on the iterator class, this
will still cause serious problems, because i will be
invalidated every time a node is removed via j.
Agreed. But other solutions are hardly better, since they
involve using remove operations with at best O(log n) behavior.
1) You can't win;
2) You can't even break even.
Yes you can ...
We just have ultra simple iterators, and *document* the
conditions under which they remain valid, and rely on
programmers being smart enough to know when those
conditions hold and when they don't, or being able to
ensure they hold (with manual updates or synchronization,
or whatever).
I you're worried about the 'naive scripters' ... well,
maybe they should just be warned away from iterators
... if they _really_ are as naive as all that (which
I doubt).
Cheers,
Miles
Miles Sabin Cromwell Media
Internet Systems Architect 5/6 Glenthorne Mews
msabin@cromwellmedia.co.uk England
