I think the current debate on priorities and policies in encoding
determination is out of place. The XML-lang spec currently:
a) suggests strongly that processors use any and all available
mechanisms to determine the encoding, and
b) offers the encoding declaration mechanism to provide authors a
place to put in-document information on this subject, should that
be helpful.
It is well-known that transcoding happens, that MIME types often
lie, and that this whole area is problematic. Also, since there is
no significant installed base of native Unicode documents, there is
no clear lesson from industry practice.
It is uncontroversially good that processors should use whatever
heuristics work in order to figure out document encodings. It is
controversial, but the result of a strong majority decision, that
XML-lang should include an in-band signaling mechanism.
I have hear any convincing argument why the spec should try to clean
up this mess single-handed, and why it should say one word more than
it does today.
Cheers, Tim Bray
tbray@textuality.com http://www.textuality.com/ +1-604-708-9592
I would like to have some mention of the possibility of using
meta-data in Appendix E. Something like the following would suffice:
I cases where it is possible to determine character encoding
from meta-data (for example, HTTP headers), this information
should be used. It is a reportable error for the meta-data and XML
declaration to differ.
Anything beyond this is unnecessary.
Tim,
I think that the role of a standard is:
a) to clearly define what is a conforming implementation, and
b) to enture that conforming implementations successfully
interwork.
Although it is useful to show suggestions for implementors, a) and b)
are the most important.
I do not think the current Part 1 fulfills a) and b). Let me show
an example.
Consider a proxy server that performs code conversion without rewritting
the PI. Consider a WWW browser or robot that does not understand XML.
Such browsers or robots certainly exist now and will not disappear in
the near future. If they save a transfered XML document in a file,
the header information will disappear and the PI will remain incorrect.
Then, an XML parser is likely to fail.
Now that implementations fail to interwork, something should be wrong.
I would like to know which one is wrong. The proxy server? The
WWW browser or a robot? The parser? Is this clear from Part 1?
Or, is this outside the scope of Part 1?
Makoto
Fuji Xerox Information Systems
E-mail: murata@apsdc.ksp.fujixerox.co.jp
Precisely why I way that we must rely on HTTP header. I'm starting to
think that Rick's proposal of requiring servers to remove the PI
is a good idea.
I have argued for treating the PI as a header a number of times, or
using an alternate header syntax. If we formalise it's role as a header,
then it should be possible to strip it without altering checksums of
documents etc. but unfortunately, it also include RMD and VERSION...
If servers remove it, it's better not to have it in the first place.
I agree, and your arguments were sound, but not I think understood.
Using MIME headers at the start of an XML file -- but _not_ at the
start of the corresponding SGML entity -- is perfectly Kosher SGML,
and would be a lot easier to deal with.
Another alternative would have been to have wrapped the XML document
in a simple XML envelope; with namespaces, this no longer pollutes the
document's namespace...
other stuff here.
Note that this is HTML-compatible, for servers such as CERN
that read http-equiv and put it out in headers...
Your document goes here
But I don't think that will happen.
Even so, I still think that
Encoding: iso8859-27
Content-type: text/xml
Your document goes here
is sensible, and has the advantage that if the server strips the
headers and puts them only in the http header, they needn't get lost,
but if they are passed through, that's good too.
This could be done even in the presence of the ?XML...? thing.
Lee
On Thu, 19 Jun 1997 16:37:24 -0400 (EDT) Gavin Nicol said:
How will relying on the external header fix matters? The problem is
that it is always possible to get a transcoding server that doesn't
understand the format it's transcoding (one reason sending binary files
via Bitnet was always such an adventurous experience if one of the nodes
involved was an ASCII site).
The best that can be hoped for is to have some chance at noticing that
there is a discrepancy -- particularly important given the frequency
with which transcoders garble the data (at least ASCII/EBCDIC
transcoders do -- perhaps the transcoders for CJK character encodings
work flawlessly all the time).
To do that, you need to have the PI retained.
-CMSMcQ
In the context of HTTP, the charset parameter on the Content-Type field
is the only thing that can be used to correctly detect the encoding.
Most receiving systems will be able to parse the PI and detect the
difference, sure. The problem is that the trancoding *server* cannot
stop them from getting false negatives unless it rewrites the PI. The
probability of HTTP being changed to require this for XML is
vanishingly small. I believe it to also be vanishingly small for any
MIME based protocol (including email).
Taking the failure cases and making them canonical doesn't remove the
problem: it just increases the number of failures.
On Mon, 23 Jun 1997 14:13:44 -0400 (EDT) Gavin Nicol said:
[quoting me]:
As has been pointed out (by me, last fall), even rewriting the MIME
headers is not always performed correctly -- especially in email.
My incoming email, on this EBCDIC machine, is full of MIME-encoded mail
claiming, in its MIME headers, to be in ASCII.
No external label is ever foolproof.
No internal label is ever foolproof.
Fools are just too doggone ingenious.
Under these circumstances, I don't see the point in *requiring* any
processor to prefer the internal to the external label, or vice versa.
Anyone who argues that one of these will always be right in cases of
conflicts must be living in a world rather unlike mine.
This is probably an argument against the proposal recently mooted, to
declare inconsistency between the internal and external labels a
well-formedness error. I'd be inclined to continue to define it as
an error, however -- but it should be an error from which it's possible
to recover.
-C. M. Sperberg-McQueen
Michael Sperberg-McQueen:
Are those systems conformant MIME implementations? Bad software is bad
software, and should be fixed.
Making it a well-formedness error would result in people ignoring the
spec. I believe.
How about removing this section of the specification:
An entity which begins with neither a Byte Order Mark nor an
encoding declaration must be in the UTF-8 encoding.
which effectively *requires* transcoding proxies to rewrite the PI, or
use UTF-8 (both of which are not likely to happen). Without this, at
least there is some appearance of equality.
I also dislike the specific recommendation of any value for the
encoding spelcification, and especially those related to Japanese. Can
we do without them, or simply state that they must be one of the IANA
names?
