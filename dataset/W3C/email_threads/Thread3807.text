Here are my thoughts on the topic of a test suite for our various
validators (including checklink by stretching the definition a bit).
I'm mixing the ideas of a test suite and test framework because I
believe the former must be designed precisely enough to allow us to
build the latter. If we don't fix this requirement for the test suite,
I guess a simple list of document (or something a bit more complex, as
Bj?rn just did).
We want a system that allows, in priority (use cases):
- a large (enough) collection of test
- (very easy) contribution of a test case with, ideally, each bug report
- testing of several instances of the validator + comparison of
different instances + comparison with expected result (so a fixed list
of validator-uri is a no-no)
- Classification of test. possibility to run only a subset of the suite
The main issue... Is to build a test suite for a test tool (duh). This
brings one (or even several) levels of complexity that I don't think
are too common in software testing (but I don't claim to be an espert
on the matter).
Let us just pretend for a moment that a test case is a document (I have
ideas to refine that, which I will explain later). Considering a test
case linked to the validation engine (as opposed to UI, formatting,
etc, which the system should also address). What's the expected result?
The result that the production validator gives, and which, for
consistency, the tested version is supposed to give, or is it the known
(or guessed) validity of the test case wrt a given DTD? This is not
such a complicated question, I guess, and once we make a clear
distinction between the expected (validity) and usual results, it is
possible to envision having the expected result "hardcoded" with the
test case, and keep former results (including the reference results
from the current prod tool) in EARL or whatever other format suits the
purpose (EARL looks quite good).
What would the system do (taken and completed from Bj?rn's list of
200402 meeting):
- read list of cases
- filter (keep only subset we want to test)
- run the tool
* get outcome (EARL format?)
* scrape results (in whatever output we want to test, usually XHTML I
suppose)
- Check UI [if included in the test description?] = escaping / URIs /
dialogs / proper output according to options (source tree etc.)
- Compare test result with expected outcome
- compare test result with known results from other instances
I gave an incomplete listing of UI tests, and the process assumes that
we also test the validation engine. I remember being told that testing
the core (openSP / libs) was not ours to do, and I guess to some extent
that is true, but I suppose a few tests could be used to check that at
least all these components are present (e.g that the lib is up to date
and that all the right DTDs are known)
From all the thoughts above I can start outlining how a test would be
described:
- URI of the tested document
- tools' options (to be appended/sent in a GET/POST)
- DTD used
- validity wrt DTD used
+ well formed (if XML)*
- long description (not fond of having the document describe itself)
(*)e.g v.w.o/dev/tests/xhtml-mathml2.html is well formed but not valid
(AFAIK)
All for now. Please give your opinion or provide additions so that I
can write a complete req. document, for the record.
thanks.
olivier
Le 09 f?vr. 2004, ? 20:44, olivier Thereaux a ?crit :
Do you mean a test case for HTML 4.01 is only an HTML 4.01 document,
like a Web page.
So here come the mess, when you have to test things like conflicting
encoding or conflicting mime-type.
An utf-8 document served as iso-8859-1
An XHTML 1.1 document served as text/html
These two cases can't be treated only as one document. It can be a
combination of two documents.
- .htaccess
- test-case-encoding-xhgn001.html
A way to have a trace somewhere that there's an additional information
for this test case is important.
Karl Dubost - http://www.w3.org/People/karl/
W3C Conformance Manager
*** Be Strict To Be Cool ***
Transfered to the QA wiki so that anyone can include their ideas -
we'll edit a formal requirement doc from that.
olivier
M12N of the code solves a number of problems in this regard, I would
expect that the resulting modules will have their own test suite in
the standard Perl make test framework...
