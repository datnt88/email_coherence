Could somebody please explain the difference between UTF8 and UTF16 to me
and why you would want to use UTF16 over UTF8?
Cheers
Ian
This email is confidential and intended solely for the use of the
individual to whom it is addressed. Any views or opinions presented are
solely those of the author and do not necessarily represent those of
SchlumbergerSema.
If you are not the intended recipient, be advised that you have received
this email in error and that any use, dissemination, forwarding, printing,
or copying of this email is strictly prohibited.
If you have received this email in error please notify the
SchlumbergerSema Helpdesk by telephone on +44 (0) 121 627 5600.
UTF-8 and UTF-16 are character encodings or transformation formats (the
terminology varies somewhat) for Unicode/ISO 10646. There's some basic
(though partly perhaps too technical) information on them in the Unicode
FAQ:
The encoding issue is not particularly related to accessibility, so some
other forum would probably be better suited for discussing the choice. But
there's an indirect accessibility impact, at least potentially. UTF-8 is
"IETF favored" encoding in the sense described in "IETF Policy on Character
Sets and Languages", RFC 2277, see e.g.
ftp://ftp.isi.edu/in-notes/rfc2277.txt In principle, the policy relates to
protocols, but it might be seen as a general recommendation to regard
support to UTF-8 as a prime objective. Thus, e.g. authors of assistive
software should give high priority to UTF-8 support. And we might read this
as a reason to favor UTF-8 over UTF-16, since a larger number of users can
be expected to have software that can handle UTF-8. The difference, if any,
is hardly a big one, but it's probably in that direction.
Jukka Korpela, senior adviser
TIEKE Finnish Information Society Development Centre
Jukka has ably answered the first part of the question. The second part, why
one would want to use UTF-16 rather than UTF-8, has two main answers.
The first is that it's easier to convert from UCS-2 to UTF-16, in fact
UTF-16 is exactly the same as UCS-2, they differ only when it comes to
character points outside of the UCS-2 range. UCS-2 is used internally in
some operating systems (Windows NT for example), and is the "natural" type
of character in some languages (VB, Java).
The second is that the way that UTF-8 encodes UCS results in shortening the
size of the bytestream when the characters are mainly from the ASCII range,
maintaining the same size when the characters are mainly from the range
U0080 - U07FF, and increasing the size when the characters are mainly above
character point U07FF. Hence for some languages UTF-16 may be more efficient
on bandwidth.
I find it hard to believe that their are many user-agents that can support
UTF-8 but not UTF-16, but maybe I'm putting too much faith in common-sense.
Certainly any browser that can process XML must be able to support UTF-16.
UTF16 uses two bytes per Unicode character (excluding the extension areas,
which use 4 bytes, but these shouldn't appear often).
UTF8 uses a variable number of bytes, such that American can be represented
in one byte, British requires two bytes, occasionally, Western European
languages require two bytes a lot of the time, and the rest of the world
needs three or four most of the time. It codes for the same set of
characters as UTF16.
UTF16 is much easier to handle for software writers and is more efficient
for world languages. Generally, world language aware software will
use UTF16 internally.
UTF8 contains all the characters needed for the language structure of
HTML in 8 bit characters, which are the same as those in ASCII.
For HTML, you can only legally use UTF16 if you include the charset
parameter in the real HTTP headers, as meta elements can't be detected
unless the character set is ASCII compatible. I'm not sure about XML;
it might recognize the Unicode byte order marks, used to signal UTF16.
Some browsers may sniff out UTF16, even when the HTTP headers don't
identify it.
Bogus confidentiality notice deleted.
Some Americans do have good English you know! Perhaps you have been
over-exposed to American TV and underexposed to their great tradition of
short-story writing?
All XML parsers can understand UTF-8 (and hence 8-bit encoded ASCII since it
is identical to the UTF-8 encoding of the same characters) and UTF-16. They
can all use the byte order mark to tell the byte-order of the UTF-16 and
they MAY carry out further heuristics to determine the byte-order in the
absence of a BOM.
If it doesn't do that it's not an XML parser; demand your money back if it's
commercial, demand your bandwidth back if it's freeware!
I'm referring to the pound sign (u+00A3). Apart from Microsoft smart quotes,
its the only reason why British internet users often need to go beyond ASCII.
This was meant to refer to HTML browsers, which should be assuming
ISO 8859/1. My guess is that IE with language detection (i.e. heuristics
for mislabelled character sets) installed may recognize UTF16 in HTML.
OK, the mist is clearing. But I'm still a little confused. Here's a section
from:
"ISO/IEC 10646-1 [ISO-10646] defines a multi-octet character set
called the Universal Character Set (UCS), which encompasses most of
the world's writing systems. Two multi-octet encodings are defined,
a four-octet per character encoding called UCS-4 and a two-octet per
character encoding called UCS-2, able to address only the first 64K
characters of the UCS (the Basic Multilingual Plane, BMP), outside of
which there are currently no assignments.
It is noteworthy that the same set of characters is defined by the
Unicode standard [UNICODE], which further defines additional
character properties and other application details of great interest
to implementors, but does not have the UCS-4 encoding."
So from this I understand that ISO 10646 is the basis for UCS4 and UCS2 and
Unicode just so happens to use the same value to represent the same
character points as ISO 10646 which is why we maybe use the terms
interchangably. Not usre what "but does not have the UCS4 encoding" means
though? Also that UCS2 is a subset of UCS4.
Again from the reference:
"UTF-16 is a scheme for transforming a subset of the UCS-4 repertoire
into pairs of UCS-2 values from a reserved range. UTF-16 impacts
UTF-8 in that UCS-2 values from the reserved range must be treated
specially in the UTF-8 transformation."
Not sure what the first sentence here means? Why only a subset and which
subset? And the reserved range? I read the last sentence to mean that each
UTF16 character representation uses a pair of UTF8 character representations
to represent each character point. But this doesn't make sense if only 2
bytes are used to represent each character in UTF16 or why UTF16 is more
compact than UTF8?
I'm sorry if I'm laboring the point (particularly as it only has a rather
tenuous link with accessibility as mentioned earlier - although language
support is clearly an accessibility issue and indeed it is in relation to
accessibility requirements I'm looking at) but I feel I'm so close to
actually understanding what's going on I just want to be absolutely clear
about it.
Also apologies if I've missed something. I seem to have had some problems
with my subscription because I've been merrily posting away to the list and
receiving replies to my own messages when they have had my address included
but nothing else. Thought things were a bit quiet!! I think I'm sorted again
now though.
Cheers
Ian
-----Original Message-----
UTF16 uses two bytes per Unicode character (excluding the extension areas,
which use 4 bytes, but these shouldn't appear often).
UTF8 uses a variable number of bytes, such that American can be represented
in one byte, British requires two bytes, occasionally, Western European
languages require two bytes a lot of the time, and the rest of the world
needs three or four most of the time. It codes for the same set of
characters as UTF16.
UTF16 is much easier to handle for software writers and is more efficient
for world languages. Generally, world language aware software will
use UTF16 internally.
UTF8 contains all the characters needed for the language structure of
HTML in 8 bit characters, which are the same as those in ASCII.
For HTML, you can only legally use UTF16 if you include the charset
parameter in the real HTTP headers, as meta elements can't be detected
unless the character set is ASCII compatible. I'm not sure about XML;
it might recognize the Unicode byte order marks, used to signal UTF16.
Some browsers may sniff out UTF16, even when the HTTP headers don't
identify it.
Bogus confidentiality notice deleted.
This email is confidential and intended solely for the use of the
individual to whom it is addressed. Any views or opinions presented are
solely those of the author and do not necessarily represent those of
SchlumbergerSema.
If you are not the intended recipient, be advised that you have received
this email in error and that any use, dissemination, forwarding, printing,
or copying of this email is strictly prohibited.
If you have received this email in error please notify the
SchlumbergerSema Helpdesk by telephone on +44 (0) 121 627 5600.
