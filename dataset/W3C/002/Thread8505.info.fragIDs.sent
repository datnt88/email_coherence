I've been mulling this over and studying the standards starting with the basic XML 1.0 standard. 
Probably lots of members of this WG are very familiar with XML processing but perhaps what I say below will be helpful for others... No canonicalization makes sense for binary things. 
Binary things, like images or executables, can reasonable be expected to be truly fixed. 
The Minimal canonicalization we are defining (canonicalize character set and line endings) makes sense for text. 
Text comes in a variety of character sets and not uncommonly gets its line endings changed from platform to platform. 
Of course, if something is handled as binary, even though its text, you can avoid canonicalization. 
But if it is going to be interoperably processed as text, you would want at least somthing like Minimal canonicalization. 
The basic process of reading XML and presenting it to an application (whether from an external file or a buffer in memory) is herent in any XML processing and is destructive as far as information goes that XML considers insignificant. 
Very explicitly, attribute value white space is normalized (unless the attribute is declared as CDATA). 
In particular, all leading and trailing white space is stripped from attribute values and all internal runs of white space are converted to a single space. 
While I found it explicitly anywhere, XML experts seem to take it as axiomatic that attribute ordering is insignificant and that white space between items inside start/end tags is insignificant. 
The XML Infoset says that a CR-LF is converted to an LF as is a CR not followed by an LF. 
There are additional areas where significance gets more murky, like white space between elements, which in XSLT for example, is stripped unless you have specifically declared it to be preserved. 
Namespaces are also a somewhat murky area but XPath and other specs treat a namespace declaration as distributing its information across all child nodes unless they are shielded by another namespace declaration with the same prefix. 
What this means is that if you have a hunk of XML like Elementz=" a, b, c " a="a"xmlns:Prefix="data:1234" 
A 1 /A B 
C Prefix:m="n" /B /Element and you did any XML processing with it, it would be nonconformant (in the presence of a DTD declaration other than CDATA) not to convert the value of the z attribute to "a, b, c". 
And if would entirely reasonable to get an internal representation which, if you output it, was something like or a variety of amounts of normalization between this and the input. 
All would be conformant to the XML rules. 
The above isn't the canonical printout according to the current W3C 
canonical XML proposal but will give you an idea of the normalization 
that can occur to the internal data structure just from reading XML for normal conformant XML processing. 
Sure, if you have XML but are treating it as binary data, you many not need any canonicalization. 
And if you have XML and treat it just as text, you may need only minimal canonicalization. 
But if you are going to process it as XML and want signatures over it that are interoperable, I don't see how you can escape the need for XML canonicalization. 
SignedInfo is XML, is signed, and I would think we would want those signatures to be interoperable. 
Thus I conclude that at least the default and quite possibly the fixed canonicalization for SignedInfo must be an XML canonicalization. 
Because we control the syntax of SignedInfo, we can make additional choices. 
Although I'm not proposing any decision at this time, we do not make any use of XML Comments in SignedInfo, for example, so if we decided that it was reasonable never to do so and if we also decided there was utility in allowing unsecured comments to be sprinkled into and removed from SignedInfo, we could specify a default XML canonicalization (or transform if you are worried about my use of the c14n word stepping on the toes of the W3C official c14n effort) that stripped out all XML comments. 
Thanks, Donald Donald E. Eastlake 3rd +1 914-276-2668 dee3@torque.pothole.com 
65 Shindegan Hill Road, RR#1 +1 914-784-7913(work) dee3@us.ibm.com 
Carmel, NY 10512 USA 
We have been over this many times. 
The ability to reconstruct signatures after a document has been parsed and 
regenerated was a dreadfull idea for ASN.1 leading to the mistake of DER. 
But for DER, ASN.1 would probably have never gained the reputation it has. 
This canonicalization algorithm you keep touting is exactly the type of baroque SGML boondogle that made invention of XML necessary in the first 
place. 
The chances of the canonicalization algorithm being generally implemented 
well enough to be relied upon and actually used for the purpose of parsing 
datastructures into databases and reconstructing them are zero. 
People had 
difficulty with PKCS#12 which is many orders of magnitude simpler. 
The fact 
that there is an argument going on on this list as to what the c14n form 
would be is a case in point. 
The processing overhead required to reassemble the attributes in order is 
significant, particularly when the object in question is a complex data structure generated from an XML schema, possibly incorporating several Mb 
of internal attributes. 
Systems are already being built based on the PKCS#7 detached signature format. 
If you insist on re-opening this issue yet again those systems will cease to be prototypes and be embedded production systems by the time the specification is complete. 
Needless to say the chances of someone taking a working production system 
and then introducing an unnecessary transformation to sort all the XML attributes into alphabetical order before verifying a signature is small. 
If you really MUST support the functionality proposed, canonicalization (which has a well defined meaning in computer science) is NOT the way to achieve it. 
Instead of canonicalization, specify a syntax constraint. 
This might state for example that the signed octets presented were presented with null space eliminated, tags fully epanded and attributes sorted into alphabetical order. 
The only functional difference between this approach and the one you propose is that the signature on the syntax constrained text would fail if it were to be passed through this 'noisy channel' that you keep claiming exists (although I have never known a noisy channel gratuitously re-order bytes). 
On the processing side such a syntax constraint would require minimal code, a few lines to check that each attribute was presented in order, no unexpected whitespace appearing etc. 
A syntax constraint is acceptable, a c14n requirement is not. 
If we continue to keep re-opening this issue I don't think consensus will be reached. 
Worse still, I predict that as with ASN.1 in PKIX there will be a steady stream of attempts to re-open the syntax issue and 'simplify', generally leading to entirely different syntaxes. 
Phill 
I've been following this group for a little while and was wondering whether the group will be putting out a book to help developers implement XML in their applications. 
Sam Brown Information Systems Manager UCLA Arthur Ashe Student Health &amp; Wellness Center (310) 206-6356 
Don, I strongly agree with both of your points. 
I definitely want to keep something like the My goal was to keep simplify the results of the canonicalization process by enforcing the use of the XML Signature default name space but, as Jim S. rightly points out, the W3C Canonicalization spec does not allow default namespaces. 
In a sense, I may have been trying to introduce some level of "syntax constraint" into the canonicalization process. 
Anyway I've been looking at James Clark's implementation of the W3C XML Canonicalization draft. 
In my view, if it is reasonable to expect that applications will have access to code that accurately implements the W3C XML Canonicalization spec, then we can 
consider using it for SignedInfo . 
However, if we expect that a significant number of applicatons will have to come up with their own canonicalization code, then we have to be wary of how complicated the canonicalization process becomes. 
Regards, Ed 
Don wrote... What you say is fine for the exampe you give but (1) people will want to embed stuff from other namespaces so we can't just drop all prefixes and namespaces. 
We could specify to drop the namespace is for the v1 standard) and its corresponding prefix, if any, leaving only other people's namespaces and their prefixes to be canonicalized but (2) thus far we have been going with the idea that, instead of a version number, an XML DSIG Version 2 (or 1.1 or 3 or ...) would be distinguished by using a different namespace. 
While I suppose we could still supress the XML DSIG v1 namespace, it doesn't really seem worth it to make such a special case when as soon as there is a v2 the namespace and prefix will have to spring back into presence in the canonicalized form. 
Donald 
To restate this point, the question is how "standardized" (how well does the spec read, how easy is it to write implementable/interoperable code from it) 
will this feature be, and do we need to place its standardization on the 
critical path. 
I feel more confident we can grapple c14n than we can Xpath/Xptr/XSLT-dereferencing-processing-model in the short term, however I don't believe either is an absolutely necessary feature that should be required. 
And the feature we are speaking of is I sign an XML document, it goes through numerous intermediate processors who may re-arrange the namespaces but otherwise don't change the content I signed, and my signature still works. 
This is very useful, but I don't think it is critical since we can orthogonally serve the community of people that don't need this feature sooner rather than later. 
Joseph Reagle Jr. Policy Analyst mailto:reagle@w3.org 
XML-Signature Co-Chair http://w3.org/People/Reagle/ 
I think it is beyond the charter of the WG to do a general book on implementing XML applications. 
It is possible that the WG could do a paper on implementing XML digital signtures but that is not currently planned. 
(see charter) Thanks, Donald From: "Brown, Sam" sbrown@saonet.ucla.edu 
Resent-Date: Mon, 25 Oct 1999 11:41:42 -0400 (EDT) Resent-Message-Id: 199910251541.LAA04953@www19.w3.org Message-ID: 1EF0C92BCCDAD2118FD40020480E269B01B0B17C@10eEXCHANGE.SAONET.UCLA.EDU "Donald E. Eastlake 3rd" dee3@torque.pothole.com 
, w3c-ietf-xmldsig@w3.org Date: Mon, 25 Oct 1999 08:37:33 -0700 
Hi Phil, From: "Phillip M Hallam-Baker" pbaker@verisign.com 
Resent-Date: Mon, 25 Oct 1999 11:13:55 -0400 (EDT) Resent-Message-Id: 199910251513.LAA04220@www19.w3.org Date: Mon, 25 Oct 1999 11:14:55 -0400 Message-ID: 000d01bf1efb$b1f96780$6e07a8c0@pbaker-pc.verisign.com 
Indeed, canonicalization appears to be a contentious area. 
I'm sure there will substantial future discussion on this topic. 
:-) 
The complexity stems from having multiple surface representations of the same internal data. 
Thus, with BER (Basic Encoding Rules) or some of the other encoding rule options for ASN.1, you can serialize the internal data in multiple ways and when the signer and verifier do so, as Murphy's Law predicts, the signture breaks. 
I don't think ASN.1 was desgined with digital signatures in mind at all but this separation between the internal data and the surface representation was a deliberate feature. 
If it were practical to always carry along a particular surface representation, there would never have been any pressure to design DER (Distinguished Encoding Rules) which provides a unique external serialization of the true internal ASN.1 data. 
Protocol uses of XML, such as IOTP, are always assembling new messages out of pieces of old message, some signed and some not. 
It is essential that such things be possible without breaking XML signatures. 
The same is true of protocols of more than trivial compexity that use ASN.1. 
For example, I have architected a SET implementation and coded it for some of the SET messages. 
(So my distaste for ASN.1 is grounded in hands-on experience.) In SET you are always tearing apart and decrypting and verifying and putting together and signing and encrypting stuff. 
It's kind of tedious but it all works fine as long as there is a canonical representation defined. 
I don't think DER had much effect on the reputation of ASN.1 but to the extent that it had an effect, it was to improve its utility and reputation. 
The canonical representation of ASN.1 provided by DER made interoperable digital signatures over ASN.1 data practical for non-trivial systems just as XML canonicalization is required to make interoperable digital signatures over XML practical. 
I don't think so. 
There were quite a few motivations for the design of XML but I don't believe they were particularly worried about digital signatures during that design process. 
As a result, they chose to provide flexible output formating. 
This improves readability, which is another goal of XML. 
As soon as the decision was made to permit multiple surface representations of the same true interal data, the requirement for XML canonicalization, to make XML digital signature practical, was born. 
However, with XML, they went even further. 
Not only are multiple surface representations permitted, but the XML standards require that some surface details, details which are insignificant to determining the true interal data represented, must not be passed to any XML conformant application. 
I find your use of the word "databases" odd. 
Implementation of DER so as to be interoperable does not seem to have been a big problem. 
I think there will be a number of interoperable open source implementations of XML canonicalization so I don't see a problem there either. 
The current W3C canonicalization form would work, as far as I can tell. 
There are some discussions in this WG of details or options for canonicalization. 
But most of the arguments seem to be pretty much on whether you should canonicalize at all. 
I think a lot of this is the conflict between the document view, which doesn't want to touch any bits, and the protocol view which believes that substantial intermediate processing of signed data is the norm. 
I do not agree. 
The number of attributes on an element is usually quite small. 
But even if there were, say, 100 attributes on an element, which is more than I have ever seen, the effort involved in treating them in a canonical order is trivial. 
I understand that Jim Clark's XML parser always gives you the attributes in order. 
The standard XML interfaces, DOM and SAX, parse start tags and the attributes therein and DOM gives them to you a leaves of the tree it provides while SAX gives you an arrary of attributes as part of the start tag event. 
I don't quite know what you are refering with with "several Mb of 
internal attributes" but I don't see any relevance of the size of the 
attribute values to the trivial effort of ordering them before you serialize them to feed to your digest function. 
It is assumed in XML that the start tag and its attributes are more or less one atom that you can hold and process. 
Large, complex data is normally content, not attribute value. 2 or 3 attributes is very much more likely than 20 or 30. 
Attribute values of a few tens of characters or less are very VERY much more likely than a megabyte attribute value, but, as I say, the length of the attribute value has almost nothing to do with the trivial effort of attribute ordering. 
Why do you believe this issue was ever closed? 
This is not a working group to rubber stamp the binary PKCS#7 format and declare it to be the XML signature format. 
Nor is it a group to design a new binary signature format that happens to be a character encoded binary format which looks like XML. 
The charter of this working group is to specify an actual XML syntax signature format. 
One consonant with the established XML standards. 
And it wouldn't hurt if it fit with the existing standard DOM interface and the de facto standard SAX interface either. 
These standards and interfaces make, for example, original attribute ordering and insignificant white space inside start tags not just insignificant but completely inaccessible to any XML application. 
I suppose there may be proprietary closed production systems that don't use any standard DOM or SAX derived interface and don't care about the XML standards. 
In my opinion, these are binary or text signature systems, not XML signature systems. 
If that works for them and they are happy, that's fine and perhaps they won't change. 
This doesn't seem to have much to with specifying an XML sytax signature standard, however. 
Yes, you could canonicalize at "print" time rather than at signature and at verification time, throwing away the readability and flexibility of XML. 
Of course, nothing that passes the result or verifies the signature could use SAX or DOM unless it was willing to re-canonicalize it (in which case it is unclear what you gained by canonicalizing at print time). 
They would have to use proprietary home-brew processing. 
"Noisy channel" is not a particular good description. 
I claim that we should assume that much of the time XML will be processing in accordance with the XML 1.0 standard and other standards and that much of the time the standard DOM and de facto standard SAX interfaces will be used. 
I think there is substantial evidence beyond my personal claim for the existence and use of SAX, the existence and use of DOM, and the existence and use of the adopted XML standards. 
The charter of this WG is to specify a digital signature in XML syntax. 
The standards for XML are such that, for anything even vaguely like our current XML digital signature syntax, XML canonicalization is required. 
I haven't the faintest idea why, in such a contentious area, you think this issue was ever closed in this WG. 
I would think that if we can 
not get to consensus on a fixed canonicalization algorithm for SignedInfo and can not even get to consensus on a default canonicalization algorithm for SignedInfo, it will just end up a mandatory to specify algorithm. 
And everyone actually doing XML processing as defined by the XML standards and implemented with DOM or SAX will specify an XML canonicalization, such as the current W3C canonicalization draft. 
I don't know what's going on in PKIX but in any area of active standards development there are normally a constant series of suggestions presented. 
I can't tell if you are claiming that adopting an XML canonicalization algorithm would lead to suggestions for change in the syntax of XML or suggestions for change in the syntax of XML digital signatures, but I don't see how either follows. 
I'm sorry that you don't like XML and wish it were a different animal than it is. 
But inherent in the adopted XML standards and in the 
standard DOM and SAX interfaces that are used to implement those standards are both the screening from any XML application of some insignificant aspects of XML input and the declaration that other aspects are insignificant. 
There is no problem with providing implementers with pre-defined options for using the XML digital signature format for more fragile non-XML signatures and the option of defining their own canonicalizations/transforms. 
But, in my opinion, the absence of a 
required to implement XML canonicalziation would mean the WG has failed in its charter duty to specify interoperable XML syntax signatures. 
Donald Donald E. Eastlake 3rd +1 914-276-2668 dee3@torque.pothole.com 
65 Shindegan Hill Rd, RR#1 +1 914-784-7913(w) dee3@us.ibm.com 
Carmel, NY 10512 USA 
Message-Id: 3.0.5.32.19991025154858.00b72360@localhost 
Date: Mon, 25 Oct 1999 15:48:58 -0400 From: "Joseph M. Reagle Jr." reagle@w3.org 
Subject: RE: XML and canonicalization 
Jim Clark has canonicalization code. 
I know there is canonicalization code in IBM that will work with any DOM and I believe that code will be made open source. 
I think there will be multiple interoperable open implementations of XML canonicalization. 
Nothing so complex is needed. 
For example, it is entirely conformant with the XML standards for any XML application to output, for readability or other reasons, an attribute value with leading or trailing white space, such Id=" foobar ". 
It is required for any XML application that is conformant with the standards to read that as Id="foobar". 
Thus, unless appropriate steps are taken, the simple act of printing XML by a conformant XML application and the reading of that XML by a second XML standards conformant application can break signatures. 
Donald Donald E. Eastlake 3rd +1 914-276-2668 dee3@torque.pothole.com 
65 Shindegan Hill Road, RR#1 +1 914-784-7913(work) dee3@us.ibm.com 
Carmel, NY 10512 USA 
I agree with Don's evaluation of the necessity of XML canonicalization. 
But one minor point about the conflict between the document view and the protocol view... If an XML document format containing digital signatures is to be read by more than one application by more than one vendor, the same issues arise as with protocol applications. 
As a workflow developer, I am of course thinking of the workflow case where a document might gain signatures from both (possibly third party) user applications and from the workflow engine as the document travels through a workflow process. 
At each step, the document will be parsed, modified (presumably in ways that shouldn't affect existing signatures), and rewritten. 
The null canonicalization would be death. 
Amusingly enough, I thought the issue was reversed: protocol applications wouldn't want canonicalization because of the overhead, while for document applications the overhead isn't significant. 
-Mark Bartel JetForm Hi Phil, Resent-Date: Mon, 25 Oct 1999 11:13:55 -0400 (EDT) Resent-Message-Id: 199910251513.LAA04220@www19.w3.org Message-ID: 000d01bf1efb$b1f96780$6e07a8c0@pbaker-pc.verisign.com 
Indeed, canonicalization appears to be a contentious area. 
I'm sure there will substantial future discussion on this topic. 
:-) 
and 
DER. The complexity stems from having multiple surface representations of the same internal data. 
Thus, with BER (Basic Encoding Rules) or some of the other encoding rule options for ASN.1, you can serialize the internal data in multiple ways and when the signer and verifier do so, as Murphy's Law predicts, the signture breaks. 
I don't think ASN.1 was desgined with digital signatures in mind at all but this separation between the internal data and the surface representation was a deliberate feature. 
If it were practical to always carry along a particular surface representation, there would never have been any pressure to design DER (Distinguished Encoding Rules) which provides a unique external serialization of the true internal ASN.1 data. 
Protocol uses of XML, such as IOTP, are always assembling new messages out of pieces of old message, some signed and some not. 
It is essential that such things be possible without breaking XML signatures. 
The same is true of protocols of more than trivial compexity that use ASN.1. 
For example, I have architected a SET implementation and coded it for some of the SET messages. 
(So my distaste for ASN.1 is grounded in hands-on experience.) In SET you are always tearing apart and decrypting and verifying and putting together and signing and encrypting stuff. 
It's kind of tedious but it all works fine as long as there is a canonical representation defined. 
has. 
I don't think DER had much effect on the reputation of ASN.1 but to the extent that it had an effect, it was to improve its utility and reputation. 
The canonical representation of ASN.1 provided by DER made interoperable digital signatures over ASN.1 data practical for non-trivial systems just as XML canonicalization is required to make interoperable digital signatures over XML practical. 
first 
I don't think so. 
There were quite a few motivations for the design of XML but I don't believe they were particularly worried about digital signatures during that design process. 
As a result, they chose to provide flexible output formating. 
This improves readability, which is another goal of XML. 
As soon as the decision was made to permit multiple surface representations of the same true interal data, the requirement for XML canonicalization, to make XML digital signature practical, was born. 
However, with XML, they went even further. 
Not only are multiple surface representations permitted, but the XML standards require that some surface details, details which are insignificant to determining the true interal data represented, must not be passed to any XML conformant application. 
implemented 
parsing 
had 
fact 
form 
I find your use of the word "databases" odd. 
Implementation of DER so as to be interoperable does not seem to have been a big problem. 
I think there will be a number of interoperable open source implementations of XML canonicalization so I don't see a problem there either. 
The current W3C canonicalization form would work, as far as I can tell. 
There are some discussions in this WG of details or options for canonicalization. 
But most of the arguments seem to be pretty much on whether you should canonicalize at all. 
I think a lot of this is the conflict between the document view, which doesn't want to touch any bits, and the protocol view which believes that substantial intermediate processing of signed data is the norm. 
is 
Mb 
I do not agree. 
The number of attributes on an element is usually quite small. 
But even if there were, say, 100 attributes on an element, which is more than I have ever seen, the effort involved in treating them in a canonical order is trivial. 
I understand that Jim Clark's XML parser always gives you the attributes in order. 
The standard XML interfaces, DOM and SAX, parse start tags and the attributes therein and DOM gives them to you a leaves of the tree it provides while SAX gives you an arrary of attributes as part of the start tag event. 
I don't quite know what you are refering with with "several Mb of internal attributes" but I don't see any relevance of the size of the attribute values to the trivial effort of ordering them before you serialize them to feed to your digest function. 
It is assumed in XML that the start tag and its attributes are more or less one atom that you can hold and process. 
Large, complex data is normally content, not attribute value. 2 or 3 attributes is very much more likely than 20 or 30. 
Attribute values of a few tens of characters or less are very VERY much more likely than a megabyte attribute value, but, as I say, the length of the attribute value has almost nothing to do with the trivial effort of attribute ordering. 
Why do you believe this issue was ever closed? 
This is not a working group to rubber stamp the binary PKCS#7 format and declare it to be the XML signature format. 
Nor is it a group to design a new binary signature format that happens to be a character encoded binary format which looks like XML. 
The charter of this working group is to specify an actual XML syntax signature format. 
One consonant with the established XML standards. 
And it wouldn't hurt if it fit with the existing standard DOM interface and the de facto standard SAX interface either. 
These standards and interfaces make, for example, original attribute ordering and insignificant white space inside start tags not just insignificant but completely inaccessible to any XML application. 
system 
small. 
I suppose there may be proprietary closed production systems that don't use any standard DOM or SAX derived interface and don't care about the XML standards. 
In my opinion, these are binary or text signature systems, not XML signature systems. 
If that works for them and they are happy, that's fine and perhaps they won't change. 
This doesn't seem to have much to with specifying an XML sytax signature standard, however. 
Yes, you could canonicalize at "print" time rather than at signature and at verification time, throwing away the readability and flexibility of XML. 
Of course, nothing that passes the result or verifies the signature could use SAX or DOM unless it was willing to re-canonicalize it (in which case it is unclear what you gained by canonicalizing at print time). 
They would have to use proprietary home-brew processing. 
"Noisy channel" is not a particular good description. 
I claim that we should assume that much of the time XML will be processing in accordance with the XML 1.0 standard and other standards and that much of the time the standard DOM and de facto standard SAX interfaces will be used. 
I think there is substantial evidence beyond my personal claim for the existence and use of SAX, the existence and use of DOM, and the existence and use of the adopted XML standards. 
The charter of this WG is to specify a digital signature in XML syntax. 
The standards for XML are such that, for anything even vaguely like our current XML digital signature syntax, XML canonicalization is required. 
I haven't the faintest idea why, in such a contentious area, you think this issue was ever closed in this WG. 
I would think that if we can 
not get to consensus on a fixed canonicalization algorithm for SignedInfo and can not even get to consensus on a default canonicalization algorithm for SignedInfo, it will just end up a mandatory to specify algorithm. 
And everyone actually doing XML processing as defined by the XML standards and implemented with DOM or SAX will specify an XML canonicalization, such as the current W3C canonicalization draft. 
I don't know what's going on in PKIX but in any area of active standards development there are normally a constant series of suggestions presented. 
I can't tell if you are claiming that adopting an XML canonicalization algorithm would lead to suggestions for change in the syntax of XML or suggestions for change in the syntax of XML digital signatures, but I don't see how either follows. 
I'm sorry that you don't like XML and wish it were a different animal than it is. 
But inherent in the adopted XML standards and in the 
standard DOM and SAX interfaces that are used to implement those standards are both the screening from any XML application of some insignificant aspects of XML input and the declaration that other aspects are insignificant. 
There is no problem with providing implementers with pre-defined options for using the XML digital signature format for more fragile non-XML signatures and the option of defining their own canonicalizations/transforms. 
But, in my opinion, the absence of a required to implement XML canonicalziation would mean the WG has failed in its charter duty to specify interoperable XML syntax signatures. 
Donald Donald E. Eastlake 3rd +1 914-276-2668 dee3@torque.pothole.com 
65 Shindegan Hill Rd, RR#1 +1 914-784-7913(w) dee3@us.ibm.com 
Carmel, NY 10512 USA 
Even Steve Kent can probably be made to agree that nobody knows of a system in which X.509 certificates are parsed into components and signatures stored separately and then rebuilt. 
Most of the real world certificate applications in the world generate DER encoding but accept BER encoding. 
In fact the number of faulty DER encodings is such that if applications were to depend on DER encodings they would break. 
I fail to discern any meaning in the above paragraph. 
For the generator the only constraint imposed by the proposed syntax constraint is that it would have to sign and deliver canonical form. 
'print time'? 
'proprietary'? 
I fail to see any connection to the proposal. 
When someone starts a post "I have been re-thinking this issue", that implies to me that they want to re-open it. 
However since you then proceeded to make exactly the same argument that you did the first time round it was not exactly 're-thinking'. 
If the issue is indeed 'contentious' then it looks a little odd if the Working group chair starts staking out such a committed interpretation of it. 
I take it then that we are agreed that there is not a consensus? 
Don, don't try to patronize me. 
I was using XML before it had a name. 
Phill 
Thanks Don, I have been looking at James Clark's canonicalization code (which is in C). Regarding IBM's XML Canonicalization code, could you 1. Find out which programming language it is in (I'm hoping for Java) 2. Confirm whether or not it will be made public on AlphaWorks (www.alphaworks.ibm.com). 
I don't need the source to be public, just the API and library (or JAR). 
Of course, having the source too would be great. 
3. If it will be made public, about when will that be. 
Ed 
I hope to have this information this Friday. 
Donald From: Ed Simon ed.simon@entrust.com 
Resent-Date: Wed, 27 Oct 1999 15:59:17 -0400 (EDT) Resent-Message-Id: 199910271959.PAA28013@www19.w3.org Message-ID: 01E1D01C12D7D211AFC70090273D20B101C4A8DB@sothmxs06.entrust.com 
Date: Wed, 27 Oct 1999 15:58:02 -0400 
Phil, From: "Phillip M Hallam-Baker" pbaker@verisign.com 
Resent-Date: Wed, 27 Oct 1999 14:35:48 -0400 (EDT) Resent-Message-Id: 199910271835.OAA26286@www19.w3.org Date: Wed, 27 Oct 1999 14:37:00 -0400 Message-ID: 001e01bf20aa$417b6c00$6e07a8c0@pbaker-pc.verisign.com 
I never said a word about certificates. 
I had nothing to do with the PKI or certificate part of the SET software I was working on. 
When ever my code got down to certificates or CRLs or BCRLIs, I just called code designed and implemented by someone else which did all the certificate parsing and key caching. 
In my mesage I talked only about messages in SET, a reasonable exmple of a complex protocol but using ASN.1 instead of XML. 
Multistage tearing apart and sometimes decrypting and sometimes verifying parts of requests and multistage building and signing and encrypting merchant to bank gateway requests including pieces of the cardholder to merchant messages and reply messages includes pieces of request message, etc. 
The creating, clean binary channel handling, and processing of certificates is probably much more like a document application than a protocol application as I have been using those terms. 
Sorry, I'll try to clarify it. 
"print" = deliver. 
Right, with your proposal, you need XML canonicalization at the signer. 
See above re print time. 
As far as I can tell, the definition of XML and the actual SAX and DOM interfaces that almost everyone uses will sometimes break your signer imposed canonicalization and thus break the signatures unless canonicalization is also implemented at the verifier. 
For soemthing to preserve the canonicalization, it would have to use some specially crafted parser, which I don't think it is entirely unjustified to call "home-brew". 
There were arguments over this previously in this WG. 
There wasn't a consensus on whether a fixed canonicalization algorithm was reasonable or what the defualt canonicalization algorithm should be for SignedInfo. 
As long as there isn't a WG consensus, the question is open. 
Under these circustances, it seems reasonable for me to think about it a bit more. 
You quote is wrong. 
See http://lists.w3.org/Archives/Public/w3c-ietf-xmldsig/1999OctDec/0148.html . 
I said "I've been mulling this over and studying the standards starting with the basic XML 1.0 standard." 
I see no reason why mulling over and studying shouldn't end up confirming a prior opinion. 
Interpretation? 
I have a technical opinion. 
You have one also. 
Consensus? 
At this point, I think there is a rough consensus that Null, Minimal, and at least one XML canonicalization and application specified canonicalziations should be optionally available for data. 
But there isn't a consensus on whether canonicalization can be fixed, and if so at what, or defaulted, and if so to what, for SignedInfo. 
Donald 
FWIW, I believe we need to default to a particular canonicalization (on both the signer and verifier side.) The problem just doesn't look that tough to implement to me (attribute ordering, namespace substitution, etc.) Sure, some vendors will screw it up (an hopefully we won't screw up the spec), but applications can certify and recommend word processors, parsers, etc. for their purpose, just as they do with browsers now. 
Signatures either pass or fail, so certification should be relatively straight forward (it would be nice if a standards body tackled certification.) Of course, an application should be able to specify null or other canonicalization for its isolated (thus insulated) purpose. 
My problem is with the larger world. 
In the scenario I hope to be dealing with in a few years, an attorney would use a word processor to create and digitally sign an XML document for submission to the court. 
As a federal court, we don't believe we should dictate that an attorney use a particular word processor for a filing. 
We can, however, require that they follow reasonable standards (for example, a document that conforms to a particular DTD.) On the court side, I want to be free to choose or switch DOM-based XML parsers without fear of breaking all the signatures (incidentally, it would be very painful to use the "source" stream for this purpose, that is, DOM + some dual hack.) I have little faith that signatures will remain viable cross-platform without at least minimal canonicalization. 
Note also that it doesn't make sense to me to tell attorneys they must set a particular "canonicalization mode" in their word processors, if there ever is such a beast. 
Thanks, Rich 
