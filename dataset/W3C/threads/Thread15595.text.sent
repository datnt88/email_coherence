Hi, All: My action items were to summarize and suggest resolutions for checkpoints 1.4 and 1.5. 
I will separate them into two emails. 
This one is dedicated to Checkpoint 1.4. 
I will send out 1.5 tomorrow. 
I actually found the sorting of all issues related to these checkpoints rather difficult and confusing (probably partly due to being a first time Bugzilla user). 
It seems like some issues have already been addressed, and maybe bugs just need to be closed? 
Anyway, here's my best shot: [INTRODUCTION] Checkpoint 1.4 from WCAG 2.0 Internal Draft (17 November 2003) currently reads: "All text can be decoded into words represented in Unicode." Success Criteria Level One: 1. text in the content is provided in Unicode or sufficient information is provided so that it can be automatically mapped back to Unicode. 
Success Criteria Level Two: 1. abbreviations and acronyms are clearly identified each time they occur if they collide with a word in the standard language that would also logically appear in the same case (e.g. all caps). 
2. symbols such as diacritic marks that are found in standard usage of the natural language of the content, and that are necessary for unambiguous identification of words, are present or another standard mechanism for disambiguation is provided. 
Success Criteria Level Three: None [OUTSTANDING ISSUES] 1. 
In a current proposal, CKW and Greg Gay suggest that checkpoint 1.4 success criterion 1 and 2 from Level 2 be combined with checkpoint 3.2, and read something like the following: 1. abbreviations and acronyms are clearly identified each time they occur or are available in a glossary that is explicitly associated with the content. 
2. symbols such as diacritic marks that are found in standard useage of the natural language of the content, and that are necessary for unambiguous identification of words, are present or another standard mechanism for disambiguation is provided. 
Also, Harvey Bingham suggests a forward reference to Checkpoint 3.2 in the "Benefits of..." section, since they both deal with acronyms. 
from CKW re-write, and bugzilla #607 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=607 , #442 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=442 , and #375 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=375 2. The re-wording proposal of Checkpoint 1.4 suggests changing from "All text can be decoded into words represented in Unicode." to "For text, use fonts that can be represented in Unicode." 
From Ben Caldwell, Bugzilla issue #606 3. Bugs #375 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=375 and #606 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=606 seem to be duplicates - a suggestion to close one over the other has been made. 
4. Bug #608 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=608 suggests re-wording for the level 2 success criterion: Current wording for Best Practice Measures for Checkpoint 1.4 1. abbreviations and acronyms are clearly identified each time they occur if they collide with a word in the standard language that would also logically appear in the same case (e.g. all caps). 
(See also checkpoint 3.1) [I#341] 2. symbols such as diacritic marks that are found in standard usage of the natural language of the content, and that are necessary for unambiguous identification of words, are present or another standard mechanism for disambiguation is provided. 
Proposed wording for Best Practice Measures for Guideline 1.4 1. abbreviations and acronyms are clearly identified each time they occur if they are identical to a word in the document's language that has a different meaning. 
(See also checkpoint 3.1) [I#341] 2. symbols such as diacritic marks that are found in standard usage of the natural language of the content, and that are necessary for precise identification of words, are present, or another standard mechanism for clear identification is provided. 
[SUMMARY OF ISSUES] 1. 
It seems that there is some confusion about the tone of this checkpoint overall. 
Although it intends to apply to "disambiguating language," its references to acronyms and abbreviations make it appear related to Checkpoint 3.2 (hence the CKW suggestion to merge the two together). 
The questions to be answered then are: (a) What separates 3.2 from 1.4? 
(b) Are the differences enough to merit a separate checkpoint? 
(c) Are there parts that merit a separate checkpoint and parts that should be merged with 3.2? 
(d) If so, what are the parts? 
2. Re-wording at all levels of the success criterion is needed for clarity. 
[PROPOSED SOLUTIONS] 1. 
The level 2 success criterion should either be clarified as being different from the requirements in checkpoint 3.2, or should be merged with checkpoint 3.2. 
This would leave the Unicode part of checkpoint 1.4 on its own. 
2. Re-wording, as proposed in bug #606 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=606 should be adopted. 
3. Re-wording for success criterion 1 and 2 of level 2 should be adopted whether or not those success criterion are merged with Checkpoint 3.2. 
(#608 http://trace.wisc.edu/bugzilla_wcag/show_bug.cgi?id=608 ) Cheers, -Kerstin Sorry if my timing is bad, I just noticed this. 
See notes below... 
I think a better way of saying this might be "All text can be decoded into words, representable using Unicode characters." 
I take it that this refers to Arabic and Hebrew? 
If so, 'standard usage of the natural language of the content' is probably not appropriate, since diacritics in these scripts are not used in standard usage. 
This isn't an accessibility issue in the first place. 
You simply have to declare your character encoding (either in HTTP headers or the meta element or both). 
Unicode, while desirable, is not the only encoding that works, and for some languages (like Vietnames, Thai, Chinese), the "legacy" encodings work better in real-world browsers. 
Could proponents of the current wording please point to people with disabilities alive and using the Web today who experience barriers or inaccessibility because some encoding other than Unicode was used? 
Joe Clark | joeclark@joeclark.org 
Author, _Building Accessible Websites_ Here's an example: this checkpoint applies to PDF files that use fonts without ToUnicode tables, so there is no way to determine which character is represented by a glyph. 
I don't think the checkpoint is trying to outlaw other encodings; only to make sure that the identity of the characters is unambiguous. 
Is there a better way to phrase this to make the distinction clearer? 
On the one hand, the clarification suggested by Richard, to wit quote cite= "http://www.w3.org/mid/005a01c3efe0$05c168e0$cd01000a@w3cishida" 
All text can be decoded into words, representable using Unicode characters. 
would make it clearer (it's already true with the existing language) that duly annotated use of Big5 etc. conforms. 
On the other hand, what we should really do concerning Unicode, encodings, etc. is, as Joe hints, defer to the Character Model; with a note-in-draft to confirm it gets to Rec before we do. 
We can remind people to observe it, but should not abridge it in text that comes across as free-standing requirements imposed by *this* document. 
This requirement is important for access by PWDs but it's not our problem alone and there is a document chartered elsewhere (i18n) to address exactly this topic. 
So if we want to claim credit for "plays well with others" we had better avoid re-inventing their wheel. 
Al My guess is that the intent was to say something along the lines of "represented in the Unicode *character set*". 
Unicode/ISO 10646 is the document character set for HTML 4.0 and XML, meaning that although any encoding can be used, characters used must be found in Unicode to fit the reference processing model. 
(See Joe, I'd be quite interested in any specific information you have about issues when representing Thai and Chinese in Unicode. 
I'm not aware of any. 
RI From: w3c-wai-gl-request@w3.org 
[mailto:w3c-wai-gl-request@w3.org] 
On Behalf Of Joe Clark Sent: 10 February 2004 19:32 Subject: Re: Summaries of issues around checkpoints 1.4 and 1.5 
