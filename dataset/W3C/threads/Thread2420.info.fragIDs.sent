I was hoping to polish this proposal a little more before floating it externally, but alas, with the meeting on Monday, time did not permit. 
I hope that I have at least stated my perspective well enough to stimulate discussion. 
Problem statement: The existing HTTP authentication model does not allow authentication realms to be distributed across servers. 
To protect user credentials. 
HTTP browsers associate each realm with a single IP address, and will not pass user credentials to multiple servers even if they claim to belong to the same realm. 
This security measure, built into the browser, has created the undesirable side effect of requiring users to re-type their user names and passwords for each protected server within a multiple server site. 
Abstract: A proposal is made to: Allow the distribution of protection realms across servers Protect user credentials from "imposter" servers which claim to belong to a realm but do no Pass user credentials securely to content servers which identify themselves as members of those realms To re-use user credentials throughout protection realms so that users are challenged for user name and password only once within the context of a single session. 
Create secure, trusted relationships between servers Centralize authentication, authorization, and directory services for one or multiple websites Centralize directory security Simplify or eliminate directory services on distributed content servers Make it scalable I propose that this new authentication scheme be named "remote authentication". 
Theory of operation: When access is first attempted to a content page which is protected by remote authentication, the browser is redirected to the "remote authentication" server for that realm. 
This redirection should be done via SSL for security. 
The user is then challenged for user name and password by basic authentication. 
The server then encrypts the user name and password with a secret (symmetric) key and returns the user name and password to the browser where they are cached for the session. 
The browser is then re-directed to the original content server and the browser passes the encrypted user name and password to the server. 
The content server, which shares the same secret key as the authentication server, is able to decrypt the user name and password. 
When the browser is challenged by another server which claims to be in the same realm, the user credentials are served. 
If the server is trusted, it will share the same secret key as the authentication server and the user name and password will be decrypted. 
If the server is an imposter, decryption will fail and the user credentials remain secure. 
The way I see it, the browser will associate each realm with an encrypted user name and password, which are simply opaque strings. 
In addition, I would like the browser to allow the server to cache an additional string of arbitrary length to pass state to the other servers in the realm. 
It could be used to store additional authentication and authorization semantics, such as an expiration time for the authentication, authorization information such as a list of groups to which the individual belongs, an index to identify the identity of the encryption key, etc. 
This information could be signed with an MD5 MAC or encrypted or just plain cleartext. 
I think that we could do the web a great big favor if we eliminated the need to replicate directories to content servers. 
The authentication piece is easy, but if we can solve the authorization part I believe that we could simplify the common registration puzzle at the same time. 
Thank you for your support, -e 
I think that the spec for "domain" is broken -- it specifies a list of URIs, but doesn't say that these can be _prefixes_ of URIs that may also use the same credentials. 
Without that, it is pretty uselss, IMHO. 
From: Scott Lawrence[SMTP:lawrence@agranat.com] Sent: Friday, December 05, 1997 10:53 AM Subject: Re: Proposal for new HTTP 1.1 authentication scheme Digest authentication already includes a mechanism (the 'domain' attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to specify that credentials may be used on multiple servers, and through the 'digest' attribute allows for mutual authentication. 
There is also the model of Kerberos to consider - developing a ticket-based authentication scheme (with the advantages and problems of any third-party mechanism) would be another area to explore. 
Could the spec be fixed without interoperability trouble emerging? 
(Query to digest implementers???). - Jim 
Most of the suggestions by Paul and Dave seem to be clarifications of the original intent. 
They should not cause problems. 
The one significant change is Paul's suggested change of the algorithm for calculating the "entity-digest". 
If implementations exist they will be incompatible. 
I don't think I would describe this as fixing a "bug" in the entity-digest algorithm. 
It might be an improvement though. 
On the other hand, if I recall correctly it was Paul who wrote the entity-digest algorithm, so he may have a right to call it a bug. 
John Franks john@math.nwu.edu 
I still feel my one objection about proxy-added headers is substantive and unresolved. 
Briefly, an origin server might omit headers that get figured into the entity-digest calculation. 
A proxy might subsequently add those headers. 
The client sees a message *with* the headers, calculates an entity-digest that figures them in, and gets a different answer from what the origin server calculated. 
Dave Kristol 
I agree that there is an issue here. 
The current spec says the proxy MUST not add these headers. 
If I recall you suggested the MUST be changed to SHOULD. 
I am not sure how this helps beyond making the proxy technically "legal." 
It doesn't materially affect the problem. 
What should a proxy do in this situation? 
It seems it must either not add headers or break the entity-digest. 
John Franks john@math.nwu.edu 
Ummm... I think my "MUST - SHOULD" had to do with a proxy's changing the content of headers. 
I think I see the words to which you're 
referring (end of p.13), and they mention Content-Length explicitly but don't mention Date. 
And there's a potential problem with Content-Length: suppose a proxy eats chunked data and wants to create a complete entity *with* Content-Length. 
Is it hereby forced to forward the entity as "chunked" because it's forbidden to add Content-Length? 
I agree it's a dilemma. 
An option is to require that clients send Content-Length and (perhaps) not Date, and forbid proxies to add either within this context. 
Dave Kristol 
Sorry, my mistake. 
...snip.. 
Here is what the spec says: The entity-info elements incorporate the values of the URI used to request the entity as well as the associated entity headers Content-Type, Content-Length, Content-Encoding, Last-Modified, and Expires. 
These headers are all end-to-end headers (see section 13.5.1 of [2]) which must not be modified by proxy caches. 
The "entity-body" is as specified by section 10.13 of [2] or RFC 1864. 
The content length MUST always be included. 
The HTTP/1.1 spec requires that content length is well defined in all messages, whether or not there is a Content-Length header. 
I was remembering "which must not be modified by proxy caches" as "which MUST NOT be modified by proxy caches." 
I guess I don't see a problem with this. 
On the question of length it says the content length must be used in the digest even if there is no Content-Length header. 
This seems fine and should cause no problem if a proxy unchunks a chunked response. 
The server has to calculate the MD5 digest of the entity so it will not be much harder to calculate the length. 
I guess the proxy better get the length right or the client better do its own length calculation and not trust the proxy. 
As for Date, I guess the only problem is servers with no clock. 
They don't send a Date header. 
Draft-v11-rev01 says A received message that does not have a Date header field MUST be assigned one by the recipient if the message will be cached by that recipient or gatewayed via a protocol which requires a Date. 
So it seems that it is fine for the proxy to forward the dateless response as long as it does not cache the entity. 
It is unlikely that an authenticated response should be cached anyway. 
Maybe there are problems I don't understand. 
John Franks john@math.nwu.edu 
Studying the specification some more I see there seems to be some ambiguity about the meaning of Content-length. 
Here are some quotes: 7.1 Entity Header Fields Entity-header fields define optional metainformation about the entity- body or, if no body is present, about the resource identified by the request. 
entity-header = ... 
Content-Length ; Section 14.14 
7.2.2 Length 
The length of an entity-body is the length of the message-body after any transfer codings have been removed... But later we have 14.14 Content-Length The Content-Length entity-header field indicates the size of the message-body, in decimal number of OCTETs, sent to the recipient... 
These seem inconsistent. 
If Content-Length means the length after transfer encodings have been applied then it is hop-by-hop and not end-to-end. 
It also cannot be an entity header as described in 7.1. 
There probably is also a need for a header meaning entity-length. 
Personally I would like to see Content-Length remain an entity header. 
All the other Content-* headers are entity headers and apply to the entity before transfer encoding. 
One way to do this would be to introduce a new "Transfer-Length" header with the stipulation that its default value is the Content-Length. 
The Content-Length would be defined as it is now in section 7, i.e. the entity length. 
Thus the Transfer-Length header would only be needed when the message length and entity length differed. 
This would give us consistent terminology (Content-* for entity, Transfer-* for message). 
It would also not break any current of which I am aware. 
At present the only widely deployed TE is chunked and it needs neither header. 
If new TEs arise which need to have the message length specified they would have to use Transfer-length (or both). 
I see no alternative other than rewriting the specification to make Content-length a hop-by-hop general header and not an entity header. 
The authentication specification would also need to be modified since it is not possible to put Authentication-Info in a chunked trailer as it is currently defined if Content-length is the length of the chunked message. 
John Franks 
From: David W. Morris[SMTP:dwm@xpasc.com] 
Sent: Friday, December 12, 1997 2:34 PM 
If a message is sent on a persistent connection using a transfer-coding that does not exactly preserve the length of the data being encoding, then the "chunked" transfer-coding MUST be used, and MUST be the last transfer-coding applied. 
Sounds like a complete solution to me. 
Of course, I think there might still be a few words about content-length to bring into alignment. 
There's a problem -- if no one implements any transfer coding other than identity or chunked, then we don't have the necessary two implementations to go to Draft. 
If they do, then I'll be they don't follow this rule -- they probably believe that Content-length is the length of the message body, not 
the entity-body. 
I also don't like having to impose chunked when it isn't needed. 
If a cache recieves a .txt 
file, and gzips it for later use in serving it to clients, it perfectly well knows the length, and can send it out with a TE of gzip and a Content-length (or Transfer-length, if we want to introduce that and get it implemented twice). 
Paul 
From: David W. Morris[SMTP:dwm@xpasc.com] 
Sent: Friday, December 12, 1997 2:34 PM 
If a message is sent on a persistent connection using a transfer-coding that does not exactly preserve the length of the data being encoding, then the "chunked" transfer-coding MUST be used, and MUST be the last transfer-coding applied. 
Sounds like a complete solution to me. 
Of course, I think there might still be a few words about content-length to bring into alignment. 
I would take that bet ... we've had multiple implementors report that they thought 'content-length' was entity length, not message length. 
NONE who reported the converse. 
Can't speak to implementations ... but imposing chunked is an almost ZERO overhead operation. 
NO MORE overhead than adding a transfer-length .... probably less: 3039[crlf] encoded message content [crlf] 0[crlf] Nothing about chunked encoding requires more than a single chunk. 
This also cleanly covers the case where the encoded length (as in compressed) is unknown UNTIL after encoding is complete. 
Just use multiple chunks. 
Dave Morris 
