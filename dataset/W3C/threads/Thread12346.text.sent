We will have a meeting today to identify the tasks for the face to face meeting. 
Please come with a list of gaps and to do items for the Techniques and Evaluation documents. 
The time is 2:30 Boston time, the number is +1.617.252.7000 
Thanks Jutta Hello all, I've been looking at the techniques document for gaps but in terms of content it seems very strong. 
I think what we need to work on is organization. 
The following ideas might increase the readability, and therefore the usefulness, of the document: 1. 
Some of the techniques are really requirements to fulfil the checkpoint (techniques for which we can not think of another way to do something, ex. all techniques starting with "ensure"), while others are merely suggestions (just one of several ways to do something). 
I think we should pull these apart by, at least, placing them under different headers for each checkpoint. 
The requirements are also the things that we'll be asking evaluators to directly check for in the evaluation process. 
2. I'd prefer not to see very similar techniques under different checkpoints. 
Instead, a multiply appearing technique could be placed under the most relevant checkpoint and other checkpoints could have a section of links to the checkpoint under which the technique now appears. 
(Ex. 
for 1.1) 3. Not everything needs a sample. 
Just those things in which we are suggesting a particular interface functionality that might need more explanation. 
4. There should only be one reference section per checkpoint and it should be distinguishable from the other techniques. 
Cheers, Jan Jan Richards Access Software Designer Adaptive Technology Resource Centre University of Toronto Sorry, I was talking a lot so they're pretty rough. 
Jan Richards William Loughborough Jutta Treviranus Fred Barnett Lou Gerard (MS) Gregory Rosmita JT: We need to decide on tasks and find gaps. 
JR: Jan explains his message sent just before meeting. 
JT: Questions? 
WL: It is a good idea to break out the core stuff and then have the techniques for different languages, etc. THere would actually be 2 docs. 
JT: That's new. 
We used to just talk about different views. 
WL: Look at WL's checker (explains it's use). 
Propose the same for toole evaluation project. 
So labels are "if your are designing a table editor" instead of "are you using tables". 
WL: In tool evaluation version we would put different kinds of tools for each row. 
JT: Will make the document more readable. 
JR: Might be a problem if one doc is required. 
GR: Not a problem if its just a search tool. 
WL: It's just a big time saver. 
JT: We do have to set out usable document structure. 
JT: OK what face to face tasks are there. 
JT: 1. Identify redundant techniques? 
WL: Positive comments JT: 2. Clean up classifications for each checkpoint. 
JT: 3. Cleaning up the examples so they are really examples. 
WL: We need examples from other companies. 
JR: What about MS's blue underline. 
LG: We do some stuff. 
Already sent examples. 
JT: OK, we also need to determine the views. 
JR: So that's basically asking what types of tools there are. 
JT: Yep. 
JR: Maybe we should do it as authoring components that can be built up into different type of tools. 
WL: That fits in with my tool. 
JT: Other issues? 
JT: What about the evaluation document? 
WL: We need a big table that specifies who needs to look at what areas. 
So the user only needs to look at the things applicable to the tools being tested or evaluated. 
JR: that's done for image. 
JT: Would also like to extend evaluation structure to the other content types that we are evaluating for. 
JR: Some of the tests in the structure are based on relative P checkpoints so they should extend across all contnet types. 
JT: We need to ensure that we have all content types agreed upon. 
JT: anything else to do? 
WL: We only have a day and a half. 
JT: OK that's it. 
We have lots to do. 
Jan Richards Access Software Designer Adaptive Technology Resource Centre University of Toronto 
