From: "Paul C. Kocher" pck@best.com 1. 
Does pre-encryption prevent MACs from being encrypted? 
8. Features are Optional Pre-encryption, pre-MAC-ing, ans password authentication are all independent options that are (typically) server configuration choices. 
Pre-encryption selection will require significant changes to how the ciphersuites are negotiated, since some can support pre-encrypted data and others probably won't be able to (i.e., Fortezza). 
At the meeting Paul was extremely apologetic that the IETF process had broken down and that much TLS work had recently occurred via private email and/or voice mail; both Paul and Win agreed to fix the process. 
Paul, thank you for posting portions of Bennet's comments. 
Bennet, I hope you will consider discussing protocol features publicly on the TLS list instead of in one-to-one email. 
My (incomplete, but perhaps better than nothing :-) notes from the TLS working group meeting in Montreal last week are as follows: 1. 
Should the TLS working group use SSL 3.0 as a starting point, or define a list of requirements and a universe of candidate protocols (SSH, PCT, Hannah, others?) which satisfy some or all of the requirements? 
The question was posed but never definitively answered - my impression was that there was consensus for using SSL 3.0 as the baseline, but to give more than just lip service to features of other protocols (such as SSH's ciphersuite-guessing quick startup) that are missing from SSL. 2. Timing - the real need to have a proposed-standard RFC out in a short timeframe (by Dec 1996) was acknowledged, but many of those present were averse to the idea of having the IETF merely rubber-stamp a vendor proposal. 
Jeff Schiller noted that the requirements for proposed standard are fairly loose - the protocol must be well-specified to allow independent implementations, but no actual implementations are required. 
SSL and SSH both appear to exceed the requirements for specification quality. 
There is a 6 month to 2 year timeframe between Proposed- and Draft- Standard status, and a minimum of 4 months between Draft- and (full) Standard. 
Christopher Allen was among those strongly in favor of quickly defining a TLS 1.0 protocol, delaying agreement on contentious elements until a 1.1 version. 
I don't have a strong objection to this, as long as there is a clear understanding and agreement by the commercial sponsors that additional requirements *must* be resolved by the WG before progressing to Draft. 
The tradeoff is between early adoption of a Proposed standard and quick progression to Draft; the total time for resolving contentious issues will probably be the same either way. 
3. Specific Features: a) Improved set of error alerts: no disagreement on the need for this, no discussion of specifics. 
b) Password authentication: (from pck:) Two issues we need to decide: - Whether to support password authentication at the TLS level. 
(Type 2 can be done today by applications using SSL 3.0) I think everyone here wants to see a certificate-based infrastructure get developed as quickly as possible, but it isn't clear whether giving people an alternative to certs is good (because it helps them make the transision to certs more smooth) or bad (because it gives them a way to avoid switching to certs). 
- Whether passwords have to be protected from brute force attacks. 
I don't give much weight to meta- (or religious) arguments that passwords are good or bad for the proliferation of public key authentication; the issue is how password support impacts the protocol (in terms of cryptographic strength, difficulty of analysis, ease of development, run-time performance, backward compatibility, etc.). 
The purpose of password support is to provide authentication-quality protection, as opposed to exportable-confidentiality-quality, protection for static (reusable) passwords. 
In the absense of export controls, there would be no need for special protocol support for password protection - they could just be sent like any other application data. 
Dan Simon's presentation leads me to believe that modifying SSL 3.0 to support password protection (by defining two additional handshake messages, but not requiring any changes to the record layer) can be done without negative impact on security or performance. 
If the feature is not used, there is no change to the bits-on-the-wire. 
There is apparently strong commercial demand for this feature. 
My previous opposition to password support was based on a misunderstanding of what it accomplished; I now have no objection to including it in the TLS protocol. 
There was no poll on password support taken at the WG meeting. 
Paul's 3 approaches: 1) use the standard encryption key - i.e. treat passwords like any other data 2) use a value derived from the master secret to hash passwords 3) give the master secret to applications 1 is the "do nothing" option; 3 is unpleasant because it precludes the possibility of isolating the crypto subsystem from unwashed, untrustworthy applications. 
Option 2 is preferred - the password authentication secret should not be the master secret or the MAC secret directly, it should be derived from the master secret in the same way as the rest of the "independent" keying material. 
c. Pre-encryption. 
A poll was taken on this topic: precisely 2 of the members present were in favor of pre-encryption support, a minority of those present were willing to at least listen to arguments in favor of pre-encryption, and a majority were strongly (Win used the word "absolutely") opposed. 
That seems like more than rough consensus - although a follow-up should be taken here on the list to confirm. 
My objection to pre-encryption is that even when it is not used, it requires potentially damaging changes to the MAC calculation (replacement of the inner hash key with a fixed value). 
Aesthetic objections to "layering violations" aside, I don't believe it's wise to trade off protocol strength for server efficiency. 
And since Web containers being developed for other purposes (document protection independent of the transmission channel) have the side effect of providing the same efficiency gains, there is little reason to try to make pre-encryption "invisible" to clients by trying to disguise Independent Data Unit Protection as session protection, weakening the session protection in the process. 
d. Modularity (separation of the record layer from the handshake layer): This was not discussed by Win, Paul, Netscape or Microsoft, but there was strong grass-roots support from the floor (including Eric Rescora) that TLS should be designed to accommodate multiple keying mechanisms. 
Unfortunately, the IPSEC working group was unable to achieve consensus on a key management protocol at this time, but Jeff Schiller set a deadline of 1 September for the ISAKMP and SKIP sponsors to come to agreement, otherwise the IESG will make the decision. 
If an IETF key management protocol is defined, TLS should be able to use it. 
I am strongly in favor of an independent record layer, for reasons discussed earlier on this list. 
Bennet Yee also made some excellent comments regarding the requirements a keying API must satisfy. 
e. 
Negotiated or fixed hash algorithms - little discussion, no resolution. 
f. Dedicated or standard tcp port numbers - ditto. 
g. Name of the protocol - ditto, although like Phil Karlton and Paul Kocher, I (david P. Kemp) think "PK" has a nice ring to it :-). 
Hi David, Paul's reply-quoting was of a message that I sent to the IETF mailing list and CC'd to Paul. 
Indeed, that message started off with I'd like to make some comments on the technical side of what was discussed at the IETF mtg, and repeat some points made there here, both to clarify and add emphasis to the points and to make sure they go into the WG email/web archive. 
Now, -I- never got a copy of my own email from the mailing list, but the message did make it into the TLS web archive -- see http://lists.w3.org/Archives/Public/ietf-tls/ . 
Maybe the mailing list software is buggy -- but my message that Paul quoted was certainly -not- one-to-one private communications. 
Anyway, don't blame me for keeping things private -- additionally, the result of the BOF at Palo Alto was that I was to write up some stuff on pre-encryption and pre-MACing for inclusion in a draft document that Paul would put together for public comment; others would similarly contribute on different topics or write separate white papers, also for public discussion. 
The PA meeting notes were emailed to this list and appears in the TLS archive at w3.org. 
As it turned out, I was slow about it (was away at a conference right after the PA BOF) and Paul decided to write it up himself (rather than wait for me) based on a phone conversation and an email message. 
This was, as far as I could tell, all more-or-less according to what was decided at the Palo Alto meeting -- to have various people contribute and produce a draft for public discussion at Montreal. 
You might argue that the procedure that was decided upon at the PA mtg was flawed, but.... 
Also, please make the distinction between pre-encrypted and pre-MACed data. 
They have very different security properties. 
I discussed the differences in my previous email message to the list, and will clarify some other issues in a subsequent message. 
-bsy Web:http://www-cse.ucsd.edu/users/bsy/ 
USPS:Dept of Comp Sci and Eng, 0114, UC San Diego, La Jolla, CA 92093-0114 Hi Paul, I wanted to clarify a technical point for the list. 
Paul's reply to my point: bsy's point: The second is slightly subtler. 
For oft retransmitted data, re-encrypting them under different keys, especially weak, 40-bit keys, for many transmissions provides attackers with more partial information about the plaintext. 
If there is only one encrypted version of the data that is always re-sent, less partial information about the data is leaked. 
TLS/SSL and other cryptosystems using 40-bit keys always include a nonce or salt with the key to prevent birthday-paradox style attacks against messages encrypted with more than one key. 
While having multiple copies of the plaintext encrypted under different keys could theoretically help for some kinds of cryptanalytic attack, this sort of data could be collected using a chosen plaintext attack as well (which the protocol must be able to resist). 
The point that I was trying to make is a generic one that applies to all ciphers, regardless of the use of IVs. 
It's also why I made a reference in the next paragraph: ... (The German nursery rhyme weakness. 
[Enigma cryptanalysis history.]) 
The idea that I'm trying to get across is related to the idea of unicity distance from classical cryptography. 
Unicity distance for a cipher relative to a data source is defined as the number of (symbols/bytes) of ciphertext that an attacker must intercept in order to determine the key (and plaintext) in an information-theoretic sense. 
It's called "unicity distance" because it's the point passed which where there would be a single interpretation of the messages -- there'd be only one possible value for the key -- whereas before that multiple keys are possible. 
You can find a definition of unicity distance in most cryptography texts. 
Some examples: For known-plaintext scenarios, the amount of ciphertext for DES is basically one block or eight bytes. 
You know the input and output to DES, and that (should) information-theoretically completely determine the key, which gives you any other plaintext encrypted with that key. 
I mumbled "should", because we don't really know for sure that there exists only one key which gives the particular input-block-to-output-block mapping. 
For known-plaintext scenarios with a stream cipher, it's basically the number of output symbols which information-theoretically allows you to completely determine the internal state of the stream cipher (call this number N). 
With me so far? 
Okay. 
I said earlier that the idea of unicity distance is relative to a source. 
What this means is that the unicity distance depends on what you know about the source -- the generator for the plaintext. 
In the known-plaintext case, we had complete knowledge of the source, and we had a relatively easy time figuring out (roughly) what the unicity distance is. 
What happens if the source is completely random? 
If the source is truly random, the unicity distance is (typically) infinite. 
This is obvious, since given the ciphertext any key could have been used to encrypt plaintext that produces the ciphertext. 
I mumbled "typically", since you -could- have a cipher that isn't globally one-to-one -- that is, the family of encryption functions (selected by the key) is one-to-one, but the range set are not identical for all of these functions, so the family of encryption functions is viewed as having a range that's the union of the ranges of the individual encryption functions, and range coverage might tell you the key after an enormous number of messages. 
(This is a weird case that people don't typically worry about, but I want to be careful w/ my informal definitions here.) Now, if the source is English text or an HTML document, the unicity distance is (obviously) something between 8 bytes and an infinite number of bytes for DES (according to Schneier, for English it's 8.2 bytes; according to Meyer and Matyas, it's 14.6 bytes [probably different models of "English"]), and between N and infinity for a stream cipher. 
In practice, we don't worry -too- much about unicity distances, since it's an information theoretic measure, and we care more about the amount of resources required to break a cipher (time &amp; space) -- this is the more modern cryptography approach, esp with scalable (public key) ciphers. 
Note, however, that the idea of unicity distance makes intuitive sense -- the more you use your cipher system with a fixed key, the more partial information you're going to leak. 
And eventually, a powerful adversary may be able to make use of that partial information to figure out your key. 
(Compare this with cryptanalytic attacks such as Differential Cryptanalysis, where even though you know the plaintext, all you care about is the xor sum of pairs of them -- which is critical knowledge about the source that's used.) Now, back to the point. 
I argued that if we had a fixed message, retransmitting it repeatedly using different keys (and IVs -- this idea is indepent of IVs and birthday attacks), we leak partial information about the data every time we retransmit it. 
This is in some ways the dual of the idea of unicity distance -- and since I haven't seen it defined elsewhere I'll call it "multiplicity distance": given a secret message (vs secret key in unicity distance) of some fixed length, the multiplicity distance of a cipher is the number of re-encryptions of that message under different keys/IVs that an adversary requires to information theoretically determine the text of the message. 
We don't bother with a model of the source of keying material, since keys are supposed to be truly random (though in -practice-....) This model can, of course, be enriched to have the multiplicity distance be a function of the entropy of the random data source as well (e.g., truly random, random English-/HTML-generators etc). 
So. 
What's the multiplicity distance (with an HTML source) for RC4? 
For DES? I don't know if anybody in the public sector knows. 
I don't. 
If we can avoid re-encrypting data, however, we should. 
-This- is my argument why pre-encryption helps security. 
There. 
Oh, yeah. 
The German nursery rhyme reference. 
That was a reference to the breaking of the Enigma system used in WWII -- the operators would often send each other nursery rhymes before the actual message to make sure that the two machines were synchronized w/ the appropriate (daily or whatever) key. 
This was repeated plaintext chosen randomly from the space of popular German nursery rhymes, and aided the Allies in the cryptanalysis of the actual messages. 
We think/hope that RC4 is stronger than Enigma, of course, but.... -bsy p.s. Maybe it's the lengthy tomes that I tend to write, but the mailing list software does not seem to be forwarding my messages to the list (conspiracy theories, anyone?:). 
In any case, until I hear that the problem's been fixed, I guess I'll periodically resubmit this tract until I get a copy myself. 
Web:http://www-cse.ucsd.edu/users/bsy/ 
USPS:Dept of Comp Sci and Eng, 0114, UC San Diego, La Jolla, CA 92093-0114 I never saw your original either. 
--- from Rodney Thayer rodney@sabletech.com 
+1 617 332 7292 --- Bennet, I heartily apologize for accusing you of being an email conspirator. 
I did look back at my TLS folder to find your messages and didn't see them, but even if the mail list were working perfectly, I could have scanned and forgotten the content and mis-filed the messages myself. 
I hope the delivery problems can be identified and resolved - especially since the messages are showing up in the archives. 
Sorry again, dpk Hi David, No prob re email conspiracy accusation. 
Apparently there was a problem w/ the w3.org mailing list system over the weekend.... On to the technical stuff: Pre-encryption does *not* require that you replace the inner hash key -- pre-MAC'ing does. 
We can have pre-encryption without pre-MAC'ing. 
Or pre-MAC'ing without pre-encryption. 
These are *independent* ideas, and I fear that many of the IETF attendees missed this fact. 
Web containers may be useful for other reasons, but unless there's a protocol hook there is no efficiency gain, since the containers will have to be sent encrypted (so plaintext data is doubly encrypted). 
If the containers are pre-encrypted data and the key simply is transmitted over the TLS-provided channel, then that key will only have an effective key-length equal to the minimum of the length of that key and the length of the TLS-channel key. 
Depending on how much fixed headers there are in the container format, it may or may not help to increase the multiplicity distance (it changes the entropy in the source, but w/o container specs it's hard to tell), and if the container-key is transmitted as TLS-channel data, this is still an important consideration. 
Now, maybe I misunderstood and you want to send the container over an unencrypted channel, but have the associated key sent over a separate, TLS-protected channel? 
In that case, the above comment about the effective key strength still holds; it also complicates the data delivery, in that two communication channels (with different security properties) are required -- and it's not just a single click anymore (though I suppose you -could- have the container include an URL to the key as a hack, and have the MIME helper talk to your browser for the key fetch [ick].) Anyhow, I have been arguing that pre-encryption, unlike pre-MAC'ing, does NOT necessarily weaken the protocol. 
(It of course would allow weak systems with bad configurations which permits pre-encryption with ROT-13 over an otherwise 56-bit DES encrypted channel; but I'd rather ignore stupid configurations -- I assume that the pre-encryption strength is acceptable to both sides.) Note also that the multiplicity distance concept applies to public key systems as well, with some suitable modifications to the definitions to move away from a strictly information theoretic viewpoint. 
Hastad's low exponent attack against RSA may be viewed within such a framework -- low exponent (e.g., 3) RSA encryption has a multiplicity distance (in messages rather than bytes) equal to the value of the exponent, since chinese remaindering of that many ciphertext messages permits message recovery. 
Paul, as for your comment about the possibility that the pre-encryption key delivery mechanism might break, this was why I originally argued for the use of a strong cryptosystem for pre-encryption key delivery (aka key management cipher) in lieu of an ad-hoc hash-based stream cipher. 
As long as the pre-encryption keys are -not- made available to the client program (and the whole point was transparency), I don't see it as a real export control problem -- but I have never played an export control lawyer on TV. (This is still certainly better than the IPSEC proposed ESP transforms re ITAR.) -bsy Web:http://www-cse.ucsd.edu/users/bsy/ 
USPS:Dept of Comp Sci and Eng, 0114, UC San Diego, La Jolla, CA 92093-0114 Not that I'm confident that we can properly design pre-encryption into TLS right now, but if we are going to there is one thing I'd like to add to the discussion. 
If there is a protocol hook made, it should also be available in such a form that it is compatible with the idea of compression. 
The option for compression exists in the current draft (though no specific compression specs have been proposed yet) and you don't want to try to compress a pre-encrypted container. 
..Christopher Allen Consensus Development Corporation.. .. ChristopherA@consensus.com 1563 Solano Avenue #355.. .. Berkeley, CA 94707-2116.. ..Home of "SSL Plus: o510/559-1500 f510/559-1505.. .. Security Integration Suite(tm)" http://www.consensus.com/SSLPlus 
.. 
