With the Host: header in HTTP/1.1 it is possible to efficiently and easily make a single host act as many "virtual" Web sites. 
What I'd like to see in HTTP/1.2 is the ability to easily and efficiently make multiple hosts act like a single virtual Web site, so that one can build huge web sites. 
For example, could be the root of the whole, HUGE, publically accessible corporate web of the MegaSoft Corp. 
The mapping of this corporate web onto physical Web Servers should be flixible and mutable over time -- as the site grows, more servers need to be added; and as the content changes, the load on various pieces will change too, necessitating rebalancing the content across the servers, even if more aren't needed. 
It is _almost_ possible to do this in HTTP/1.1 today. 
The 302 (Moved Temporarily) response can be used to automatically redirect someone who did a GET on html to by including a Location: header with that URL, where, say, server42 is set up to handle all requests that start with (said setup being a server implementation issue, not a protocol issue) so that the GET is transparently redirected to server42. 
I said "almost" above, because the 302 is only allowed by the 1.1 spec to work automatically for GET and HEAD requests "since this might change the conditions under which the request was issued". 
For other requests, the user has to be prompted first. 
In order to make a multi-host virtual site, this needs to be automatic for all requests. 
Another factor that makes it an "almost" is that even if 302 was alow to work automatically for all requests, it wouldn't be efficient enough to scale well -- every request would come in to www.megasoft.com's 
server to be redirected. 
This would both load www.megasoft.com, 
and cause the client to suffer an extra round trip. 
(The 302s can have an Expires: to allow them to be cached, and that would help, but there would be an awful lot of them.) The way to make it efficient is to somehow note that the 302 applies to _all_ the resources with a particular prefix, and only cache this one thing. 
A third factor is that, in order to scale, it is sometimes necessary to replicate the content on multiple servers. 
However, 302 responses can only contain one URI (and hence server) in the Location: header returned to specify where the redirect is to go. 
(Arguably, there is a way to do this with the DNS, so I'm flexible on this.) I would like to add an option response header that says that it is OK to redirect all requests with a certain prefix of the Request-URI to a list of other places. 
(It's a new response header instead of a new status code for backwards compatibility. 
Old clients will just ignore it and prompt on methods other than GET and HEAD). 
A possible syntax: Referral= "Referral" ":" prefix 1#referral-URI prefix = absoluteURI referral= absoluteURI The prefix has to be a prefix of the original Request-URI -- this prevents spoofing. 
The response to this is to take everything after the prefix in the original requestURI, append it to one of the referrals to get a new URI, and retry the request using the new URI. 
If there is a Cache-Control or Expires header, then it can be cached, and used for subsequent requests for resources that have the same prefix -- thus getting the client directly to the correct server without an extra round trip. 
This proposal is nothing more than the application to HTTP of the tried and true referral mechanisms of distributed directory and file systems. 
Comments? 
Redmond, WA 98052 From: Jeffrey Mogul[SMTP:mogul@pa.dec.com] 
I consider it potentially quite important that Paul's proposal allows more than one referral-URI. 
This not only allows replication for performance, it also allows replication for fault-tolerance. 
The use of multiple DNS bindings, for example, doesn't help much if one of the actual servers is dead (or your path through the Internet to that server is broken). 
On the other hand, Paul's description ("append [the suffix of the original URI] to one of the referrals") might not be sufficiently well specified. 
Agreed. 
I promised Larry to get some indication that I wanted this out within a week of the IETF, so not every detail was perfect. 
Of course with more time it would have been :-). 
I.e., if a client chooses one of the referral-URIs, and then after a few successful transactions that server seems to die, should the client re-negotiate with the original server? 
Pick another referral-URI? 
Keep trying? 
The behavior of the client in this case has some potential implications for the failure semantics, especially if the client is updating a database at the server end. 
Good point. 
Yes. 
I was thinking about this too. 
I didn't want to complicate the original proposal. 
Thus, a referral would include not only a new URI, but a maximum age ("time-to-live" in DNS terms) and a cost metric. 
I thought the TTL would be in Expires header for the cached 302 response in which this was returned. 
You probably want to spread the requests out among the referral-URIs in proportion to the cost; otherwise everyone will jump on the lowest cost site and it will get overloaded. 
It turns out this is exactly was the proposal for SRV RRs in DNS does, which is why I dithered about having whether it was better to use DNS. 
Also, there's another proposal for location RRs in DNS (I think the name is LOC) that include longitude and latitude -- not perfect, as all the backbone suppliers tell me, but certainly good enough to pick a server on the same continent in preference to one on a different continent. 
If it makes sense to have them in the DNS, it makes sense to have them here, too -- or to just use the DNS. 
The problem with the latter is that SRV and LOC aren't standards, I don't know if they will become standards, and even if they do it takes a while to get them into the "official" DNS implementation. 
Maybe a DNS guru can comment? 
Or a friend of one can forward it along? 
I'd hate to duplicate the functionality. 
We could perhaps get rid of the expiration (delta-seconds) parameter if we made it explicit that a referral lasts only as long as the Cache-control: max-age of the response that carries it. 
But you need an expiration mechanism of some sort, or else these bindings are impossible to revoke. 
Right. 
Expires or max-age seemed like a good thing to re-use. 
We might want to modify that client algorithm so that "unexpired referral-info" includes "which has not been unresponsive recently". 
I.e., if a server is playing hard-to-GET, drop it from the list. 
All good suggestions. 
I wanted to see if there was enough interest to produce a real I-D. 
Sounds like there is. 
Any suggestions received to date will be incorporated; as will answers to as many objections as I can -- including future ones received as I'm writing, so keep them coming. 
Thanks, Paul I'm not a DNS guru, but here's my two new pence's worth anyway... 
The docs in question are: draft-gulbrandsen-dns-rr-srvcs-03.txt &amp; RFC 1876, by the way. 
LOC is supported in BIND 4.9.4, which is the current production release. 
SRV has been slated for BIND 4.9.5, which is meant to be coming out later this year. 
One of the co-authors, Paul Vixie, is also the BIND maintainer. 
For more info, check out the Internet Software Consortium Web pages at URL:http://www.isc.org/isc/ 
OK, so SRV can be expected to see the light of day quite soon in the most widely used DNS implementation. 
Can I interest any browser and/or proxy authors in taking advantage of it ? 
Methinks added resilience via SRV would be a good selling point ... :-) Martin PS There was also something about putting SRV up for Proposed Standard ielding[SMTP:fielding@liege.ICS.UCI.EDU] Sent: Wednesday, July 10, 1996 8:17 PM Subject: Re: short names for headers From the Montreal minutes suggestion for a new charter's milestones: Aug 1: (Leach) draft on sticky headers, short names for headers, and context identifiers I suggest dropping this as a product of the working group. 
Although it is certainly possible to compress the protocol through two adjacent communicating parties by the addition of stateful interaction (what is meant by sticky headers and context identifiers) and tokenizing protocol elements (a small part of which is covered by short names for headers), it is not a trivial task and needs to be extremely careful with regard to proxy-proxy communication with requests from multiple user agents being interleaved. 
This falls into the category of "research" and should be investigated outside the standards arena. 
In other words, I don't think this task can be accomplished within the lifetime of this WG. 
Furthermore, even if it were accomplished, our long term (within a year) plans for HTTP would end up replacing it by the multiplexing and tokenizing of HTTP/2.x. 
Therefore, I think it would be better to focus within the WG on things that can be done before the next IETF meeting, and encourage other efforts to focus on the research work that already needs doing for multiplexing and tokenizing of HTTP/2.x. ...Roy T. Fielding Department of Information &amp; Computer Science (fielding@ics.uci.edu) 
