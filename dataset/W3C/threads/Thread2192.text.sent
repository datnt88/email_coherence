PEP Battle Plan The PEP extension strategy has been on the shelf for a while now. 
I have been working in this space for a year now, and like a killer mountain, I have learned to respect this problem. 
It is easy to fall down ratholes, or to ascend along the wrong path. 
Only two months ago did I learn this fundamental lesson about the PEP drafts that have passed so far: Extension and Negotiation are separate problems. 
There has been a lot of resistance to PEP because even if it was well specified on paper, the operational model has never been clear because these two problems were intertwingled ? 
lots of black boxes that said "and the HTTP agent will satisfice all of the counterparty's requests". 
Historically, we at W3C have been pushing a PEP-like strategy since our inception. 
Many of the problems we/I tackled were inherently negotiation-centric: the SHTTP-like Security Extension Architecture, the JEPI payments negotiation project, the PICS label request protocol? 
so for us, to make HTTP extension natural for legions of 4th party developers, it is absolutely critical to build upon both of these concepts. 
On the other hand, it is clear that to the smaller community of HTTP designers, correctness, compactness, and simplicity are more important. 
The complexity of selecting an extension, initiating it, and coordinating it with other extensions is like toothpaste in a tube: it can be squeezed back and forth from the HTTP agent to the plug-in extension. 
The August PEP draft was designed to be the bedrock of the JEPI project: on the basis of that draft, the JEPI project is going forward, leaving change control now firmly in the hands of this IETF HTTP-WG as long as it wishes to move PEP forward on standards-track. 
There are several changes I have discussed privately with PEP reviewers from several organizations. 
Many of them are presented herein. 
Other novel questions have only been sketched out, and will require vigorous debate here: how does PEP interact with caching? 
How can PEP be used to implement other proposed features? 
Proposed PEP changes I will soon forward a revised drafting of PEP that simplifies the spec into two components: an Extension Protocol and an Extension Negotiation Protocol that builds on top of EP. EP has two components: Protocol: indicates that this message has been extended Protocol-Info: advertises that an extension is available for future use at this extension We hope to separately motivate Protocol-Request and Protocol-Query machinery as an extension upon that base which says when to begin using an extension, why, and what compatible extensions may be substituted. 
ENP, tentatively, is a separate add-on draft that we at W3C, through long experience with PEP application scenarios believe is required, but can be safely split from the mandatory core HTTP requires. 
We also hope to demonstrate that EP can be deployed without a version number upgrade and perhaps without a new method, either ? 
using Connection: and Cache-Control:. 
It will cost one round-trip to be absolutely sure your counterparty supports PEP. 
Other changes to the PEP draft include: id # tagging of each Protocol-* directive. 
The id# allows us to 1) claim all headers beginning with that id #, 2) use in Content-Encoding for pipeline order, 3) pairing up related directives across request-response pairs modifying the *-matching rule for the URIs in a for list (essentially, * anywhere in a URI) use of headers to pass all extension-specific data (less religion about bags) Better explanation of error reporting ? 
and more Of course, I need to forward the revised draft before any detailed discussion can begin. 
Progress on an extension mechanism is essential because it is the future of 1.x and binary encodings of it. 
Most of what we are discussing for 1.2 can be accommodated over PEP. 
W3C has been gaining experience in a number of domains which leverage such a facility. 
Experience gained today will port over to more efficient encodings of 1.x. 
Caching and backwards compatibility problems can be solved. 
That's the battle plan from this end. 
Next Steps create a PEP home page in the W3C Protocols area submit new draft ? 
by midweek track libWWW5 implementation investigate technical writing support for white papers, implementation guides, perhaps the spec itself The more essential problem is that there is not enough evidence -- by the lack of traffic on the list -- that others besides Rohit are committed to PEP or even care about PEP. 
Right now, Rohit's battle plan has Rohit wrangling this problem alone. 
I just don't think that's workable. 
I'd like to see a small subcommittee of 3-5 people commit to producing a PEP draft that they agree on, to be produced within a fixed period of time. 
If Rohit the appropriate leader for that subcommittee that's OK with me, but we need some others to explicitly step forward, identify themselves, commit to working on it, etc. Larry I realize this is heresy here, but I have to wonder if it's worth building the extension mechanism into HTTP. 
An efficient URI resolution protocol would allow for a smooth transition away from HTTP 1.x and to 2.x or other protocols (smb? 
webnfs? 
multicast?), 
without invalidating old clients and without the overhead of establishing a TCP connection. 
New protocols could then be designed from scratch to take into account everything that has been learned from HTTP, without inheriting the complexity. 
It would also improve scalability, fault tolerance, ability to screen files (for content ratings, price, language, etc.) before downloading, selection of multiple variants of a resource (by allowing the client, rather than the server, to make the selection), client selection of multiple locations of a resource, etc. Seems like we need to take a step back and look at the web as a whole before we commit to the direction of adding more complexity to HTTP. 
Keith I don't think there's anything heretical about "declare victory and move on." 
I, ,too, don't think PEP is the future of HTTP. 
I have my own favorite site of wire protocols to suggest here. 
:) Sounds like a call for a directory service. 
/r$ Close, but not quite. 
A resolution service, not a directory service. 
(Where by "directory" I mean something that can do content-based searching.) *much* easier to build and deploy than a searchable directory. 
Keith By directory, I mean attribute-based searching. 
That seems to be both the past (X.500), the would-be future (XFN), and the fad-of-the-moment (LDAP). 
But the latter is where the world is going. 
Someone could make a real name for themselves by publishing a list of OID's, representations, and semantics for web-page attributes. 
/r$ There's nothing wrong with having attribute-based searching, but it's better if it's viewed as layered on top of a name-resolution substrate. 
One reason for this is that much of the time you don't need to do a search; having the two layers separated enhances the scalability of the system. 
Another reason is that the most effective search engines are specific to a subject domain, rather than generic. 
So we need a single, generic, distributed document store as a substrate for both generic and domain specific resource discovery tools. 
Of course, if some providers want to provide attribute-based searching, co-locate the two services, and even return name resolution information along with the response to a directory query, that's fine -- as long as there's a well-defined standard for doing the name resolution by itself. 
No offense, but I've heard the official wisdom about the future of the world so many times that ... well, I'm skeptical. 
Relatively simple solutions to problems, which fit in well with the existing infrastructure, offered at the right time, seem to be the best way to acheive success. 
In this case, I'm talking about ~10000 lines of code for each of server and client glue, not counting the rpc and db libraries. 
(actually, the current implementation of RCDS is ~4000 lines in the server, 2500 lines in the client, and 3300 lines of common code...but of course it will have to grow somewhat) Which is not to say that there's no need for attribute- based searching, it's just not what I'm talking about here. 
(Actually, this discussion probably belongs on the URI mailing list.) -Keith Keith Moore: [Rohit:] of I don't see PEP primarily as a means to enable smooth transitions to faster content transmission protocols. 
PEP will (at least, I hope it will) enable the addition of new _services_ on top of the HTTP content transmission service. 
Examples of such services are content rating and (micro)payments. 
PEP will (at least, I hope it will) allow such services to have a very low overhead, by piggy-backing them onto HTTP transactions which are already happening. 
This is the part of the PEP draft that makes me interested in PEP: In addition to reliably describing statically extended HTTP servers and clients, PEP will work with dynamically extended agents. 
Indeed, the authors expect that PEP will drive the deployment of a new generation of exensible agents (such as W3C's Jigsaw server and libWWW reference library). 
Now, I also see some problems with PEP: - Dynamic service extension is still somewhat of a research problem. 
I don't know how much of this research problem has already been solved behind the w3c member-only firewall. 
I don't think this WG will want to commit itself to solving huge research problems in the PEP area, but it may want to commit itself to making the tradeoffs left when the research problems are solved. 
- The vision of intelligently cooperating clouds of objects/components/applets/agents adding huge value to the internet experience has been around for some time. 
I have so far not been able to determine how much of this vision the W3C wants to enable with PEP. 
PEP will probably not be able to live up completely to this vision, but how close do we want to get instead? 
Should PEP just be a simple header collision avoidance protocol, or should it define a powerful framework for intelligent cooperation? 
- PEP is not the only mechanism claiming to allow for the smooth addition of services. 
Java is another one. 
(I know too little about plugins to determine if they too claim something here.) If some powerful vendor starts trying to set ad-hoc standards in this area to get a competitive advantage, PEP may go the way of HTML 3.0. 
PEP negotiates on _services_. 
Negotiation on _content_ is orthogonal to PEP, and this WG is already working on a content negotiation mechanism with the attributes you mention above. 
Koen. 
I think this is a good idea and I think that I have subscribed myself to be part of that group. 
What if I (and other interested) together with Rohit figure out a plan for PEP and send this plan to the list by Wednesday? 
Thanks, Henrik Henrik Frystyk Nielsen, frystyk@w3.org 
World Wide Web Consortium, MIT/LCS NE43-356 545 Technology Square, Cambridge MA 02139, USA future of Every protocol needs an extension model, otherwise they will become obsolute before you know it. 
What PEP is all about is realizing that the current RFC-822 extension mechanism inherited by HTTP in many cases isn't enough. 
The PEP framework provides three types of services: A) PEP gives the parties the possibility of enquering and enumerating available extensions. 
B) PEP gives you the possibility of defining three important attributes of any extension: - consequence - what is the consequence of having / not having an extension? 
- ordering - does extension A come before or after B? - scope - this is partially solved with the HTTP/1.1 Connection header but it needs to be bound to the PEP frame work as well C) PEP helps avoiding header name collisions. 
Neither of these services are provided by the traditional model of simply adding new headers. 
I don't say that the existing model isn't enough in some cases - just not en all of them. 
Hang on, after much discussion at the Montreal IETF http-wg meeting, it was decided to continue working on Content negotiation, User Agent, and PEP. 
Please check the minutes as posted to the mailing list This has nothing to do with victory or any other terms borrowed from the battlefield. 
If you have constructive critisism of the current draft as issued August 19th then you should forward them to this list. 
Henrik Frystyk Nielsen, frystyk@w3.org 
World Wide Web Consortium, MIT/LCS NE43-356 545 Technology Square, Cambridge MA 02139, USA I don't think there's much of a difference at all! 
PEP is about extensions and as more Web applications get beefed up with plug-ins, the capabilities that many content providers in practice infer from the User-Agent becomes invalid or at least a small subset. 
Current examples are HTML math, style sheets, and HTML tables not to mention what versions of these are supported. 
These things may very well be supported by plug-ins and hence the distance between extensions and content negotiation disappears. 
The only dimension currently in content negotiation that is difficult to consider in this game is natural language but it is by no means "untouchable". 
It's all features, really, and this is why PEP is interesting as being part of HTTP. 
Henrik Henrik Frystyk Nielsen, frystyk@w3.org 
World Wide Web Consortium, MIT/LCS NE43-356 545 Technology Square, Cambridge MA 02139, USA Henrik Frystyk Nielsen: Well, I think we need to make a separation between 1) extensions which offer content rendering facilities and 2) extensions which offer protocol services. 
In my opinion, not making this separation will destroy cachability. 
Transparent content negotiation was designed to handle 1) among other things. 
I hope that PEP will handle 2). 
If you think that PEP should be made to handle 1) because nothing else does, we have a big synchronization problem. 
Koen. 
Yes, if a) there was an efficient URI resolution protocol with sufficient deployment and compatibility with old clients; b) new clients were developed with such a dynamic protocol interface. 
That is unquestionably the best way to improve the Web's extensibility in terms of new protocol capabilities, and I support it whole-heartedly. 
However, I've been supporting that for three years now and it is no closer to being a reality now than it was then, in spite of a lot of excellent work by some brilliant people. 
The real problems are economic, not technical, and until the service exists it is difficult to say what it will accomplish. 
I do not see any conflict here. 
While a URI resolution service would add a selection and indirection layer above the hard-wired URL, it does nothing to change the work required to actually apply a method to the resource once it has been located. 
That work will still need a protocol that understands hierarchical proxies and can reasonably sustain extensions which ensure robust handling across all recipients. 
Since none of the above-mentioned protocols can do what HTTP/1.1 already does, let alone support the reasoning about extensions proposed by EP/PEP/whatever, I don't see the creation of a URI resolution service as having an impact on how or how not to extend HTTP. 
Having said that, I would also not commit the IETF toward any single direction, PEP or otherwise. 
I am inclined to leave research to individuals and not assign things to a WG until the solution is (at least believed to be) known. 
The problem is deciding when that transition should occur, and whether it should be part of this WG or a different WG or outside the IETF. 
Finally, I think there is something missing from this discussion. 
Extensions have occurred, and will continue to occur, regardless of IETF opinions. 
They occur because users need them, which results in implementers implementing them, often in spite of the WG's recommendations. 
Almost all of my work invested in HTTP/1.1 was toward making the protocol *more* extensible in areas that had previously faltered due to poor implementations, so that people out there can implement what they want and at least have some inkling of what effect it will have on correctly implemented applications. 
We now have a protocol that should be able to sustain any optional extensions, but we also know that some people want more than just optional extensions. 
Not defining son-of-PEP will not stop people from making those extensions, nor would it make those extensions any less complex in terms of their addition to HTTP (quite the opposite, in fact). 
A better question, then, is whether we would prefer those extensions to be added within a framework approved by the IETF, or one outside the IETF? 
...Roy T. Fielding Department of Information &amp; Computer Science (fielding@ics.uci.edu) 
