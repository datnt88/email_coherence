Hi Folks, on Wednesday I attended the RNID's day-long session in Brussels on access to the information society for people who are Deaf. 
A mixture of politics (all the great and good of European Accessibility were present, from Erkki Liikaanen and Inma Placiencia to Jorgen Friis and Shadi Abou Zahra among others), community building, and technical discussion, it was overall a valuable day. 
Much of the discussion was about problems more basic than web accessibility, such as the lack of an emergency service that people can access (you can dial 000 or 911 or 17 or whatever the local equivalent 
is on a mobile phone, but you can't SMS it and there is no guarantee 
that SMS gets delivered anyway). 
There was also an encouragingly strong thread of discussion about the need to ensure that standard, mainstream technology development is used, rather than being locked into outdated specialised systems as is the case with text phones (which were developed in the 20s as a replacement for telegraphy, adopted in the 60's when businesses started to move beyond them and therefore give them away, and use a raft of different standards making Europe a series of little islands that can't call each other). 
Captioning got a run, (I'll leave it to the experts, but there were no great surprises) as did the need for sign-language interpretation, particularly where the content was moderately complex and the captioning risked losing the many relatively poor readers among the deaf community. 
And one area that got people very excited was the lack of a suitable chat framework. 
In general they are line by line, rather than being character-by-character interactive as text phones are. 
I am sure blind users appreciate that fact, because with a screen reader the alternative would be a nightmare. 
Deaf users, on the other hand, find them enormously frustrating - particularly when translated to the mobile world. 
Normally small isolated communities get offered only the most well-known, most mainstream technology, so the new MSN, ICQ, Yahoo chat systems are all that many poeple have seen, and they compare unfavourably to text phones. 
The older, widespread and robust Unix "talk" is in fact what they are looking for, but without a champion it seems that the concern went into making chat accessible for the blind with no particular consideration of what Deaf people actually do. 
Well, where accessibility entered into the picture at all, anyway. 
GSM came in for a lot of harsh treatment - much as it has enabled people to have a whole lot more communication, through SMS, in something approximating an international standard (it's used in virtually every country except Japan, although it is not so common in North America which still has a mish-mash of competing mobile telephony platforms). 
Because there are many arbitrary barriers to using it - I cannot send SMS to france or finland from my australian phone, because the network providers block access, although there is no real technical reason. 
It used to be impossible to send messages between networks in Australia, but some behind-the-scenes discussion about whether that was discriminating against the Deaf began and the networks opened up shortly afterwards. 
And there is no guarantee that SMS messages get delivered anyway - like email it is a send-and-pray protocol, although non-delivery is rare enough that people forget it happens and get surprised by it. 
(Jorgen Friis is from ETSI, the European standards body that developed GSM, and from Denmark, a generally nice country :-) Anyway, such are my thoughts. 
Many thanks are due to RNID for the organisation of the event, to the participants, to an army of interpreters (3 sign languages plus lip-speaking) for facilitating side conversations, introductions, and answering my assorted questions about signing. 
Cheers Chaals Charles McCathieNevile Fundaci?n Sidar charles@sidar.org http://www.sidar.org 
This, I think, is because the service providers have had problems working out a business model for it. 
For a very long time it was not available at all, even though in the standard from the beginning. 
Then it was only available within a service provider. 
In the UK, I believe it was only government intervention that forced it to be made available between providers. 
As the network resources used by SMS are negligible, it is surprising that the providers were so unwilling to allow it. 
Maybe they thought that the GBP 0.10 typically charged for the message was all that people would bear, when they could get much more than this for a long voice call. 
Part of the original mistake may have been to assume that there would be one SMS call replacing a voice call, not a whole SMS conversation. 
Restricting to one provider avoided reconcilliation questions, but also was probably aimed to get your friends onto the same network. 
There is often a way of getting delivery confirmation, although sometimes a relay may accept delivery and fail to complete. 
On the network I use, and I believe some others, starting the message text with *0# will cause delivery status notifications to be generated. 
Some networks may charge for this. 
This is an exploration of some ideas. 
I have got a bit of review from Inmaculada Fajardo Bravo, an cognitive scientist working with deaf people, and I have asked several others to have a quick look. 
But the responsibility for whether the ideas are sound or insane is all mine. 
I am hoping to identify issues where WAI's current work could be improved, and take them to the relevant working groups, and in the process give us all an opportunity to evaluate what we are doing in the light of experience from people who are our audience. 
Don't tell me I 
am an idiot, because I know how much of an idiot I am. Do tell me that I have misunderstood or forgotten or misrepresented something... Feel free to enjoy or stop reading and delete :-) 
For a community of deaf users who are not good readers, signing is their native language. 
Captioning is considered a nice idea, but not actually the preferred way, for many deaf people, of understanding what is happening. 
LIkewise, text chat is considered a good thing. 
(Real-time character-by-character interactive, as provided by text phones or the unix "talk" program, more than the line-mode 'asynchronous' modern chat software or SMS). 
But this community is much happier signing, and would prefer to be able to do that as a way of communicating. 
There are a couple of approaches to having signing on the phone. 
The obvious one is video-phone. 
Work done in the Deaf Australia Online project and its successor looked at the actual technical requirements for making this workable (by testing with users of Auslan. 
Mileage may differ in other sign languages, but it is at least a marker point). 
Their report said "The minimum standard of video transmission for Deaf signing should offer a temporal resolution of 25 frames per second with a picture resolution of 352x288 pixels (CIF) which generally requires a transmission rate of 384kbps" [dao2] This, coincidentally, is what is available on the Allen-10 multifunction terminal that they recommended buying for people. 
However this kind of bandwidth and functionality is now pretty widely available - to the extent of being possible on mobile telephones, if not yet widespread. 
In addition, better compression and video technology can reduce the bandwidth required. 
Another approach is the use of signing avatars - animated figures. 
There are several companies who have developed such systems, and on tools that can identify and record people signing. 
The benefit of this is that you can reduce the bandwidth to the transmission of instructions for the animation, with the drawback (for many people) of not being able to see the person you are communicating with. 
Although there are various methods for encoding the information, as far as I know there is no consensus on a standard, which makes it difficult to deploy these systems in the mainstream Web today [signAv]. 
On the other hand, this seems a field that is ripe for work, since it should be possible to integrate this kind of work with indexing and dictionary lookup systems that are being used to build large-scale translation and search services, that work by brute-force processing of lots of information instead of trying to work with artificial intelligence approaches. 
Although this doesn't always work, for many use cases it becomes relatively simple to produce effective results. 
Another issue is the way that people process information. 
Although this has been extensively studied by cognitive scientists, and the results have been adopted to some extent by usability specialists, the penetration of this information to accessibility seems fairly low. 
One interesting piece of work by Inmaculada Fajardo and others in Spain suggests that signing deaf users seem to have particular requirements in this area which are not necessarily the same as are useful for others [inma]. 
What are the implications for these findings if we can in fact provide semantically rich guides to the content of a site, and manipulate them on the fly (including being able to swap around labelling schemes across languages - something we could expect to be simple to develop in the context of European sites which already have the relevant information)? 
At the very least, going beyond WCAG's "provide different serach mechanisms" to some particular examples based on user testing, in line with everything we know, and perhaps even tested carefully themselves would be a step forward... 
I'm not offering any particular answers here, and I am not even sure that I understand all the questions. 
But it seems there are some interesting ones that people have already studied, and some results that don't necessarily fit with the current thrust of work on the WAI guidelines as well as they could if that work was taken into account. 
[signAv] several links: Google-converted HTML at (long URI) http://www.google.com/search?q=cache:DpUkrCplGfcJ: 
www.casadecritters.com/Becky/ 
Journal_Draft.doc+Auslan+avatar&amp;hl=en&amp;ie=UTF-8 [dao2] http://www.circit.rmit.edu.au/projects/dao2/DAO2_final.pdf (3 MB PDF. 
Relevant section is the 58th page of the document, which has page number 52) [inma] http://www.ugr.es/~ergocogn/articulos/working_deaf.pdf (Working Memory Architecture and its Implications for Hypertext Design: Insight from Deaf Signer Users Research - published in the proceeding of the HCI conference, 2003) Random other references: Signwriting - A set of systems for encoding different kinds of movement, including signs, dance and others: RNID - a group that does a lot of stuff in Europe: Gallaudet University - an American University for the Deaf: NMIT centre of excellence - some folks in Melbourne who work on the stuff: http://online.nmit.vic.edu.au/deaf/ conference on access to information technology for deaf people): 
Charles McCathieNevile Fundaci?n Sidar charles@sidar.org http://www.sidar.org 
Hi Chaas et al, Just a quick note that there is a company that is making a signing type captioning to distance ed classes. 
There seems to be still many problems with it, but it is pretty far along. 
I cant remember the facts off the top of my head but will look into it. 
The video phone piques my interest being an old telecom software engineer. 
sigh True the bandwith with the new 3G networks will support the video streaming. 
However, I doubt that compression would be a viable solution for mobile phones as that is a very consuming process. 
I wouldnt worry too much about compression and go with the full signal at maybe a reduced resolution. 
I would also be wary of reducing the frames/second too as you may lose some subtle movements that may be key. 
And now that I touched on visual information, I think signing in person is preferable than aviators for a couple of reasons: 1) Signing is more than just the hand signals, there is body language, lips being read, etc to help fill in the total communication. 
2) In using aviators, where would you store the avaitors to send? 
You will build up a huge collection very quickly. 
My thinking is that avaitors would be similar to PECS symbols in AAC. 
Sorry to be a bit on the negative side, just wanted to head off anyone spending too much time on this without those thoughts. 
I DO like your idea Chaas and will look for that company/product. 
-Steve 
I am sending this message on because it hasn't appeared in the archives, and I have never seen it come from the list. 
I'll respond to it separately, in pieces, but I would like to thank Guido for taking the time to write it. 
cheers Chaals From: Guido.Gybels@rnid.org.uk 
Subject: Beyond text captions Re: Deaf users, Date: 1 April 2004 18:23:15 GMT+10:00 Charles, 
Thanks for sharing your thoughts with us and apologies for the lateness of this reply, but I have been extremely busy lately. 
Before I address some of the specific issues you have raised, I need to point out that RNID already replied to WAI's draft WCAG 2 guidelines through the Office of the e-Envoy in September last year. 
You can see the text of our formal reply here: I hope this did not get lost in WAI and would appreciate it if you could give some more attention to these issues were required. 
You will see that our response already makes reference to avatars or sign language video streaming and other issues beyond captioning. 
For a community of deaf users who are not good readers, signing is their native language. 
Captioning is considered a nice idea, but not actually the preferred way, for many deaf people, of understanding what is happening. 
Sign language users form a minority of the overall group of deaf and hard of hearing people, but indeed they face quite specific challenges in dealing with web and other content precisely because of the fact that their first language is sign, not the spoken language of the land. 
So, in Britain, British Sign Language (BSL) is the first language for a 
group estimated to be about 50.000 users (the numbers themselves are cause for great controversy by the way). 
That means that English is for these people at best a second language. 
The problem is further aggravated because of the fact that BSL is not based on English, it is a quite different language and there is no one to one relationship in terms of semantics, grammar etc. between English and BSL. 
Some sign languages, like for example German sign language, are very closely related to the spoken language, making it easier to be fluent in both and to transcode between spoken and signed versions. 
For languages like BSL that is not the case and this complicates problems quite considerably. 
So, yes, sign language users can benefit from having signed content available to them as opposed to written English (or whatever language domain in question). 
Nevertheless, it doesn't mean that simply transcoding every multimedia content in sign language is in fact the best possible option. 
There are a number of problems: a) For written, rather static content, it might make sense to provide a signed version through the use of video clips or signing avatars. 
However, not all documents are equally suitable to be delivered in signed form. 
Train timetables for example, or schedules are not very effective in signed form. 
In other cases, like for long, legal documents, it can become very tedious to have to go through a long winded signed clip (making abstraction of practical problems like filesizes etc) and it might be actually better to simply provide a short signed resume of the key points with pointers to further relevant information where needed. 
b) Multimedia content can be complicated too: providing open or closed signing for a newsspot for example is very helpful to sign language users. 
Doing the same for a high action clip with little dialogue of an uncomplicated nature might be far less effective. 
In fact, you see that for things like sports or action movies, many sign language users even if given the choice will actually prefer the subtitles because it is less intrusive and distracts less from the action than the signing does. 
So, it is not all as black and white as some people purport it to be. 
We have already carried out quite a lot of research into this and are continuing to do so. 
One of our projects is in collaboration with the BBC and looks specifically at these issues of preferences and how genre, timings etc. impact on that. 
c) Since there is no simple way of automatically transcoding between written and signed content, providing signing either through avatars or via video clips requires specific work to create the signed content separately. 
Apart from the obvious challenges this poses in terms of content management and processes (keeping both versions synchronised for example). 
This is challenging enough for static content, becomes very problematic for dynamic content and extremely challenging for transactional systems. 
RNID has been involved in a number of projects addressing some of these issues, but it is quite clear that a great many problems remain unsolved and will require many years of hard work before we get even close to full availability and manageability of these forms of content in signed format. 
An interesting problem being also how to allow sign language users to input feedback for transaction handling back into the system... d) Sign language interpreters for the translation of written into signed content are a very scarce resource. 
In the UK for example, there are only about just over 200 registered interpreters available for the whole of the UK and for all the interpreting (face-to-face, media, video interpreting, relay, etc.). 
This means that the resource is very expensive as well. 
Producing signed content is far from a minor problem. 
That is precisely why RNID is involved in the development of avatars, which we see as a way to increase the provision of signed content: pageid=666&amp;co_pageid=3 What it all boils down to is that information providers should not make any assumptions about people's preferences simply based on the fact that they are deaf or not. 
In the end, the choice of whether or not to use subtitles for example is up to the user. 
The provider therefore should offer the possibility to the user and leave it to them to decide what they want to use where. 
Offering in addition signed content as an alternative to written content is equally important, yet there are significant practical and organisational constraints for doing so. 
We will have to accept that for the foreseeable future, it is simply not going to be possible to offer signing everywhere and for all the massive amounts of written content that is available, let alone for dynamic or even transactional content. 
Work done in the Deaf Australia Online project and its successor looked at the actual technical requirements for making this workable We know very well that some research has been done on the subject of using video technology for remote signing and/or lipreading (in fact we are carrying out such research continuously here at RNID), but the fact remains that there are significant questions that remain. 
One of the intrinsic problems in using video technology is that these systems are not designed specifically to be carriers for BSL, but rather have been developed as generic tools for video encoding, compression and transport. 
That means that the specific feature set required for BSL might sometimes be compromised due to design decisions in favour of general usage. 
For example, common compression techniques used to compress the video stream into the bandwidth available will favour reproduction of broad movement over fine detail in order to keep the bitrate within the bandwidth limits. 
That often results in increased blurring and/or pixilation in favour of transmitting the broad movement itself at the highest available framerate. 
This might cause serious degradation in details like facial expression or individual finger shapes, essential for BSL. 
A lot of the work done focuses on very coarse measures like framerate or picture size to establish minimal requirements. 
However, in reality, such measures do not address many of the important aspects of successful sign language and/or lipspeaking communication. 
Some of these are: - Problems of temporal and spatial resolution: these impact on things like sufficient resolution for detailed aspects of finger spelling and lipmovements or eye-gaze. 
Even at high framerates and with large picture sizes, a suitable resolution to convey enough detail for these to be readable is problematic and no actual scientific data exists to establish a minimum set. 
This is further complicated by the fact that compression and encoding impact directly on this: to achieve for example a perceivable framerate effect of 20fps taking into account the realities of compression, round trip delays and decompression requires probably at least 25fps effective framerate. 
- In addition, compression schemes impact on effective resolution, a problem that has not been understood enough either. 
Pixelation and blurring impact on the legibility, and those are not only difficult to measure quantitatively, but they also are hugely variable. 
- Timing and synchronisation: framerates are only just part of the story in terms of timing. 
Round trip delays can impact on the real-time aspect of the conversation but even more important is the synchronisation issue. 
For lipreading for example, where most lipreaders actually combine audio and visual perception to understand what is being said, synchronisation has to be very tight. 
While this is less of an issue for profoundly deaf people that use lipreading to add to the understanding of the conversation, most lipreaders actually do use the audio as well and the synchronisation required comes very close to the boundaries of what is technically possible with even optimal network situations, let alone in realistic environments like standard long-distance ISDN or even broadband connections. 
- Dynamic resolution adjustment: shape-encoding like in mpeg-4 for instance would allow to overcome some of the problems raised before, yet we lack again relevant data to design services: specifically what is needed is a proper design for algorithms to provide this dynamic adjustment, based on factual understanding of how to define priority areas and shape forms and the effect all of this will have on signal processing and coding. 
- Lighting and viewing angles: while the importance of lighting is recognised by most, there is again very little quantitative information available and the interaction between the various lighting features and the other picture aspects like temporal and spatial resolution or encoding is not well understood in terms of impact on the signing or lipspeaking. 
- Safe areas in pictures and the impact of near-border distortion etc. 
These are just some of the outstanding issues. 
In addition, IP networks with their own topology and varying bandwidth designs pose their own set of problems and challenges, all of which we will need to understand properly. 
The bottom line is that the matter cannot be reduced to very crude requirements in terms of either bandwidth, picture size or framerate. 
In fact, this can be counter productive as some providers might get the impression that providing a CIF image at 25 fps is all it takes to deliver successful sign language content. 
Deaf people would lose out significantly if such perceptions would not be challenged. 
Yes, as I said, we are well at the forefront of all this work, apart from the clip above, you might be interested in the following: Signing avatars are being developed for quite a number of years now, and there are two global approaches to this: - Motion capture based systems whereby a motion capture database of signs or parts of signs is used to concatenate them into larger signed sequences; - Synthetic systems where the signing is generated entirely through animation techniques and does not rely on motion capture databases. 
The quality of synthetic signing, although improving all the time, is still far from being usable in real systems. 
Also, the problems of sign language notation are still not solved. 
Motion capture is laborious and requires expensive and time-consuming setups. 
We have also experimented with mixed models, combining MoCap with synthetic signing. 
In addition, we are working with organisations like the BBC to make MoCap less laborious and thus easier to use so that more content can be provided in less time and at a lesser cost. 
This email is getting already too long, so I'll leave it at this for now. 
I hope the information provided is useful and helps in better understanding some of the issues we are dealing with. 
In the next 6-12 months, RNID will be publishing a white paper on sign language provision using video technology and of course the avatar work is progressing as well, which should result in avatars being used for transactional content by the end of the year as well. 
Best wishes, Guido Guido Gybels Director of New Technologies RNID, 19-23 Featherstone Street London EC1Y 8SL, UK Tel +44(0)207 294 3713 "The Royal National Institute for Deaf People (RNID) is the largest charity representing the 9 million deaf and hard of hearing people in the UK. 
As a membership charity, we aim to achieve a radically better quality of life for deaf and hard of hearing people. 
We do this by campaigning and lobbying vigorously, by raising awareness of deafness and hearing loss, by providing services and through social, medical and technical research." 
This email and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. 
Any views or opinions expressed are solely those of the author and do not necessarily represent RNID policy. 
If you are not the intended recipient you are advised that any use, dissemination, forwarding, printing or copying of this email is strictly prohibited. 
If you have received this email in error please notify the RNID Helpdesk by telephone on: +44 (0) 207 296 8282. 
The Royal National Institute for Deaf People Registered Office 19-23 Featherstone Street London EC1Y 8SL No. 454169 (England) Registered Charity No. 207720 
As have I, but I finally got around to getting permission from Inma Fajardo to post her responses here as well. 
The following text is hers: My experience with deaf people in the field of web interaction is not as extensive and it's more specialized on their cognitive functioning. 
However, I'll try to contribute with some of our insights. 
In my opinion, the main problem of WAI's current work is that deafness is almost exclusively considered a sensorial problem, that is, a deficiency characterized by the lack of auditory stimuli processing by the sense of hearing. 
Consequently, the accessibility guidelines for deaf users are focused on overcoming this sensorial problem, for instance, providing visual information instead of acoustic one. 
However, deafness also influences the functioning of cognitive processes and the representation and organization of knowledge, affecting complex tasks such as problem solving and decision making (Marschark, 2003). 
Nevertheless, these cognitive peculiarities are not necessarily negative if designers of devices and interaction systems (e.g. web designers) take them into account. 
For example, it has been demonstrated that the use of a visuospatial language (sing language) improves some aspects of other visuospatial tasks, such as the memory for spatial places (deaf signers have a longer spatial memory span than the hearing non-signers ([Wilson, 1997-266]), the discrimination of faces, the processing of facial characteristics and the recognition of faces or shoes (Arnold and Mills, 2001). 
Precisely, as a cognitive ergonomist my work lies in studying and researching how this advantage of deaf people could be harnessed for interacting with hypertexts. 
The conclusion of our recent empirical work is that websites' designers could distribute verbal content along more layers of nodes in the hypertext structure which, in addition, could serve as semantic spatial clues for text comprehension. 
On the other hand, it is possible to formulate the question: could the replacement of textual information by visual content improves web interaction of deaf users? 
We have empirical data that would support this guideline but only partially in the case of information retrieval tasks (Fajardo, Ca?as, Salmer?n and Abascal, 2004). 
Deaf signer users only improve the web searching with visual targets when the search does not imply a categorical decision, that is, when there are not involved semantic factors in the information retrieval task and the search is based on visual factors as visuo-perceptual speed (related to the visuospatial store of working memory process). 
Following the cited finding, some semantic aspects related to long term memory of users (LTM) seem fundamental to perform information search task in a Web Site. 
Whether users are not able, or have difficulties, to generate the category where the concept they are searching for could be (for instance, the category Sports, if users are searching the news concerning to football matches in a digital newspaper), it is probable that their performance would drop. 
This fact could be applicable to both, verbal and graphical interfaces. 
In the case of icons, different ways of organizing knowledge in memory could affect the users' judgement of semantic distance (or judgement of the relation icon-referent) and, in this manner, the efficiency in the selection of the icon which would open the searched site or would activate the desired function. 
Actually, we have found that deaf signers had more problems than hearing non signers to find visual targets in a newspaper website when the targets were in a deeper layer of the web structure and it was necessary to take more categorical decisions for finding it. 
We have concluded that the qualitative difference in knowledge organization between deaf and hearing people, found in a previous normative study about semantic distance of icon-targets used in the experiment, could be determining the difference in the web information retrieval task. 
That is, if we have to use icons, images or pictures for information retrieval task, we have to take into account that all users do not extract the same meaning from them. 
This could mediate the applicability of accessibility guidelines for deaf and cognitive disable people, such as, provide well illustrated text (WAI, 1999), provide content-related images in pages full of text (WAI, 1999) or provide visual information instead of acoustic one (Emiliani, 2001). 
Anyway, what is important of the empirical studies with real users is that, on some occasions, the apparently useful solutions are not so useful. 
For this reason, the analysis of users cognitive processing and the empirical research are fundamental. 
Apart from the low technological requirements, one explicative hypothesis of deaf people's preference for "text chat" communication is that they can use an idiosyncratic written language in this context for talking with other deaf people. 
Deaf people who use sign language as first language have problems with the use of articles, conjunctions, prepositions and grammar of oral language in general (Moores, 1997) because the grammar, at least in Spanish sign language, is completely different to oral language. 
However, on some occasions, problems come when users need to "talk" with a machine (e.g. web search engine). 
Some interaction systems can not understand this idiosyncratic language. 
In fact, in these cases, one interesting option is the use of an alternative way of communication with the systems, for example, by means of video technology and computer vision techniques which capture and interpret sign language. 
I'm not very familiarized with the interesting work in this area, like those cited by Guido Gybels. 
However, some colleagues of my University (Research Group of Computer Vision) are currently working on Spanish sign language alphabet recognition system using PCA algorithms. 
Besides, they are working in facial expression and body position recognition. 
Their future project is to integrate these 3 recognition systems for supporting the deaf signers communication. 
The frame size that will use such systems is 640x480. 
These systems will work in real time (25 fps). 
The final objective is to get that the three systems work in parallel in a grid computing system (8 PCs). 
This solution could overcome the temporal resolution problem (real time recognition). 
looking forward to hearing your feedback! 
inma Inmaculada Fajardo Bravo Laboratory of HCI for Special Needs. 
ATC Department Computer Science Faculty Manuel Lardizabal Pasealekua 1, E-20018 Donostia 
