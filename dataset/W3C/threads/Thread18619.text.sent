From:Al Gilman [SMTP:asgilman@access.digex.net] 
Sent:Tuesday, August 26, 1997 2:34 PM Subject:Re: Audio Access to follow up on what Geoff Freed said: [David Pawson] ..snip [referring to...] For those using synthetic speech to access text, there are potential problems when the sound effect, and/or the spoken text of a description of the sound effect, collides (in the audio delivered to the user) with the presentation of spoken text extracted from the page. 
[David Pawson] Are we approaching a 'channelling' effect? 
The impact of personal choice would leave a user instructing the browser to selectively action visual and auditory output, leaving [for example] presented material to be channelled primarily to an audio device ( for the visually impaired reader) which would eliminate any audio [switched off] from the page. 
Similarly, auto-generated sounds from the web page might be switched off and replaced with visual [alts] output for the user who has no use for auditory output. 
Channels would need to be defined for Primary output visual Primary output audio [One of these may be defined as my preferred prime channel] Secondary output visual Secondary output audio if we wanted to get exotic, the presence of a secondary channel output could lead to an event to which I may wish to respond, by halting the main channel output to listen, look at the secondary channel? 
Just a thought. 
DaveP to follow up on what David Pawson said: [Al Gilman] Yes, you have got the basic idea very well. 
User control over how streams of information from the source get directed to sensory channels of the user. 
Text-to-speech gives us some crossover capability. 
The web page author thinks of text as destined for the user's eyes. 
But the eyes-free user redirects the text to his/her ears. 
If there is already an audio track targeted to the audio sensation, there is contention. 
In the case of a movie description done at NCAM, you can mix it with the sound track because it is synchronously designed and edited to be overlayed in that way. 
On the web, the sound effects are designed asynchronously, the collisions are less benign, and the user will have to exercise more choices concerning whether to mix the sound streams or break them apart, muting one or another of them at times. 
[Snip] [David Pawson] [Al Gilman] Because we have some ability to shift content between user-sensation channels in the user equipment, the content providers don't have to provide separate data for all the profiles of user capability and preference that will be served. 
There is not an end-to-end set of parallel channels. 
The information flow has some redirection and mixing capability on the user side. 
A combination of some redundancy (such as by transcripts and captions, which shadow sound in text) in the data bundle offered by the source, together with user control over how the source-provided streams or components are presented, gives us the maximum adaptability for the minimum cost. 
[David Pawson] [Al Gilman] Yes, I was imagining something that exotic. 
Consider a slide-show presentation with a continuous audio track and a sequence of still images. 
On can imagine the blind user playing the audio track in near real time. 
They could skim along with just the titles of the slides automatically spliced into the audio at the points where the slide changes. 
Then, when the voice track doesn't convey a complete story, the user could stop the playback, reset the play mode, and have it read the text on the slides and possibly an audio or textual description of the slide before proceeding with each frame's-worth of the sound track. 
To be realistic, I think we have to talk separately about the audio, visual, and tactile channels by which the information finally gets to the user a little separately from the media types that carry the information from the Web server to the Web client. 
HTML text with CSS styling is a media type that lives in the HTTP dialog that has dual capability to be presented in sight or sound. 
With an ACSS style in the library, the sound can be even better. 
But other content, like GIF files, is not that flexible. 
For these we have to build in separate data [the description] to make the message accessible in sound. 
Sometimes alternate presentation of portions of the information will be prepared at the source, and sometimes they will be generated at the user. 
The author will not in general have thought through all the combinations and conflicts that can arise, so the system has to reserve some control to the user. 
-- Al Gilman AL A combination of some redundancy (such as by transcripts and [David Pawson] I was thinking only on the browser / stylesheet end. 
My starting point is the information availability at the page of html. 
I have the source available (a graphic [ maybe an alt], a sound, some text). 
What is needed is some preparatory work to assign these to 'my' preferred channel, with event association. 
Then go ahead and 'read'. 
If we can block the browser from its asynchronous actions (playing the audio, showing the video clip) then I can have a single source sound / site / feel, fed serially to me, the way I want it, rather than the way the author intended it (blazing saddles clip with the sound of farting in the background!). 
I can't remember which of TVR's 3 modes we are talking about, but it seems a good candidate for audio/tactile/visual stylesheet commonality IMHO. 
Al Sometimes alternate presentation of portions of the information [David Pawson] If we can 'generate' alts [ e.g. here's some sound the author forgot to describe] then even better, the channel source is switching from data sink to source to assist the user need. 
Regards, DaveP 
