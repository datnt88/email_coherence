Several weeks ago, Paul Leach and I submitted an Internet-Draft on "Simple Hit-Metering for HTTP": ftp://ds.internic.net/internet-drafts/draft-mogul-http-hit-metering-00.txt and announced it to the HTTP-WG mailing list. 
Although our previous (and quite different) proposal, at the end of July, resulted in some discussion, this one has raised no comments on the mailing list (although we have received a few private comments). 
Since we have not seen any criticism of our latest proposal, we would like to interpret this as lack of criticism rather than lack of interest, because we already have evidence that several large customers are eager to deploy implementations of our proposal. 
Therefore, we intend to submit this I-D, or a minor revision thereof, to the IESG as a Proposed Standard as soon as possible. 
Note that the criteria for Proposed Standard in RFC2026 says A Proposed Standard specification is generally stable, has resolved known design choices, is believed to be well-understood, has received significant community review, and appears to enjoy enough community interest to be considered valuable. 
However, further experience might result in a change or even retraction of the specification before it advances. 
so we *would* like to encourage further community review. 
If there is significant criticism based on technical merit, then we will reconsider our intention to submit it as a Proposed Standard. 
Of course, we are also eager to hear from people who support our proposal, or who would like to suggest revisions. 
There are a few minor open "Design Questions" still listed in this draft. 
-Jeff and Paul P.S.: We should also note that Larry Masinter has suggested that this should be submitted for "Experimental" rather than "Proposed Standard" status. 
Our reading of RFC2026, however, convinces us that "Experimental" would be inappropriate. 
Larry may still disagree. 
We certainly cannot solve all of the problems arising from trying to simultaneously optimize security, bandwidth, latency, and ease of management. 
Our proposal does not, however, force a cache to use any particular path through the mesh (i.e., if it has multiple paths, we don't force GETs to follow any specific path, although we would at least expect HEADs to return reports to the appropriate server.) Since we open up another possible dimension for optimization (i.e., one path allows metering, one does not) this makes the optimization problem harder, but the default solution is not any worse. 
How about resources that are avilable from several sources (URNs)? 
Your proposal does not force a particular path through a mesh, but the implementations of your proposal are likely to do that unless the issues concerning these aspects are raised, discussed and not recommended in your proposal. 
It probably would help to think of our proposed design as counting "uses" of "responses", not as counting "uses" of "resources". 
When one takes this perspectice, the fact that a resource may have several names is largely irrelevant, as long as the mapping from the name (URL) given in an HTTP request to a *cachable* response is stable. 
(If it were not stable, then I would not expect the response to be cachable in the first place!) 
As I said in my response to Ted Hardie, our specification probably ought to say explicitly that the proxy needs to record the identity of the immediate source of a response, and this is another example where that is important. 
How will this influence load-sharing? 
It should not. 
It only influences who needs to see the hit-meter reports, and it would be entirely acceptable for the proxy to store multiple source-identities if it is willing to do the bookkeeping according to the rules we defined. 
The implementation becomes somewhat more complex, but this is the tradeoff for trying to optimize things. 
The proxy may have to store where I did get a document, and the origin server may have to (or want to) store who it gave a document. 
No, this is not required by our proposal. 
The origin server may want to do this if it wants to play complex games with the usage-limiting mechanism, but for hit-metering, I see no reason for the origin server to remember which proxies have a copy of a response. 
(Some people are working on cache-invalidation schemes that would require some sort of server-side database, but this is a separate issue entirely.) 
As a cachemanager I will (probably) have to handle flow-information that I do not have to care about today. 
I would like to count and send count to the server, without caring about flow. 
Combinations of hit-metering and usage-limiting may force me to store flow-information. 
I'm not sure what you mean by "flow-information." 
Can you give a specific example? 
-Jeff In the case of URNs that resolve to multiple URLs, it seems the answer depends upon the purpose of hit metering. 
If it is a server management tool, then the most important information is the number of hits to a given resource (meaning something identified by a URL). 
If the point i to measure the usage level of a URN-resource (Surely, there must be some established terminology here!) then it seems appropriate to meter the accesses to the URN. 
This is problematic, though. 
Under the NAPTR proposal (which, of course, is only one possible approach), then the question arises of how to handle N2L or N2Ls responses, where each URN-request results in a URL request (terminology?), but with N2R responses, the actual resource could potentially be served by any of a number of protocols. 
It seems to me that hit metering (restricted to HTTP) and resource usage (where a resourcer is more broadly defined as something identified by a URI which needn't necessarily be an http: URL) are different problems. 
Gregory Woodhouse gjw@wnetc.com home page: http://www.wnetc.com/home.html 
resource page: http://www.wnetc.com/resource/ 
