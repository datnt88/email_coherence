Hi all I ran the demo showing the framework, explained the building process and the directory structure, went over what software is needed, showed an xml doc as well as ECMA and Java output and ran the DOM1 Core jar testing Xerces as well as alltests.html 
testing Mozilla during this week's F2F. 
Everything went fine, and reactions were in general positive. 
In particular, the original idea of using the xml format to enable generating of more than the proprietary bindings as well as documentation and metadata was better understood now, during the demo, than by describing it alone. 
Hopefully companies in the WG will see to that resources get available for populating the framework with more tests in the very near future, especially tests on new modules of the DOM specs. 
As far as future ideas is concerned, here is a list of things that were discussed: 1. Provide a simple, runnable, pre-release distribution in order to have people starting testing even now. 
My reaction was that we on the one hand want to ensure the integrity of the test suite, but that we on the other hand should allow for people to run tests, since the source and all tools needed are publically available in any case. 
I want to do this fairly soon, so we should now try to evaluate the tests we have and resolve any issues that exist now. 
All implementors on the list, please check the available code and send comments to correctness of the tests to this list, using [Test Review - testname.xml] 
_your reactions_ as the subject to this list as soon as possible. 
2. Provide a simple transform to read a spec and do a smoke test; ripping out tests on each interface with all its methods and attributes, say. 
This would greatly enhance coverage, on the one hand, but would also serve as a good starting point for tests that could be further enhanced. 
It could also serve as the basic functionality tests on each module that the WG wants to see for level 3. 3. Documentation was asked for, since it is my action item from a long time back, I'll see to to provide if not a full documentation, then at least a draft in the CVS for completion by all parties that have been involved on their particular lines of work. 
4. Provide dates or version numbers on the tests so that it's easier to extract information from running the different versions of the tests without having to refer to the version number of the suite as such. 
Can we have another round of packaging and versioning issues on the list? 
Please excuse that I may have forgotten things; I will report on those as soon as the minutes are available. 
/Dimitris Reactions were in fact more than general positive. 
The WG realized the quantity of work done in the DOM Test Suite effort and was impressed by the results showed during the meeting. 
It is likely that WG members will install the tests to run them against their implementations in order to fix bugs (or crashes...). 
Thank you and well done everyone for your effort, Philippe, DOM WG Chair. 
I wouldn't have any problem with releasing the suite as is to the WG. 
It would be helpful to the test suite process to have the implementors run the tests and provide feedback to us. 
Using this feedback, we can generate discussion on whether there is a problem with the spec, the test, etc. 
It might be helpful if we could define a way to report the results in xml. 
Then we could write a transform that showed how the implementations did on each test -- this would be useful in resolving issues. 
I came across the following article on the junit web site. 
It outlines an approach similar to ours, except that it captures the results in an xml file, and then uses a transformation to display a nice test report. 
Any thoughts? 
Yes, this would be a logical next step -- to be able to automatically generate a set of tests from the spec. 
I've been thinking a bit about this one -- I'll look into it and see what I can come up with. 
Okay. 
Anyone know what CVS does by default? 
Do we have to put in a version number or is one automatically generated? 
--Mary After "release", there should be no non-cosmetic changes to a test. 
If there is a change that could potentially change the results of the test, a new test should be created and the previous test, if invalid, should be deprecated. 
Thanks for clarifying this. 
The idea was that we should provice information on each test to indicate version of TS, in order for implementors to monitor incrementally higher degree of support. 
Since we already operate with the concept of version, I don't think it's a problem. 
However, for conflicting tests, we definitely take away any old test that conflicts with a new moderated one. 
This is in part what I thought the versions would indicate. 
/Dimitris 
