The recent discussions on semantics and UDDI made me wonder about what happens when a query is formulated in terms of a particular ontology, but the registry contains information related to ontologies other than that one. 
I imagine some ontologies are so far apart (e.g. describing completely separate vertical indus tries) that reasoning across them would not make much sense. 
Other ontologies m ight be closer to each other, and including them in the same reasoning process could make more sense. 
Does anybody know what is the current state of the art in cross-ontologies reas oning? 
Are there metrics that would allow us to determine how closely related t wo separate ontologies are and how much sense it would make to reason across th em? Thank you, Ugo 
Hi Ugo, This idea of making deductive logical inferences across ontologies is one of the principals of the OWL-DL flavor of the language. 
You do it 
by establishing axioms that express equivalencies, sub-class, or other relationships between the two ontologies (or many more ontologies) and use a mechanism such as owl:import to provide a linkage. 
If you have an inferencing technology, then you can maintain logical consistency across these relationships. 
"Closeness" is a matter of interpretation and can be influenced somewhat by the form of the "bridge" axioms expressed. 
If ontologies are far apart -- ie different concepts -- the logic processor would not infer that they represent the same or similar things. 
Note also that in effect you can obtain a chaining effect from the ontologies --- the introduction of a 3rd ontology only requires bridging axioms to some of the concepts in 1 of the previous two ontologies, and so on. 
So you are protected from an N-squared mapping problem. 
The state of the art is that these systems are available. 
The notion of federated or composite ontologies is directly instrumented in the OWL language. 
Our company ships an OWL-DL suite that provides this capability, as well as authoring tools and aids that help to provide these "bridging" statements. 
Hope this helps. 
Jack jack.berkowitz@networkinference.com 
Actually, I'll make the stronger claim that this is true for all flavors of OWL. 
What sort of inferences OWL Full supports is a trickier question :) Via the LBase translation, you may well get all sorts of deductive inferences, assuming you can pile enough metamodeling and mapping axioms ontop. 
You do it by establishing axioms that express equivalencies, sub-class, or other relationships between the two ontologies (or many more ontologies) and use a mechanism such as owl:import to provide a linkage. 
If you have an inferencing technology, then you can maintain logical consistency across these relationships. 
"Closeness" is a matter of interpretation and can be influenced somewhat by the form of the "bridge" axioms expressed. 
If ontologies are far apart -- ie different concepts -- the logic processor would not infer that they represent the same or similar things. 
There's also a fair bit beyond this in the current Description Logic Literature alone, e.g., concept unification and matching, concept approximation, Description Logics with similarity (see wolter-2.pdf), distributed description logics (http://citeseer.nj.nec.com/504663.html), and cross connections between kbs with differently expressive concept languages (http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-81/ 
wolter-1.pdf). 
Aside from more or less straight forward deductive (and related) logical reasoning, there's some thought that "linking" between ontologies (and kbs, and other things) might have some useful semantic significance. 
This is a rather fluffier notion. 
[snip] I'm just tipping the iceberg, of course :) Cheers, Bijan Parsia. 
Ehem Notwithstanding the technologies being discussed, *translation* between ontologies is about as tractable in the general case as mapping between English and Japanese. 
However, an approach based on translating 
queries is, in principle, more doable than arbitrary mapping. 
Frank 
You do it by establishing axioms that express equivalencies, sub-class, or other relationships between the two ontologies (or many more ontologies) and use a mechanism such as owl:import to provide a linkage. 
If you have an inferencing technology, then you can maintain logical consistency across these relationships. 
"Closeness" is a matter of interpretation and can be influenced somewhat by the form of the "bridge" axioms expressed. 
If ontologies are far apart -- ie different concepts -- the logic processor would not infer that they represent the same or similar things. 
Depends on what you mean by general case, of course. 
But I don't think Jack and I were misleading on this front. 
I mean, the simplest reading of "reading across ontology" compatible with all that Jack wrote is fairly trivial (i.e., reasoning across concept heirarchies partial defined in many files owned by diverse, non-coordinated authors; of course, such reasoning may often be uninteresting). 
Care to articulate the difference that makes the difference in your eyes? 
Cheers, Bijan Parsia. 
[Francis McCabe] 
Notwithstanding the technologies being discussed, *translation* between ontologies is about as tractable in the general case as mapping between English and Japanese. 
This assessment is overly pessimistic. 
We're not talking about translating Japanese literature into English. 
In most cases the 
differences between ontologies fall into categories such as these: * One ontology represents a concept as a class, the other as a property * One ontology makes fine distinctions about a concept; the other uses a broader brush. 
* One ontology uses a predicate with n arguments where the other uses a similar predicate with n+1. 
The missing argument must be deleted or inferred somehow. 
* and so forth 
Translating back and forth can be done by straightforward deductions. 
Perhaps you meant merely to say that the deductions would end up consuming exponential amounts of time. 
(Which is _not_ the problem with translating between two natural languages, such as English and Japanese!) You may be right, but it's not obvious. 
Or maybe you meant to say that the translation rules could not be generated automatically. 
I agree with you there. 
-- Drew McDermott Yale Computer Science Department 
Actually, a more realistic scenario is that there are missing relations and missing concepts between the two ontologies. 
Think about a office planner's chair ontology, compared to a carpenter's version. 
On the other hand, if you are an office planner, and you want to ask a carpenter about getting some chairs, you stand a chance of communicating that ... Frank 
Yes, and so forth. 
Going so forth, we see that also: * One ontology takes one point of view on how the world should be divided up into concepts, while the other takes a quite different point of view. 
Just guessing here, but I suspect that if you compare the DOLCE ontology with the top level of SUO you would see this kind of thing going on. 
* and so forth Jeff 
[Frank McCabe] Actually, a more realistic scenario is that there are missing relations and missing concepts between the two ontologies. 
Think about a office planner's chair ontology, compared to a carpenter's version. 
You're right about that, which is why it's better to think in terms of _merging_ ontologies rather than eliminating one in favor of another. 
See the bibrefs at the end of this message. 
On the other hand, if you are an office planner, and you want to ask a carpenter about getting some chairs, you stand a chance of communicating that ... I gather your point is that people are better at communicating using natural language than computers are? 
Hard to argue with that. 
-- Drew Dejing Dou, Drew McDermott, and Peishen Qi 2002 Ontology translation by ontology merging and automated reasoning. 
In {\it Proc. 
EKAW Workshop on Ontologies for Multi-Agent Systems}. 
Dejing Dou, Drew McDermott, and Peishen Qi 2003 Ontology translation on the semantic web. 
In {\it Proc. 
Int'l Conf. on Ontologies, Databases and Applications of SEmantics (ODBASE) 2003} Alexandar Maedche, B. Motik, N. Silva, R. Volz 2003 MAFRA --- a mapping framework for distributed ontologies. 
In {\it Proc. 
EKAW (Knowledge Engineering and Knowledge Management) 2002}, Volume 2473 of Lecture Notes in Computer Science. 
Springer-Verlag -- Drew McDermott Yale Computer Science Department 
To answer Bijan's question: 
I do not have the energy to prove this, but there is an analogy with the halting problem. 
The corollary to the halting problem is, of course, that any *given* program/data set pair can be checked to see if it halts (by simply running it). 
In the case of Ontologies, I think that in practice it makes a significant difference to map a single query from one Ontology to another; compared to translating the entire ontology. 
The reasoning is 
that in mapping a single query you are projecting a one-dimensional vector from the foreign ontology to the target ontology. 
In simpler terms, you only need to verify the relatively small fragment uncovered by a query is correctly interpretable in the target ontology. 
Of course, I am aware that it is possible to construct a query like "tell me all you know" which will reduce to the general case. 
But, again, in practice, this is unlikely. 
A far more likely query is "what does this customer's address" mean in your system? 
That way, if there are concepts in the foreign system that are unknown to the target, then they will not normally 'show up' in the queries originating from the target system. 
Frank 
And what? 
Waiting? 
What if it halts after the heat death of the universe? 
[snip] Aha! Yes, I totally agree with this. 
I certainly have Jim Hendler dinning "Partial mappings!!!" into my ear constantly. 
There are, of course, loads of issues, but not doing what you don't need to do seems verra wise. 
Cheers, Bijan. 
