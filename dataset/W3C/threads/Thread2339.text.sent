Some comments on the sticky headers draft:- 1) The asymmetry between the client/server responses may be due more to the current limited way in which HTTP is used than a feature of the problem itself. 
If we get servers which can implement the PUT and POST mechanisms then I think that the situation might well change. 
2) I'm not sure of the amount that these proceedures buy us. 
It would be nice to have figures. 
Jim G. has made good points about the importance of getting as much usefull information in the first packet send out (i.e. before we hit the slow start throttle). 
This mechanism appears to be aimed more at increasing the efficiency of later packets. 
I suspect that the control data is not a substantial fraction of the total message size. 
It may be more effective to push on people to implement compression/decompression of message data rather than to worry overmuch on the size of the control data. 
Or at the very least point out this issue in the draft. 
3) Section 2.2 asserts that proxies typically multiplex server connections across multiple clients. 
Is this in fact the case? 
What is the actual benefit of doing this? 
How often do two people from the same site wish to connect to the same remote site simultaneously? 
I am very skeptical about people having implemented such a feature in a multi-process server where the interprocess communication overhead would be very large for the payoff. 
Certainly the phrase "typical" does not seem appropriate. 
I could see it being possible in a single process, multi-thread server. 
I would like to see figures showing how often this case came up before compromising other optimisations to adapt to this complication. 
I point out this problem because much of the complication of the spec appears to be working arround this convergence of independent sessions into a single stream. 
On the other hand there may well be a number of proxies performing usefull work undoing the effects of simultaneous connections for image downloads. 
In this situation combining 4 streams from one anti-social browser into one is quite plausible, but note that in this case the headers will probably be compresssable! 
4) The sticky header and possibly the connection header should be explicitly excluded from the set of sticky headers (!) 5) Section 6. 
This section should note that "replay attack" problem will always be present whenever the compression technique is possible. 
If an authentication technique authenticates the message itself then it will have to be a function of the message body and hence not "sticky-able". 
6) Some mechanism for flushing the header cache would be usefull. 
This would help in the multiplexing proxy server case. 
After it is finished receiving input from one source the proxy can send a "flush" message and reset the stream. 
Overall I would like to see an awfull lot of numbers based on empirical measurement before deciding whether this is a worthwhile scheme or not. 
Although it looks OK to me I know from experience that without hard numbers it is very easy to overoptimise corner cases that almost never occur. 
Phill This is because the internet backbone links are the bottleneck, not the LAN between your proxy and your user agent. 
Who cares if you get, say, 30% savings in web traffic on the LAN? 
In this game, it is backbone savings that count. 
Not everyone has a LAN link between user agent and proxy. 
Just to confirm this: we (at Digital's Palo Alto site) run the main proxy between the corporation (still almost 60K people) and the Internet. 
We have something over 100 Mbits/sec of bandwidth between our proxy and the Internet, but several orders of magnitude less bandwidth to most of our internal sites. 
In most cases, the RTTs between the proxy and the users are on the order of 100 msec or more. 
We have numerous users, unfortunately, who have internal connections at 56 kbits/sec and multi-hundred msec RTTs. 
Anything that saves lots of bits over these wires is Good. 
-Jeff From: koen@win.tue.nl[SMTP:koen@win.tue.nl] 
Subject: Re: Sticky stuff. 
2. It was taken between proxy and server, This is because the internet backbone links are the bottleneck, not the LAN between your proxy and your user agent. 
Who cares if you get, say, 30% savings in web traffic on the LAN? 
In this game, it is backbone savings that count. 
Not everyone has a LAN link between user agent and proxy. 
Some small number of people dial in to their proxy, which is owned by their service provider. 
Say order 10 million or so, with providers like AOL and MSN. where the effect of 304 (Not Modified) is not seen. 
(The low % in headers in your because the average response entity-body was ~10,000 bytes. 
In a 304 response, the entity-body size is 0, so the savings in request header size is more imprtant). 
I still have my original datasets, and just ran some new statistics on them. 
In proxy - outside server traffic, 200 responses made up 82% of all responses, and 304 responses 4%. 
Again, I was talking about user-agent to proxy traffic, and the 304s that would arise when the user-agent had something in its cache that it needed to revalidate. 
3. It didn't consider asymmetric bandwidth situations. 
In the limit that the downstream connection is infinite in speed and with low latency, the savings of 80% in request header size that your study used would be significant. 
(The most likely early deployment of cable modems will use ordinary telephone lines as the request channel. 
And the modem hooked to the ordinary telephone line will probably do data compression, so you would gain little extra with sticky headers, I believe. 
In your model, a 200 byte request would be reduced by useing sticky headers to 40 bytes, and then the 40 bytes would be compressed to (say) 20 bytes (assuming a 2-1 compression ratio). 
Without sticky headers, and the same compression ratio, requests would be 100 bytes. 
The savings from sticky headers is still the same 80%. 
I think a new study would be needed to take these effects into account before we can conclude that sticky headers aren't worth the effrort. 
I agree. 
To get something like a firm conclusion, at least one other study is needed. 
My study was done a year ago, with a small sample (145 Mb of traffic), and by someone who is not a statistician. 
However, I think there is enough data to conclude that sticky headers are *unlikely* to be worth the effort. 
Seems to me that there is enough data to show that it won't help much in your environment. 
In other environments, it's at least not so clear-cut. 
Paul 
