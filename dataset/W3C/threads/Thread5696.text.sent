Hi all, Any help would be appreciated. 
I'm struggling with entailment tests for OWL DL. 
I have added to my OWL inference rulebase a bunch of inference rules for owl:intersectionOf and owl:complementOf, and it made my Bossam engine successfully pass five description-logic-2xx tests, which are 201,202,204,205 and 207. 
:-) But I got two failures on 203 and 206. 
:-( What are the purposes of these tests? 
The descriptions on the tests just say something cryptic like k_branch, k_d4, k_dum, k_grz, k_lin, k_path, and k_ph. 
I cannot see any differences between the tests by reading premise documents. 
They just look very similar to each other. 
Can anybody shed some light on me, please? 
Best, Minsu Minsu Jang Senior Member of Engineering Staff Business Knowledge Research Team Electronics and Telecommunications Research Institute Hi Minsu I am copying Sean on this message, he was the author of these particular tests (as you can see from the dc:creator in the Manifest files), The tests themselves come from DL'98. 
The following link gives the introduction: Page 2 of that document is probably the best description of these tests. 
The original test data is still available from Horrocks: (I have just noticed that the link in the CR document was broken, I believe the above links are OK). 
See [DL 98] in the references in the OWL Test Cases From my point of view your message reveals the purpose of these tests - to break your system !! (And other peoples). 
A failing test is an opportunity to improve your code. 
We chose to include tests from previous work by the Description Logic community. 
We hoped to gain from their experience of some things that are difficult to implement. 
The tests in the test suite are intended to have a range of difficulty, so that even the best systems struggle to pass all of them. 
We have tried to avoid really impossible tests (except perhaps in the extra credit section). 
In the acknowledgements section there is the list of test authors. 
You will see that it is fairly long, and because of that the test themselves show a variety of flavours - those authored by myself and Sean tend to have rather cryptic abstract concept names - and we do not appear to be thinking about a real world problem. 
I personally tend to think about OWL in a fairly abstract way, and my tests are merely symbolic manipulation. 
Those from Dan Connolly or Jos De Roo tend in general to refer to real world problems, and hence tend to be easier to understand. 
I hope this message helps - I take it that your comment was not a request to change the document in any way, merely one implementor talking to another ... If you actually want it to be taken as a formal comment, maybe a request for additional clarifying text to be included in the document, please reply and I will take such a request to the working group. 
(Personally I would not be too happy - because it looks like a lot of work to do that for every test). 
Good luck, I hope you get the rules right soon. 
Jeremy Hi, Thanks so much for your comment and links to the docs. 
I'm carefully reviewing the documents and my OWL inference rules... 
As for my previous message, I was just asking for some help from DL gurus and implementors. 
It was not a formal comment or request. 
Thanks. 
Best, Minsu The tests come from data used in a system comparison at DL'98 [1], but were derived from modal logic tests devised by Heuerding and Schwendimann. 
Ian @TechReport{Heuerding96a, author = {A. 
Heuerding and S. Schwendimann}, title = {A benchmark method for the propositional modal logics {K}, {KT}, and {S4}}, institution = {University of Bern, Switzerland}, year = 1996, type = {Technical report}, number = {IAM-96-015}, month = Oct [1] http://dl.kr.org/dl98/comparison/data.html 
