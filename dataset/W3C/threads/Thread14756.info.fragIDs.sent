In our call we wrestled with 2.1 (new) 
Some of the approaches from that 1.Operable with device independent handlers - does this include text input? 
- what are these? 
- do they apply to all technologies? 
- Would they all be operable from keyboard? 
2.Operable from Keyboard 
- assumes all devices have a keyboard? 
- what keys on keyboard? 
(function keys too)? 
- what keyboard? 
Apple? 
PC? 
- Can one assume that a kiosk will have that many keys? 
3.All functionality is operable using event handlers that can be activated with commands. 
- can text be input with commands? 
- What does commands mean? 
4.All functionality is operable via text input plus command operable events 
5.All functionality operable via text input plus tab, up, down, 
left, right, and enter. 
(these are the text and command keys that can be ensured would be on all 
"keyboards" (real or virtual).) 6.All functionality is operable via text input plus 
Step-to-next-control, up, down, left, right, and activate. 
Have fun Gregg Gregg C Vanderheiden Ph.D. Professor - Human Factors Depts of Ind. 
Engr. 
&amp; BioMed Engr. 
Director - Trace R &amp; D Center University of Wisconsin-Madison Gv@trace.wisc.edu 
mailto:Gv@trace.wisc.edu , http://trace.wisc.edu/ 
For a list of our listserves send "lists" to listproc@trace.wisc.edu 
Some of the approaches from that 1.Operable with device independent handlers - does this include text input? 
- what are these? 
- do they apply to all technologies? 
- Would they all be operable from keyboard? 
I think this is important. 
There has been some discussion in the User agent 
group about how to use DOM 2 to provide device independent handlers - essentially the user agent decides what triggers to give, and can be configured. 
character input is one approach to triggering them, mouse/keyboard commands and menus is another. 
2.Operable from Keyboard - assumes all devices have a keyboard? 
- what keys on keyboard? 
(function keys too)? 
This is too general (what keys, is there a keydown/keyup function? 
etc) and 
too specific - not all devices have a keyboard, and many have only a very 
small one. 
3.All functionality is operable using event handlers that can be activated with commands. 
- can text be input with commands? 
- What does commands mean? 
This just seems a bit vague on what it really means. 
4.All functionality is operable via text input plus command operable events 
this starts to make sense, but I don't like it - it should be possible in 
most cases to just use a mouse to drive everything, yet that doesn't seem to 
be supported here. 
5.All functionality operable via text input plus tab, up, down, 
left, right, and enter. 
(these are the text and command keys that can be ensured would be on all 
"keyboards" (real or virtual).) No they are not. 
One of my two keyboards doesn't have this. 
And speech-based 
systems don't have up, down, left, right as ways of relating things. 
This is 
too specific to visual environments. 
just my 2c worth chaals 
in 
seem to 
Question, How would you enter text with a mouse? 
(Using an on screen keyboard doesn't count since that is a keyboard as 
far as the application is concerned). 
all 
speech-based 
This is 
3 Questions 1- Which keys were missing. 
The arrowkeys? 
2 - What speech input system doesn't provide a way to operate keyboard 
keys? (One on a system without a keyboard?) 3 - How about 
--- Text input plus "step to next" (TAB) and "Activate". 
(ENTER). 
The arrowkeys can be optional but all function needs to be operable with 
text and the two functions. 
Gregg 
Gregg Vanderheiden Ph.D. Ind Engr - Biomed - Trace, Univ of Wis gv@trace.wisc.edu 
agent 
etc) and 
very 
in 
seem to 
all 
speech-based 
This is 
this starts to make sense, but I don't like it - it should be possible in 
most cases to just use a mouse to drive everything, yet that doesn't seem 
to be supported here. 
Question, How would you enter text with a mouse? 
(Using an on screen keyboard doesn't count since that is a keyboard as 
far as the application is concerned). 
response: Yes, I mean that it should be possible to use an onscreen keyboard (as is 
done in kiosk type environments in some cases). 
The point is that most users 
now prefer to use a mouse, and in some situations (such as the EIAD browser 
designed for people with brain injuries) rely on a touch screen. 
Using the keyboard to move around requires an abstraction of navigation. 
Point and click doesn't, for folks who can use it. 
We need to support both 
cases, I think. 
ALSO in response to 5. All functionality operable via text input plus tab, up, down, left, 
right, and enter. 
(these are the text and command keys that can be ensured would be on all 
"keyboards" (real or virtual).) No they are not. 
One of my two keyboards doesn't have this. 
And speech-based systems don't have up, down, left, right as ways of relating 
things. 
This is too specific to visual environments. 
3 Questions 1- Which keys were missing. 
The arrowkeys? 
Yep, my phone doesn't have them, nor a tab key. 
And text entry isn't too 
efficient either. 
2 - What speech input system doesn't provide a way to operate keyboard 
keys? (One on a system without a keyboard?) 
One which isn't simply a speech input interface to a desktop computer model - 
for example a VoiceXML application, or similar system. 
Again, my phone has 
voice control of many functions, but not voice simulation of keyboard use. 
3 - How about --- Text input plus "step to next" (TAB) and "Activate". 
(ENTER). 
The arrowkeys can be optional but all function needs to be operable with 
text and the two functions. 
Well, I prefer to start from "device independent mechanisms, including direct 
activation where available (e.g. Voice, point and click) and navigation among 
options (e.g. "next", "previous", "activate" using voice commands, or keyboard input) By the way, this is the kind of problem that User Agent group has dealt with 
fairly extensively - it would be worth asking their thoughts in my humble 
opinion. 
cheers Chaals 
Hi Charles. 
I am very confused. 
Are you saying that all web pages have to be navigable and operable from a phone keypad that doesn't even support tab? 
Gregg Gregg Vanderheiden Ph.D. Ind Engr - Biomed - Trace, Univ of Wis gv@trace.wisc.edu 
From: w3c-wai-gl-request@w3.org 
[mailto:w3c-wai-gl-request@w3.org] 
On 
Behalf 
possible in 
seem 
as 
is 
users 
browser 
navigation. 
both 
left, 
all 
relating 
too 
keyboard 
model - 
has 
use. 
with 
direct 
navigation among 
dealt with 
humble 
Yes I am. 
But that isn't a problem - most web pages already are. 
It is just a question of how we phrase the requirement, so we can ensure that we aren't accidentally excluding something, or requiring an implementation detail too specific for the use cases we have. 
Cheers Chaals Hi Charles. 
I am very confused. 
Are you saying that all web pages have to be navigable and operable from a phone keypad that doesn't even support tab? 
Gregg Gregg Vanderheiden Ph.D. Ind Engr - Biomed - Trace, Univ of Wis gv@trace.wisc.edu 
From: w3c-wai-gl-request@w3.org 
[mailto:w3c-wai-gl-request@w3.org] 
On 
Behalf 
possible in 
seem 
as 
is 
users 
browser 
navigation. 
both 
left, 
all 
relating 
too 
keyboard 
model - 
has 
use. 
with 
direct 
navigation among 
dealt with 
humble 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) 
Though I haven't arrived at a satisfactory solution, I do have some ideas to share. 
1. 
The requirement under checkpoint 2.1 should not forbid the use of certain kinds of device-specific event handling. 
Rather, it should insist that the web content ought not to rely on them in order to be operable. 
2. Text input is, I think, a reasonable expectation. 
It isn't possible to design, for example, a large data base with a search facility that doesn't rely on text entry. 
We could however add a suggestion that alternatives to text entry be provided; actually I think this is already mentioned under the "handling of input errors" checkpoint, or at least used to be. 3. The problem is: how to define the kinds of input processing that we want to allow, without confining ourselves to implementation-specific scenarios which have undesired implications, or unduly restricting the range of implementation choices. 
The fundamental idea, I think, is that preference should be given to methods of input handling which can be characterized as logical actions within an application context, rather than as events which reflect the physical state of the input device. 
A clear example of the latter is an event which takes the coordinates of a pointing device as parameter. 
4. In general terms, then, the success criteria are as follows: a. 
Where the implementation technology provides logically defined events, ensure that the web application can be operated entirely using these events. 
b. 
Where the implementation technology provides only device-dependent events (that is, events which are described in terms of the state of specific input devices) provide parallel, redundant implementations of the input mechanism so that it can be operated entirely from each type of input device for which events are defined. 
Exception: support to enable pointing devices to perform character input is typically provided by assistive technologies and need not be supplied by the application itself. 
This is a very rough initial proposal. 
Improvements are welcome, as is a draft of what would become the actual checkpoint. 
The exception at the end of the previous paragraph is somewhat anomalous and could be refined further. 
