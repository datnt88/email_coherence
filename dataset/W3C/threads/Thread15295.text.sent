During the Techniques teleconference on Wednesday, some issues came up that we felt needed the attention of the entire group. 
These issues are testability, Techniques for technologies that don't meet WCAG, and variable user agent support for Techniques. 
1. Testability One proposed requirement is that Techniques state whether they are testable; that has been further refined to be that they should state whether they are a) machine testable, b) not machine testable but human testable, or c) not testable. 
"Machine testable" means that a machine can validate conformance to the Technique without human input. 
We include the possibility of "not testable" Techniques for those that apply to Additional Ideas in a WCAG Checkpoint and are not required to be testable. 
In discussion we identified a couple issues with this. 
First, the line between machine testable and not is fuzzy. 
As evaluation tools improve a Technique that was not machine testable may become so; therefore it may not be desirable to hard-code the testability nature. 
Second, we could not agree that Techniques could ever be truly untestable; if that were the case, there would be no measurable benefit if a document includes features guided by Additional Ideas. 
Although we did not propose removing the requirement to provide information on testability, we think that testability is unclear and needs further discussion. 
2. Technologies that don't conform to the guidelines For some technologies, it might not be possible to meet all WCAG checkpoints in that technology. 
There are two possible reasons for this: a) the technology is intended to be used alongside another technology that does meet WCAG, e.g., CSS, b) the technology is not fully WCAG-conformant. 
In the first case, there may be a "path" by which the technology essentially does meet the guidelines. 
When examining CSS used with HTML, if any of the Core, HTML, or CSS Techniques documents provide a Technique to implement a given Checkpoint, the CSS can be said to conform and the omission of relevant Techniques from the CSS Techniques document would not pose a problem - the Checkpoint would be deemed "not applicable". 
In the case that there is no possible way to implement a Checkpoint in a given technology (e.g., a plugin), there would be no "path". 
The omission of relevant Techniques has a different meaning and would be a problem as it would not be possible to follow the Techniques document and arrive at fully WCAG-compliant content - the technology is "non-conformant". 
The question is, how do we want to address this issue? 
One approach would be to provide Techniques for Checkpoints that can be met in the technology, and indicate that missing techniques were not omitted or not applicable, but could not be created. 
This would have the advantage of providing guidance for users to create documents or implementations that are "as WCAG conformant as possible", which is better than nothing, and supports the idea that some technologies are taking steps towards complete accessibility though they are not there yet. 
The second approach would be simply to say that the technology is non-conformant, and provide, essentially, a single Core Technique that says "provide equivalent alternatives if this technology is used". 
This approach supports the creation of fully WCAG-compliant content but at the cost of locking out technologies. 
Note that even though we do not see this problem arising immediately because WAI will only provide Techniques for vendor-neutral technologies, we desire to address this issue in the requirements. 
This is because we hope that other organizations will adopt the format we have created to provide Techniques for some vendor-specific technologies, and because other W3C groups have expressed an interest in adopting the Techniques format we are defining, and the issue may arise with the guidelines used by those groups. 
3. User agent compatibility There is a requirement that each Technique state the user agents in which it works. 
This is useful information, but complex and subject to frequent change. 
For instance, for some Techniques there may be no current UA support. 
For others UA support may be partial or variable (e.g., accesskey is supported by some browsers now but it locks out access to menu items that use the same access key - is that "full support"?). 
There may be other Technique combinations in which a given technique is preferred when target UAs support it (e.g., tabindex to move navigation to the end of the tab cycle) but an alternate Technique may otherwise be needed (e.g., a skip navigation link). 
Furthermore, User Agent support is likely to change fast, possibly faster than we are likely to update the Techniques documents. 
There is therefore a desire to externalize the user agent support information, and point to it from the Techniques document. 
If we do this, we will need to set up a repository of that information. 
We will additionally need to figure out how Techniques documents can point to this information, when the information is newer than the Techniques doc. 
One possibility would be that the repository actively point into the Techniques rather than the other way around, but if so how does this affect our definition of this requirement? 
Michael Cooper Accessibility Project Manager Watchfire 1 Hines Rd Kanata, ON K2K 3C7 Canada Here is a proposal. 
A technique is machine testable if and only if there is known to be an algorithm that will determine, with complete reliability, whether the technique has been implemented or not. 
This is intended to exclude probabilistic approaches, which by their nature involve some possibility of error. 
Techniques that are not machine testable are human testable, but there may exist probabilistic or other techniques for assisting the human being in carrying out the testing by identifying a large class of possible cases of non-implementation. 
My focus on algorithms is intended to take advantage of the fact that tools change much more quickly than algorithms do, and if a knowledgeable person can't identify an algorithm that will yield the correct result reliably, then the technique isn't machine testable in the sense that Michael characterized above (of not requiring human input) until some advance has been made not just in evaluation tools, but in the state of the art of, say, artificial intelligence or some other field of research. 
Second, we could not agree The working group has already agreed that a success criterion, and this applies to the techniques also, is deemed untestable if it is considered that 8 out of 10 informed evaluators wouldn't agree in their judgments in a very wide variety of cases. 
The term "non-testable" may be misleading; it doesn't mean that no possible evidence can count for or against the claim that a technique has been implemented, it just means that in the opinion of the working group the available (human) testing methods don't meet the required threshold of reliability. 
In practice, any technique related to an item appearing under a WCAG review requirement (including those under the "additional ideas" sections of the current document) won't be testable in the relevant sense, though as Lisa pointed out in connection with checkpoint 4.1 there may exist tools that can facilitate a human evaluator by applying suitable heuristics. 
The above proposals are based as far as possible on consensus decisions of the working group with respect to the notion of testability. 
I have tried to provide clear and workable definitions that address the issues identified. 
I suggest that in the case under discussion we should establish a rule that no techniques may be published by the WAI for technologies in which, even if combined with other relevant technologies, can't be used to attain conformance. 
We can't set rules for other organisations; all we can do is ask them not to publish techniques documents using our format that are misleading as to whether their technologies support WCAG conformance or not. 
My concern is that if a content developer finds a techniques document corresponding to a given technology, the assumption will be made that it is possible to write accessible content using that technology, even if the only way to create WCAG-conformant content is to provide a complete alternative version in some other format. 
Thus to avoid the confusion that would otherwise result I suggest we make it clear that we strongly discourage the publication of such techniques documents until such time as the technology has been improved to the point of supporting WCAG conformance, and that if such techniques documents (for technologies that don't presently offer the necessary support) are published, they should carry clear and prominent notices stating that pending future developments, it is not possible to write WCAG-conformant content using this technology, and advising that alternative versions of the content in other formats be provided. 
To summarize, I am suggesting that (1) we don't publish any such documents, and (2) that we discourage others from doing so, but if they are going to do it they should provide very clear statements to developers to the effect that their technology doesn't support the creation of WCAG-conformant content. 
As I said, though, we don't have any control over what third parties might decide to publish. 
However, it would be very misleading indeed to publish a document that purports to be WCAG 2.0 techniques, but for a technology in which it isn't possible to create WCAG-conformant content. 
It is also an open question whether we will have the resources to track down this kind of information. 
One solution would be to create a repository in which third parties (including user agent developers) can provide information on u a support, though of course unless it can somehow be verified we would have to provide it with appropriate disclaimers as to accuracy. 
Of course, references to information maintained by third parties are an ideal solution. 
I suggest we may want to consider reworking the requirement somewhat. 
Instead of listing the user agents that do or do not support a technique, we should only require that for each technique we identify whether or not the necessary feature support has been implemented at all, by any user agent, as at the time of publication. 
Another option would be to link to third-party information on u a support without providing any ourselves, or to encourage u a developers to volunteer this information (linking to it if they do). 
My concern is that we won't have the resources to obtain this information comprehensively. 
After all, our resources are stretched to the limit as it is. 
