I'm interested in receiving feedback from the group on whether they feel having distributed authoring and versioning functionality is best performed via a POST (with many new content types), which is descreibed in the current spec., or whether it would be better to have this functionality implemented as many new methods, with parameters in headers, and mostly blank entity types. 
The primary constraint, I think, is how various proxy &amp; security gateway services might deal with POST-with-new-entity-body vs. a new method. 
A survey ("what's actually implemented?") 
would be useful, since otherwise we're left with speculation. 
A secondary issue (which doesn't actually affect the choice) is the question of cache invalidation, e.g., after copy(a, b), any cache entries for B should be invalidated even if are otherwise fresh, if we're going to require sequential transparency of information delivered through the same set of proxies. 
E.g., if you do copy(a, b) and then ask for b, then YOU see the version you copied even if others who use a different cache might be updated later. 
This is already an issue for POST, PUT and DELETE, but http-wg didn't (yet) create any mechanism for doing this. 
Larry I was thinking more of a return header from ANY request that identified a set of other URLs whose cache entries should be marked stale. 
So, if you POST a new entry to you might get back a return header that it updated: or (even) This puts the computational burden on the update method rather than retrieval, and is predicated on an assumption that reads happen far more frequently than writes. 
Larry This is actually an issue in which I have an interest. 
At present, the approach I am contemplating is to 1. Require that the resource have a validator which is guaranteed to be replaced with a new value upon update. 
I.e., an update to a resource cannot result in a new validator which is equal to the validator for an earlier version of the same resource. 
2. Perform conditional GETs using If-Match on the validator. 
This is not yet an alorithm and there are certainly issues with HTTP/1.1 caches, but I think that a HEAD request will return the current entity tag in all cases, and once this value is retrieved a conditional GET using If-Match will fail in the case of a race condition (an intervening PUT, POST or DELETE that modifies the resource). 
I hope you'll forgive the preliminary nauture of my thoughts here. 
Gregory Woodhouse gjw@wnetc.com home page: http://www.wnetc.com/ 
resource page: http://www.wnetc.com/resource/ 
I like the idea of placing the burden on the update method (this sounds a lot like write-through) but I don't see how this method can guarantee that a returned entity is up to date. 
The most obvious problem is that an update can traverse an entirely different set of caches than a previous GET, thus allowing for the possibility that some caches would not yet be aware that its cache entry for the resource is stale. 
Of course, it everything works fine as long as both paths hit the same cache at some point. 
Gregory Woodhouse gjw@wnetc.com home page: http://www.wnetc.com/ 
resource page: http://www.wnetc.com/resource/ 
There's both a previous GET and a subsequent one: GET a / modify b / GET a where the first &amp; second GET use the same cache but the Modify uses a different cache. 
In such a situation, the cache used by GET has to know to revalidate for the second one: it either has to revalidate every time or else somehow be notified. 
I can't think of any way around this situation. 
I think this could be handled by adding a requirement on HTTP clients (or intermediate proxies in a chain) that switch between proxies: If the client(proxy) has performed an UPDATE on a given URL, and then switches to a different (subsequent) proxy, the client should include in each request to any potentially affected URLs a 'max-age' which is less than the time since the last UPDATE. 
The simplest way to implement this is just use 'time the proxy server changed', and the next simpler is just to remember, for each host, the time since the last update method to that host. 
Of course, finer-grained information can be kept. 
This still puts the implementation burden on 'caches that can switch proxy servers', which is probably where it belongs. 
Larry I like this idea - but it isn't very practical, I fear. 
If the request is handled by a CGI, then how does the server know what's been updated? 
For highly automated sites, the list of URLs could be huge. 
In fact, a set of URLs which access a database is pretty much a one-way function as far as calculating validity goes (though I did have this crazy idea about inverse SQL once...). 
Cheers, Ben. 
Technical Director URL: http://www.algroup.co.uk/Apache-SSL A.L. Digital Ltd, Apache Group member (http://www.apache.org) 
London, England. 
Apache-SSL author Note: I recognize that cache consistency could be considered outside the purview of the distributed authoring group, so I can certainly understand if others think this discussion should be moved. 
On the other hand, it seems to me that resolution of this issue is essential to the proper functioning of distributed authoring systems. 
The difficulty here is tha the processes involved my be unrelated, even unaware of eachothers' existence. 
If they reside on separate hosts they may follow different proxy chains simply because of network topology and no switching of proxies involved. 
One possibility that presents itself is to use the Via: header. 
Since the resource itself resides on only one host, there is absolute time. 
In particular, from the origin server's point of view, the requests affecting a given resource cqan be totally ordered. 
Now, if the origin server always returns a resource with an entity tage containing the Via: header from its most recent update, then a client doing a subsequent GET will be able to determine that an update followed a different proxy chain and therefore know that it must revalidate the resource. 
Gregory Woodhouse gjw@wnetc.com home page: http://www.wnetc.com/ 
resource page: http://www.wnetc.com/resource/ 
I'm sorry, I was being really terse when I talked about "sequential transparency". 
What I meant was that if client A does a "GET x; UPDATE y; GET x" that the second "GET x" properly reflects the changes made by "UPDATE y"; however, if client B does "GET x", even at a time AFTER the "UPDATE y" happened, client B might see the original data. 
If what you want is "sequential transparency" then you don't need to deal with totally unrelated processes, you only have to deal with a single process (client A) which potentially might switch between one proxy and and another. 
Larry a) use patterns to invalidate (mark stale) many URLs at once (this was the '*' in http://host.dom/container/3q96/*) 
b) Don't invalidate if you don't care about sequential transparency c) If you don't want to invalidate things because there are too many and you don't want to warn about updates, then don't let servers cache them without validation (e.g., send them out originally with max-age=0). 
So, I disagree that it 'isn't very practical': it's practical in many situations, and when it's not practical, you don't have to use it. 
Larry I've been doing other stuff, so haven't been able to process everything as fully as I want. 
However, I've got some comments and I'll mouth off even thought I'm in arrears for the revised requirements document. 
I think that Yaron has a point about the equivalence of POST w/ content types, and separate methods, but only when you consider the mechanisms in isolation from the way things already are. 
We had about 8-9 messages on the caching implications of the new update operations, including several proposals for ways to revise HTTP to handle this kind of cache-invalidation. 
I think many of you will agree that this is a more-general HTTP issue, and should be solved in that context. 
Given that assumption, separate methods are a better solution, since they don't provide any new constaints on the HTTP group: They already have seaprate methods with update semantics. 
Adding a few more methods to the list will not complicate things in principle, while if we overload POST we may be creating POST operations that need special processing that the POST already in place might not be able to deal with. 
It also seems to me that the separate method approach is closer to the original simple OO model behind the WEB, with operations defined on resources. 
It also seems to me that having content-types for our new operations like MERGE will make supporting multiple data formats for input to these operations easier. 
-- David David Durand dgd@cs.bu.edu \ david@dynamicDiagrams.com 
Boston University Computer Science \ Sr. Analyst --------------------------------------------\ http://dynamicDiagrams.com/ MAPA: mapping for the WWW \__________________________ The issue is change control: once you define the semantics of a new method, there's little or no way to change it or update it. 
Adding new methods is currently (intentionally) difficult. 
PEP might make it easier, but I'll believe in PEP when I see more progress on it. 
On the other hand, there's a well defined mechanism for defining, modifying, agreeing on, registering new media types. 
So "POST with new media type" isn't equivalent to "new method" in the important dimension of "what happens if we get it wrong". 
Larry I'll disagree with Larry and Yaron on this one -- there is a giant difference between using media types to define the intended action and using methods to define the intended action. 
a) access control is based on methods, not media types. 
It is true that you could change all WWW software and HTTP semantics such that you could do access control via media types, but there had better be a damn good reason for it [I haven't seen any yet]. 
b) the HTTP interface is designed to be capable of being the interface to a general object store, where the method really is an OO method to be applied to an object. 
For a variety of reasons, it is better to have separate names for separate semantics, rather than a single name for all method calls and having the object determine the semantics by some case-based switch on one of the parameters. 
I'll also disagree with Larry on the notion of media types being any easier to change than methods. 
Anybody ever try to change application/x-www-form-urlencoded (the media type used by default in WWW form-based entry)? 
That was an incredibly poor design decision, known from the start, and yet we still can't get rid of it. 
I personally would rather have the definition of standard methods go through the RFC process; non-standard methods don't have to go through any process. 
As an implementer, it is easier (and better) to add support for a new method to the Apache server than it is to add access control by media type. 
If you get it wrong, just change the method name. 
.....Roy # a) access control is based on methods, not media types. 
It is true that # you could change all WWW software and HTTP semantics such that you # could do access control via media types, but there had better be a # damn good reason for it [I haven't seen any yet]. 
Surely this isn't a HTTP semantics issue, but it is a WWW software issue. 
HTTP access control policy is completely up to the server, and if you wanted to write a server that allowed POST-ing .png 
files and but not .gif files, I don't think that would change the semantics of HTTP at all. 
# b) the HTTP interface is designed to be capable of being the interface # to a general object store, where the method really is an OO method # to be applied to an object. 
For a variety of reasons, it is better # to have separate names for separate semantics, rather than a single # name for all method calls and having the object determine the # semantics by some case-based switch on one of the parameters. 
The first sentence is a stretch, but I'll go along with it. 
And I'll agree with the second sentence. 
But we're frequently bluring the semantics of a whole host of operations under 'POST'. 
# I'll also disagree with Larry on the notion of media types being any # easier to change than methods. 
Anybody ever try to change # application/x-www-form-urlencoded # (the media type used by default in WWW form-based entry)? 
That was an # incredibly poor design decision, known from the start, and yet we still # can't get rid of it. 
Well, actually, I did try to change application/x-www-form-urlencoded with RFC 1867: Form-based File Upload defines multipart/form-data and I think it's been modestly successful: there are applications that now use it for form submission. 
Those applications didn't need a change to HTTP to do so. 
But I agree, this is a weak argument. 
We should just be careful that we have the semantics we'll want for a while before we start putting in 'PATCH' and 'REVERT-TO-VERSION' as HTTP methods. 
Larry There's not a big difference between the two world views you described ('a collection of objects with methods' and 'a collection of agents'), and besides, the 'world view' of Yaron and Roy (or you and me) don't really matter much. 
And besides, while the design decision may be influenced by 'world view', there are usually other considerations that are more compelling than first principle arguments: compatibility with current implementations, extensibility, performance, reliability, etc. 
I think we can make progress if we just hold off on 'world view' arguments and focus on the other considerations. 
Larry # I have discussed this off line with Roy and frankly I think this is a # religious issue. 
Roy and I have fundamentally different visions of what # HTTP should become. 
I think your characterization of this as a 'religious' issue is insulting, and the idea that a discussion at the November conference might resolve something about the future of HTTP is presumptuous. 
I've yet to see anything here that constitutes a 'religious' argument. 
I've tried to raise in a constructive manner the concerns about representations of metadata and document attributes in network protocols and in HTTP that I've seen, and raised the points in a way that I'd hoped was constructive. 
Well, having been able to overhear Roy's end of he and Yaron's phone conversation, it was evident to me they had very different views on this topic. 
In a nutshell, Roy and Yaron differ in their model of a web server. 
Roy sees a web server as a collection of objects, with methods defined on them, a la object-oriented programming. 
Yaron sees a web server as a collection of agents (computational entities), of which some serve documents, while others perform activities like "copy" or "server diff." 
In fact, there may be many agents capable of performing an activity, and a single agent may be capable of handling more than one type of activity. 
In Roy's view (Object Oriented) of HTTP, an HTTP message is directed to an object (the Request-URI), which handles the method in the request message. 
In Yaron's view (Agent) of HTTP, an HTTP message is directed to an agent (or a default HTTP agent), which then handles either the HTTP/1.1 style message or processes the command specified in the MIME type of the request message. 
The advantage of the Agent view is the range of capability of the agents isn't hardwired, and there may be many agents, with slightly different characteristcs, all active simultaneously in the same server. 
The advantages of the Object Oriented view stem from the fixed set of methods: this fixed set is understood better by existing Web technology (e.g., caches), and can be used to implement a simple access control scheme (method x user -- ACL). 
I think it is this difference in how Roy and Yaron view HTTP which Yaron was referring to when he wrote, "Roy and I have fundamentally different visions of what HTTP should become." 
While I agree this issue should be an item for discussion at the upcoming meeting, I also feel that some discussion of this issue on the list is of value before we meet. 
- Jim Jim summarized ... Ummmm, not quite. 
My view (and my experience) is that HTTP is an interface mechanism between clients and servers. 
Any reduction of that interface based on the assumed needs of one particular implementation of an HTTP server is just plain wrong -- wrong because it limits the possible implementations of whatever it is you want to accomplish. 
JigSaw and Apache are two very different ways of implementing a server, but both ways have their advantages. 
(JigSaw is OO-based and Apache is handler-based). 
Likewise, a Hyper-G server's view of what a "directory" may be is substantially different from that of the NCSA or CERN servers. 
Nevertheless, everything that you might want to do in the way of distributed authoring can be done in such a way that the implementation is independent of the interface, unless you make the mistake of assuming things about the implementation when you design the interface. 
HTTP includes methods (that define actions) and resources (that define the point of the server's namespace) and parameters (request headers) and data (entity headers and body). 
What the server does with those things is defined by the semantics of the method and acknowledged by the server's response (including Vary, if necessary). 
How the server does it is none of HTTP's business. 
Gack! Bzzzzt. 
There is no such thing as a fixed set of methods, and the fact that all (not just simple) access control schemes are based on the method is because the method defines the semantics for the request or, in the singular case of the POST method, the resource is given complete control over the semantics. 
The notion that the media type defines those semantics is totally unchartered territory. 
I understand that it is possible to ignore that and to do everything under a single method and to encapsulate the semantics of the request within the message body. 
The reason we don't do that is because it is simpler for a client (which must consider the presentation of the semantics) and a server (which may have multiple layers of access control) to embody the semantics of a request within the method. 
Aside from access control, it also frees the media type from having any semantics other than "this is the format of what is contained within the body." 
All of the above are (at least some of) the technical arguments regarding why actions should be defined as methods and not as media types. 
If you would like a religious argument as well, then: That's the way we've always done it, and that's the way we've implemented the existing Web, so that's the way I would expect any standard to develop unless there is a good reason against it. 
If we are talking about extensions to HTTP/1.x, then I expect those extensions to follow the framework of HTTP. 
If you don't like that framework, then there is nothing stopping you from defining a protocol which is not HTTP/1.x, or extend some other protocol, like z39.50 or FTP, which have different frameworks. 
What you lose is the existing implementation base and the advantages provided by HTTP's generic interface, but you will lose that anyway if you require things that do not correspond to the existing framework. 
.....Roy Ah! Terribly good point: if we say that feature X is triggered by using media type MX, then we've lost the ability to use other data formats to do X; we've lost the traditional 'format negotiation' degree of freedom. 
One way to address that is to make sure MX is a compound (aka multipart) data format with a typed 'payload' subpart. 
But that starts to get contorted Real Fast. 
So I'm in favor of separate methods for COPY, MOVE, RENAME, and other long-standing filesystem operations. 
My convictions are less strong about less traditional idioms like BROWSE, LOCK, etc., but my intuitions are in the same direction. 
The only time I struggle with the issue is when the operation can be piggy-backed on GET, PUT, or POST with 'carrier wave' headers; for example, GET-with-lock could use a GET method with funky headers. 
That reduces the: C: LOCK R S: 2xx OK, here's the lock ---or--- S: 5xx Huh? what's LOCK? 
C: GET R S: 200 OK, here's the document conversation to: C: GET-with-lock R S: 2xx OK, here's the document and the lock ---or--- S: 200 OK, here's the document (doesn't grok with-lock) Even that's a bad example, since consuming a resource like a lock is a goofy thing to do in an idempotent[sic] method like GET. 
Dan This kind of philosophizing is fun, but I think that the HTTP framework for 'POST' really does allow a fairly wide range of semantic options based on the content (not the content-type, but the content) of the message. 
On a different discussion group, I was asked how to implement printing using HTTP, and in particular the operations # SubmitJob # CancelJob # ListJobAttributes # ModifyJob # ResubmitJob I suggested: # SubmitJob could easily be a POST which returns a job attributes and a # Location: header that is the Job URL. 
CancelJob could be done with a # DELETE of the Job URL, ListJobAttributes might be GET of the Job URL, # ModifyJob could be a PUT of the Job URL with a new set of Job # Attributes (or Post). 
I don't know what ResubmitJob might be, but # perhaps it's a POST too. 
Now, just so we don't think we've gone too far from 'distributed authoring and versioning', remember that 'distributed authoring' blurs into 'workflow' pretty easily, so that each document uploaded might in fact be accompanied by some processing instructions, and that the processing instructions might be considered the metadata. 
In fact, many applications of 'distributed authoring' do exactly that: transaction semantics on document metadata allow a complete workflow system to be built. 
So, is it in the spirit of HTTP to add a SUBMIT method, or is POST of the job instructions to the job recipient URL good enough? 
Larry 
