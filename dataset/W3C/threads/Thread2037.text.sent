Apologies if this is clear in the text, but it didn't seem to be when I scanned 1.1. 
Some older proxies seemed to be modifying URLs, e.g., if you GET http://foo.com/test#frob 
HTTP/1.0 they might ask foo.com for GET /test%23frob HTTP/1.0 or vice versa. 
Is there any reason to disallow this, and if so, what language would be put in the spec to disallow it; alternatively, if proxies might do this kind of transformation, what should we say? 
Any digest that included the URI would be wrong if the URI is munged by the proxy. 
It would break Digest Authentication, for example. 
] From: Larry Masinter masinter@parc.xerox.com 
] http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com ] Subject: proxies rewriting URLs ] Date: Wednesday, March 06, 1996 11:52PM ] Apologies if this is clear in the text, but it didn't seem to be when ] I scanned 1.1. 
Some older proxies seemed to be modifying URLs, e.g., ] if you ] GET http://foo.com/test#frob 
HTTP/1.0 ] they might ask foo.com for ] GET /test%23frob HTTP/1.0 ] or vice versa. 
Is there any reason to disallow this, and if so, what ] language would be put in the spec to disallow it; alternatively, if ] proxies might do this kind of transformation, what should we say? 
Well, it's really a question of what should a proxy do when it receives a syntactically invalid Request-URI. 
My preference is that it make no changes whatsoever to the path part of the Request-URI when it forwards the request. 
However, this would mean the proxy would be sending a non-compliant request, which is also bad. 
I guess that the best thing to include is a paragraph saying A proxy may encode portions of the Request-URI prior to forwarding the message if and only if such encoding is necessary to make the message compliant with HTTP. 
Where possible, proxies should attempt to use the same characters in the forwarded Request-URI as were received from the client, since some servers improperly apply special semantics to characters outside the "reserved" set. 
Note, however, that "#" is never allowed in a Request-URI. 
...Roy T. Fielding Department of Information &amp; Computer Science (fielding@ics.uci.edu) 
This language doesn't really go together. 
"If and only if" implies necessity, at least to any mathematically oriented readers, so it ought to be "must", not "may", or else you should get rid of the "if" part. 
--Shel I think that it is a question of what people can encode into URIs. 
URIs do not specify a canonical form so that there can be several URIs which logically mean the same thing. 
If we are clear on the "meaning" part then two syntactic variants of the same URI should be interchangeable. 
If something breaks because of this problem then it is something which relied upon a syntactic variation and was therefore broken. 
For example I might embed a href="http://foo.bar$" 
into a document where $ is some ambiguous character. 
I might know that the xyz browser will canonicalize differently if the URI comes from the browser or from the bookmark file and have one of those $5million venture capital "companies" wrapped arround this hack (don't laugh I've seen this type of thing more than once). 
Personally I think that trying to provide support for such constructs is ill advised. 
It is likely to close up options down the line. 
In particular I'm thinking of HTTP-NG here. 
I can think of many reasons why a proxy might choose to canonicalize URIs internally, cache matches for one. 
If one considers the action of a caching proxy I think that canonicalization of URIs in passed on requests is likely to be highly desirable. 
Since Larry reports that there are already proxies doing this sort of transformation I think it best to leave things as they are but include a warning to state that problems might occur. 
Phill Absolutely! 
A proxy that did not return the cached document http://foo/%7Euser in response to a requset for http://foo/~user is not behaving efficiently. 
So proxies need to canonicalise the URL for the cache key. 
Hence it would be confusing if the proxy did not use this canonicalised URL in requests it issued. 
This is why the proxy in Apache 1.1 beta does a lot of URL rewriting; not only %xx - char as appropiate, but also and (perhaps dubiously) http://foo/bar? 
- http://foo/bar David Robinson. 
] From: John Franks john@math.nwu.edu ] Date: Sunday, March 10, 1996 4:41PM ]  Any digest that included the URI would be wrong if the URI is munged by ]  the proxy. 
It would break Digest Authentication, for example. 
] Actually, a proxy munging the URL will cause no problem for digest ] authentication. 
The URL is duplicated in the uri field of the ] authentication header to deal with exactly this issue. 
Of course, ] if a proxy munges the Authorization: header then there will be ] problems. 
Interesting. 
What happens if I do this: GET /secret.txt 
HTTP/1.1 Authorization: uri="/public.txt", 
nonce="deadbeef", response="0123456789abcdef0123456789abcdef" If the server checks the authorization header and its URI, but then uses the URI from the Request-URI in the request line, the whole exercise will have been wasted. 
And if proxies are allowed to munge the URI in unknown ways, the server can't compare the request-URI with the uri in the Authorization header. 
The Digest draft should say the the server MUST use the URI from the Authorization header, as that is the only one that has been authenticated. 
Paul A proxy that did not return the cached document http://foo/%7Euser in response to a request for http://foo/~user is not behaving efficiently. 
So proxies need to canonicalise the URL for the cache key. 
Hence it would be confusing if the proxy did not use this canonicalised URL in requests it issued. 
I think it would be more "confusing" if by canonicalizing a URL the proxy turned it into something that identified a different resource. 
I.e., we should be quite cautious about pursuing "efficiency" here. 
Efficiency is great as long as it gets the right answers. 
This is why the proxy in Apache 1.1 beta does a lot of URL rewriting; not only %xx - char as appropiate, but also This might be appropriate in some cases, but it's clearly not appropriate for every instance of %xx. 
RFC1738 specifically states, for example, The character "#" is unsafe and should always be encoded because it is used in World Wide Web and in other systems to delimit a URL from a fragment/anchor identifier that might follow it. 
The character "%" is unsafe because it is used for encodings of other characters. 
so a proxy that converted to would be non-compliant with RFC1738. 
I'd like to see a specific citation to a standard or even an I-D that makes this a compliant transformation. 
In draft-ietf-http-v11-spec-01.txt, for example, I find this BNF: URI = ( absoluteURI | relativeURI ) [ "#" fragment ] absoluteURI = scheme ":" *( uchar | reserved ) which suggests that the URI need not end in "/". 
and (perhaps dubiously) http://foo/bar? 
- http://foo/bar Very dubiously, especially since we also have a well-understood heuristic that caches do not store responses to GETs with "?" in the URL. 
If you have a series of caching proxies in the path, and the first one does this transformation (dropping the "?"), the second one will not realize that the GET is a potentially non-cachable query. 
-Jeff I would say we have to let this pass. 
Principally because of the large number of proxies out there which already do this. 
There is a principled reasoning behind this. 
We distinguish a proxy or gateway from a tunnel by the level at which the agent acts. 
Tunnels act purely at the syntactic level. 
Proxies and gateways act at the semantic level. 
We permit a proxy to perform any action provided it is semantically neutral and in addition may permit a number of semantic transformations. 
[Another area to consider is using URLs as a subliminal channel or a basis for steganography. 
The URL above effectively allows us to encode 0s and 1s. 
There may well be cases where we don't want that to happen. 
This is a fairly pointless nit to pick but I thought I would get in first] The reason why the semantic transformation stuff matters is that a firewall proxy may want to strip out all header lines it does not understand, effectively down-rating the connection to HTTP/1.0 or some other known safe subset. 
This requires a complete parse of the request and regeneration of the passed on request. 
The parser may wish to specifically check URLs for strict validity to avoid possible holes occurring with illegal escape sequences. 
Thus it is a legitimate proceedure to parse out the URL, cannonicalise and regenerate. 
I think that if we want strict syntactic netrality in a proxy we have to use the wrapped method. 
Phill Personally, I don't think proxies *should* rewrite URLs, and that origin servers should decide on logical equivalence and report it. 
This is the only way that servers that are not 'multihomed' but happen to have multiple DNS entries can be declared to be equivalent. 
Otherwise, we have to define equivalence for URLs unambiguously, and I'm reluctant to engage in that because it's likely we'll get it wrong. 
In the example I quoted, I showed GET http://foo.com/test#frob 
HTTP/1.0 turned into GET /test%23frob HTTP/1.0 but this wasn't actually the example; it was a proxy that turned GET http://foo.com/test!place 
HTTP/1.0 into GET /test%21place HTTP/1.0 
In general, I'm inclined to agree. 
IF the proxy believes the request to be a protocol violation which would require re-writing, it should instead reject the request with appropriate diagnostic insight. 
BUT the current specs indicate that: should become: If I understand they wording. 
I would not that at least one browser in common use does the opposite. 
Dave Morris 
