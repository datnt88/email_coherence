In reading draft-ietf-http-v11-spec-rev-03 I found some revisions which are potentially damaging. 
I'll discuss them below. 
I would like to see these issues resolved before proceeding to draft standard. 
- Section 8.2.3: 
This section now says: # If a user agent sees the transport connection close before it # receives a final response to its request, if the request method is # idempotent (see section 9.1.2), the user agent SHOULD retry the # request without user interaction. 
The `befire it receives a final response' is a bit ambiguous, but I think the most obvious reading is `before it has received the *entire* final response'. 
By contrast, the text in 2068 has the SHOULD in a much more limited case: ...and it sees the connection close before receiving any status from the server, the client SHOULD retry the request without user interaction.... 
I don't know if the change from `any status' to `the final response' was intentional or whether it was an editing mistake. 
In any case I consider the change to be quite dangerous: it requires a fully compliant user agent to automatically retry, for example, a GET request yielding a 1 MB response if the connection closes halfway due to some unusual error condition. 
The correct action for the user agent would be to alert the user of the unusual error. 
An automatic retry could lead to very nasty problem scenarios. 
In summary, I want the SHOULD retry condition to be restored to the one in 2068. 
- Section 13.10: This section introduces a new (as far as I can see) requirement: # A cache that passes through requests for methods it does not understand # should invalidate any entities referred to by the Request-URI. 
This may seem like a good safety measure on the surface but I think that it is in fact quite damaging. 
First, designers of new methods cannot benefit much from the above rule because 1.0 and 2068 caches will not adhere to it. 
On the other hand, the new rule introduces a performance penalty for new methods which do not in fact cause any invalidation. 
One such method would be M-GET, a GET extended with a mandatory extension, for example. 
The performance penalty blocks implied by the new rule makes certain ways of extending the protocol too expensive and thus shortens the lifetime of the 1.x suite. 
I want the requirement to be removed. 
- Section 14.2: rfc2068 had the sentence The ISO-8859-1 character set can be assumed to be acceptable to all user agents. 
but this sentence has been deleted in the new draft! 
I can't remember that there was any rationale or discussion on the list for deleting it. 
In any case, I think the deletion damages the protocol. 
It is important to specify a charset which is always supported by all clients, else servers have no reliable way of sending fatal error diagnostics in text/html entities. 
The deletion of the above sentence also makes the third paragraph in 14.2 slightly nonsensical. 
I know that the choice of ISO-8859-1 as the `always supported' charset has been a subject of contention in the past, but that is no reason to omit specifying a fallback charset. 
If implementation experience has shown that universally supporting ISO-8859-1 is too difficult, we can discuss changing the fallback charset to US-ASCII. 
In short, the above sentence should be un-deleted. 
- Section 14.44: This section introduces a new (as far as I can see) requirement: # The "*" value MUST NOT be generated by a proxy server; it may only be # generated by an origin server. 
I don't see any reason for having this requirement. 
The general rule should be that transparent proxies may never change or add a Vary header (this is already implied elsewhere in the spec I believe), and that non-transparent proxies can do whatever they want. 
By the way, proxies which support the TCN protocol extension _will_ sometimes generate "*", this is explicitly allowed by TCN. 
In short, the requirement should be deleted. 
In addition to the above points, I found a potential problem which was already present in 2068. 
It looks to me like there is a subtle requirement on proxies buried in the following text from section 4.4: # 4. If the message uses the media type "multipart/byteranges", and # the transfer-length is not otherwise specified, then this self- # delimiting media type defines the transfer-length. 
This media type # MUST NOT be used unless the sender knows that the recipient can # parse it; the presence in a request of a Range header with multiple # byte-range specifiers implies that the client can parse # multipart/byteranges responses. 
As far as I can see, a 1.1 proxy will not delete an existing Range header field when forwarding a request. 
According to the last sentence above, any 1.1 proxy which forwards a request with a Range header will be able to parse multipart/byteranges responses. 
This seems to imply that any 1.1 proxy intended for use with byterange-capable clients will always include code which can parse multipart/byteranges responses to determine their length. 
Was this the original intention? 
The start of 14.35.1 seems to imply otherwise. 
Has anybody tried sending a multipart/byteranges response without chunking and content-length through a 1.1 proxy which does not support byte range operations? 
Koen. 
I think I'm the instigator of this change. 
While your example seems benign enough, the danger is from methods that change the underlying object, e.g., M-PUT. 
The object in the cache would no longer look like the one at the origin server and must be invalidated. 
In the absence of a way to tell intervening caches to invalidate their view of the object the proxy cache has to do so by default. 
I suppose a compromise would be for a cache to mark a cached object as "must-revalidate" when it sees an unknown method that it passes along. 
Cache experts: would that work? 
Dave Kristol I think I'm the instigator of this change. 
While your example seems benign enough, the danger is from methods that change the underlying object, e.g., M-PUT. 
The object in the cache would no longer look like the one at the origin server and must be invalidated. 
In the absence of a way to tell intervening caches to invalidate their view of the object the proxy cache has to do so by default. 
I suppose a compromise would be for a cache to mark a cached object as "must-revalidate" when it sees an unknown method that it passes along. 
Cache experts: would that work? 
How does mark the cached object as "must-revalidate" differ from invalidate the cached object except that the former propagates the change to outbound caches? 
I'm not sure that Koen would view this as a compromise :-) Would it work? 
Well, the concept of invalidation-based protocols is in general not supported by HTTP. 
My preference is to err on the side of transparency rather than performance, although I agree with Koen that the transparency in this case might be somewhat illusory. 
But I'm not sure what the problem is; my understanding is that the whole point of creating the M-GET method is to prevent "proxies that do not understand the method" from forwarding it. 
I.e., they are supposed to return 501 (Not Implemented) or act as a tunnel (i.e., not cache anything). 
So any caching proxy that does forward M-GET does "understand" it, and isn't covered by the requirement that Koen objects to. 
-Jeff Well I'm confused (but you knew that already)! 
Section 13.10 says a cache *can* pass through a method it doesn't understand. 
But Jeff says M-GET is meant to prevent the forwarding. 
Seems like a contradiction to me. 
Dave Kristol In my mind, at least, invalidate implies "never return it in response to any later request", and usually, that means to delete it from the cache. 
Whereas "must-revalidate" implies "keep the bits in the cache, but do a conditional GET (or whatever) before returning them in any later request". 
Thus, even if they are technically identical, the implication I would form upon reading the two alternatives are quite different. 
In my mind, at least, invalidate implies "never return it in response to any later request", and usually, that means to delete it from the cache. 
Whereas "must-revalidate" implies "keep the bits in the cache, but do a conditional GET (or whatever) before returning them in any later request". 
Thus, even if they are technically identical, the implication I would form upon reading the two alternatives are quite different. 
I would agree that someone reading the phrase "should invalidate any entities" who has no understanding of the "HTTP way", and who has an understanding of CPU cache design, might understand it the way you did. 
However, section 13.10 (the one in question) says specifically: In this section, the phrase "invalidate an entity" means that the cache should either remove all instances of that entity from its storage, or should mark these as "invalid" and in need of a mandatory revalidation before they can be returned in response to a subsequent request. 
We added this paragraph precisely because the term "invalidate" was being used in various different ways by different people. 
-Jeff Jeffrey Mogul: You are right, I don't. 
I agree that Dave's new rule would add some extra safety to protect against outdated cache entries, but no absolute safety. 
The HTTP caching system was never designed to offer absolute safety. 
The system is unsafe in several ways: there may be a mesh of proxy caches so that some caches don't see the passing M-PUT, a proxy cache may switch to gateway mode, thus avoiding having to care about any rule applying to proxies, and legacy proxy caches won't invalidate anyway. 
The amount of safety added by the new rule is so small that it does not outweigh the loss in extensibility. 
Especially considering that a designer of a new method can have the same extra safety that would be provided by the new rule -- IF he wants it -- by including a content-location header in the response to the new method (see section 16.3 last paragraph). 
So as the safety/transparency side is very weak, I'd rather err on the side of extensibility. 
Hmm, as far as I recall, proxies which don't understand the method _will_ forward it in general. 
It is origin servers which are expected to return an error message. 
If they act as a tunnel, they also won't invalidate anything in the existing cache memory, which was the whole point of the rule. 
Koen. 
