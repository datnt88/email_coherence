A site has come to my attention that bears on the discussion we've had from time to time concerning developing a lexical pictorial language or representation. 
It takes a different approach though but provides some intresting information. the @sign article found here may be of particular interest. 
Hands-on Technolog(eye)s Touching The Internet voice: 301.949.7599 
I'm currently reading a popular exposition of humourous 20th century graphic design: 'A Smile in the Mind'*. 
how well do these images transfer their purpose to text? 
tx David @ is now linked from: I'd welcome any other pertinent links. 
sc_b_1/002-1068442-2388852 jonathan chetwynd IT teacher (LDD) j.chetwynd@btinternet.com on the web" 
AG:: OK, let me give this away. 
This is worth an NSF grant, in my opinion. 
I had been dog in the mager hoarding it on that basis. 
What you can do with a small matter of programming is a cross between Google and Atomica. 
The joy of this is that it runs on energy from an Internet Game, and it is symmetrical in delivering words to explain pictures and pictures to explain words. 
The basic resource is a thesaurus which relates words and pictures. 
What words relate to what pictures and vice versa. 
Data mining done with an internet computing [think SETI@Home] compute resource creates this, with a small elite upper crust of picture/word ligatures that have been reviewed and endorsed by a) volunteer and/or b) expert analysts. 
The ultra-clever step is that the volunteers are playing an Internet hosted version of Pictionary, and people contribute their computer time to the Internet computing pool in order to play the game. 
And then we launch the game on the market with a celebrity charity game that people can watch on TV on the Web. 
But I get ahead of myself. 
The Google part is how you refine this raw relation into "what pages or neighborhoods in the words on the pages are on the basis of what others have done about them likeliest to be helpful in understanding this picture" and "what pictures, in terms of what others have done about them (in ways observable in the web content and clickstream experience) most likely to be helpful in understanding this word." 
The Atomica part is what you get as "whazzat" explanation of a word for ALT-clicking on it. 
It's that simple. 
But you have a preference set that Atomica understands that says "please explain in pictures, if you can." 
The document you get back is a Google-like prioritized top of the hit set in the heap of word-picture associations. 
For those of you who have experienced Sesame Street, there is an episode form that they use "one of these things is not like the others." 
In that schtick, there are multiple graphic panels that all but one contain a common element or theme, which is broken in one instance. 
The challenge is for the user to formulate a metapattern hypothesis in which they determine what the pattern is that is present in all but one. 
This is harder than the Pictionary display that comes from "Picture it for me Atomica" the way I can imagine it. 
The principle behind the "Picture it for me Atomica" form of display is "none of these things is not like the others." 
The sense of the word explained is _the_ common theme in _all_ the images protrayed. 
The Google/Atomica logic could be enriched by attempting to ensure that the word being explained is the _only_ common theme of the four to eight images shown. 
This is algorithmically determinable from the strength of the picture-word associations if we keep but one real-number-valued [in (0 .. 1)] strength weight per arc. 
There is is. it takes mobilization of an organized team to pull it off and maintain the hub resources. 
Who wants to make it happen? 
Al 
To keep Al sweet i've added a link to atomica from our query site: thanks Al, did you try looking up car? 
Someone someplace has a serious problem... elsewise it seems great, i'd love a picture dictionary, not too childish, if anyone finds one... jonathan chetwynd IT teacher (LDD) j.chetwynd@btinternet.com on the web" 
Charles, Danbri, Libby, Marja, and the "Dublin Core community" among others. 
In discussing SVG as a leap forward for accessibility, a major part of it is the ability to pull apart and put together known pieces of imagery thought of as less useful, like .gif and .jpg) 
In looking at the benefits of the semantic web, we see the other piece of the puzzle - the ability to record these linking statements The annotea project allows us to make statements about anything on the Web, including RDF explicitly relating an image to a word (for example derived from wordnet, a dictionary that itself makes formalrelationships between words, such as "the noun 'eagle' refers to a subclass of 'raptor' - and that is self is a type of 'bird'"). 
Dublin Core is a small vocabulary of statements about things, and a vast amount of information is collected in dublin core statements. 
One of the many tools for doing this is photo-rdf http://www.w3.org/TR/photo-rdf which adds information directly into jpeg files (which is the typical format for photographs). 
Dan and Libby have been working on a tool for recording "codepictions" - pictures of two or more people ( http://www.w3.org/2001/08/rdfweb/svg-foaf ). 
I added to that a technique for marking which bit of a whole image is a picture of which person - easy really, because it simply involved using the ideas written about SVG and semantic web coming together. 
These are a couple of home-grown solutions - rather like http in that sense. 
If nobody uses them, they die, if people pick them up they have potential for providing masses of information. 
There are of course other such systems, and the trick is to link them. 
==== technical philosophy discussion: Why RDF in particular and the semantic web in general? 
Well, RDF in particular is a format for recording semantic web information where there are lots of available tools that can be re-used. 
It is like the choice of HTML for web pages - it has a lot of working tools, and some good features. 
It probably isn't perfect yet, any more than the alternatives, but it seems likely that it has critical mass to become a standard (as in "the thing that everyone uses", not just "the thing we said people should use"). 
The semantic web is a way of collecting up the various bits of information without having to have a single massive server, which would require a lot of resources. 
It has the downside that collecting up information that is initially very scattered is slow. 
But as the connections are made, the rate of information available grows extremely rapidly - connect two services together of the same sizes and you double what is there. 
And so on... (well, this is how the Web grew to a fairly large extent). 
OK, let me give this away. 
This is worth an NSF grant, in my opinion. 
I had been dog in the mager hoarding it on that basis. 
[snip] The basic resource is a thesaurus which relates words and pictures. 
What [snip] There is is. it takes mobilization of an organized team to pull it off and maintain the hub resources. 
Who wants to make it happen? 
