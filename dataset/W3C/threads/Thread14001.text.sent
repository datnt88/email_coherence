We have a technique in the latest version (which will be coming out this afternoon) that deal with two types of foreign language markup. 
One of them has to do with providing markup to text that is buried in the body of a larger work in another language. 
For example, a document that is written all in English with a French phrase in the middle of it. 
The other deals with marking the document as a whole as to its language. 
That is, putting lang = en at the top of all documents on a website that are in English. 
There are two uses for these tags. 
The first is the fact that Braille translators need to know what language the document is in in order to properly translate the document into Braille. 
The second has to do with the emerging ability to automatically translate documents if the language in which they are written is known. 
Only the first of these is actually an accessibility issue as relates to disability. 
The second one will also be very powerful, but is probably not something we can deal with directly in disability access guidelines. 
It is a usability or accessibility problem faced by all. 
Please give both of these guidelines a read and provide comments back to us on them. 
In addition, we are looking for input with regard to priority. 
It is felt that this type of markup is important, and the debate we had with ourselves was between whether it would be a priority one or a priority two. 
In the current guidelines, they are listed as priority two for the following reasons. 
Priority one is reserved for those things that if they are not done, the user cannot access the information on the page even with extra effort. 
Priority two is designed for situations where it is hard for the user to access a page if it is not done. 
Lets look at the Whole page in and unexpected language. 
An individual trying to access a page would find that the content that was presented to them in either speech or Braille was incomprehensible to them. 
This is somewhat similar to the experience that anyone has when they go to a webpage that is written in a foreign language - except it is harder to "look" at a page in braille and figure out the other language. 
Lets assume that this person is not a liguist and knows only two or maybe three languages. 
If they hit a page that comes out in Braille in a form they cannot make sense of it, they could try one or two languages that they were familiar with and see if the page was comprehensible in those languages. 
If not then, like all of us, they would probably give up on the page. 
Thus if it were in a language they could understand, it would be accessible but take extra effort to find out. 
Next lets look at the Embedded foreign text problem. 
In this situation, a similar problem occurs for both sighted individuals and individuals using Braille or speech. 
The user suddenly encounters some text that they do not understand. 
If using speech (or Braille) the information could be requested on a character by character basis, and the person might be able to figure out what the phrase was. 
(presuming that if it did use special characters that the person was able to deal with, their synthesizer or braille program would be able to recognize the UNICODE and correctly identify them) The worse case is that the user, just like every individual without a disability who is also not a linguist, will not know what the foreign quotation means unless it is translated someplace in the text. 
In either case, it did not appear as if the individual with a disability was at anymore of a significant disadvantage in accessing the information than the majority of the people who are not multi-lingual. 
Since it does provide severe usability problem, however, it was rated as a priority two. 
We also considered a priority one except that there have been increasing complaints from people about sites that fail to make accessibility ratings but which were completely usable by everyone who went to try them. 
Declaring a site of 16,000 pages as being inaccessible when it is completely accessible except for the fact the pages do not have lang = en at the top of them, seemed to fall in this category. 
Also, we have kept in mind that anytime we add another item to the list of priority ones we weaken all of the other priority ones. 
This was a tough call for us since we do see it as a serious usability problem and that is the reason for this long e-mail describing some of the thought process. 
Your thoughts either concurring or differing are invited. 
Gregg For the Editors Gregg C Vanderheiden Ph.D. Professor - Human Factors Dept of Ind. 
Engr. - U of Wis. 
Director - Trace R &amp; D Center Gv@trace.wisc.edu, 
http://trace.wisc.edu/ 
For a list of our listserves send "lists" to listproc@trace.wisc.edu 
Another scenario involves user agents that support multilingual speech synthesizers: the text to speech conversion process is highly language dependent. 
I'm not sure if I agree or not. 
Maybe the problem should be re-expressed into marking up the Character set. 
I can read a bit of Japanese and Greek, but if the character set is not marked, and the language is not marked, then I simply have to guess at all the character sets I can think of. 
Then if I make a lucky guess, I have to work out a font. 
This is from the perspective of a sighted user. 
I can recognise whether I am dealing with Chinese, Japanese, Korean, or Thai at a glance. 
(Although I may get Thai confused with Khmer, or other languages close by). 
With a hint from the origin of the document, it gets a little easier. 
But for me it seems that to compund that problem with one of lack of Language markup and ask a braille translator or speech synthesis system to make any sense at all is ridiculous. 
If it would help anyone, I will make up some pages in a few different languages (chinese, Japanese, Korean, Vietnamese, French, German, Russian) and post them with no markup so people can try them out. 
Charles McCathieNevile Two points can be made here: 1. 
With Unicode as the official HTML character set, and the possibility that any character within the entire repertoire may potentially appear within the document, the need for explicit language markup becomes even more acute. 
If it is not provided, maybe the braille/speech software could identify the characters being used and help the user to work out which language is in effect. 
Unicode is undoubtedly problematic for the developers of braille software, due to the number of characters available and the need for language-specific rules/algorithms to handle each language. 
2. I recall having read that there is an HTTP header which allows the language, not just the character set, of the document to be conveyed to the user agent. 
If this is so, it could be implemented throughout a site or a defined portion thereof, and would avoid the need for a LANG attribute at the start of each document. 
However, LANG would still be needed in multilingual texts whenever the language changes. 
Exactly. 
And Unicode 3.0 is even trying to include language tags by itself in Plane 14. 
Yes, there is a "Content-Language" HTTP response header. 
For example, http://www.w3.org/Press/1998/DOM-REC is provided in 6 languages, namely, Dutch, English, French, German, Japanese and Swedish. 
We're adding appropriate Content-Language header in addition to charset parameter of Contnt-Type header. 
Regards, Masayasu Ishikawa / mimasa@w3.org 
W3C - World Wide Web Consortium HTML only has one document character set: Unicode. 
Excuse me, but this seems to be confusing HTML with some proprietary word-processing format. 
Well, alright, there are some misguided pseudo-HTML documents that are made that way (I'm thinking most particularly of documents that go FONT FACE="Symbol" and then expect their Roman letters to be displayed as Greek, but the same has been seen with other alphabets). 
But these are not well-formed WWW documents, surely the WAI does not have to devise ways of displaying them? 
HTML documents use quite a number of different encodings (designated by that confusingly-named "charset" parameter on the content-type header), but every properly-transmitted document has this "charset" explicitly stated (except for pre-HTML4.0 
documents in iso-8859-1, where the charset attribute is optional). 
Now, I have to admit I am entirely unfamiliar with how a screen reader would deal with this, but I firmly feel that whatever it does, it has to be based on a proper recognition of the interworking protocols. 
Technically, the language of the content and its character encoding are two unrelated issues. 
(Even if that seems unrealistic and impractical, I'd say that trying to take any other view leads to far too many anomalies). 
And knowing that a document is in iso-8859-1 does not help to know how to pronounce the document if one does not know whether it is Icelandic, Gaelic, Portugese... Nor would it be more than an unpleasant kludge to take a stab at the language based on the DNS name. 
Anyway, a solution for a site which has been constructed without explicit content language specifications would seem straightforward: simply arrange for the server to send out an HTTP content-language header. 
It needs no editing of the web pages themselves (if documents are available in various languages, some action may be needed to identify them, e.g by appropriate choice of filename - see Apache's Multiviews for ideas). 
The meaning of the HTTP content-language header is subtly different from language specifications within an HTML document, it's true, but I'd argue that either solution would be serviceable, for individual documents that are in a single language. 
I suppose I'll get told that the existing client agents don't do anything with this HTTP header, so it doesn't help them with their rendering. 
That would be unfortunate, as this is a bona fide part of the protocol. 
best regards Comments interspersed - look for CMcCN:: or AJF:: CMcCN:: Well, maybe. 
But most documents have a variety of character sets - ISO-8859-1, or Shift-JIS, or Windows-1252, or ISO-8859-5, or EUC-2022-KR or whatever. 
Having these marked up would be helpful, but is not necessarily sufficient to guess the language. 
AJF:: CMcCN:: (what I said above...) AJF:: CMcCN:: Agreed. 
But it can be a hint, ad is important information. 
More below... AJF:: CMcCN:: No, that's not my big complaint here (although it is in the User Agent group). 
My big complaint is that most authors do not have the ability to set up how their server deals with language negotiation, but they do have the ability through a combination of META HTTP-EQUIV elements (I have written on this topic here a couple of months ago) and LANG="xx" statements, to make it explicit in their pages. 
It is important that it be explicit, and either we try changing the way ISPs work (which seems unlikely and not the most efficient place to deal with it anyway - authors know better what language they write) or we change the way authors write, by asking them to mark up their language explicitly. 
Cheers Charles McCathieNevile I'm sorry, the point I was trying to make is that these are encodings, in the language of HTML4.0. 
I'm sorry if this appears to be unreasonably pedantic, but there seems to me to be much confusion about this area of i18n, and I think it's worthwhile to strive for clarity when discussing it. 
5.2 in the HTML4.0 spec has some useful remarks: http://www.w3.org/TR/REC-html40/charset.html#h-5.2 
Please excuse me if this is thought excessive, but I think it may be useful to quote a paragraph from 5.2.1, as follows --quote begins-- Authoring tools (e.g., text editors) may encode HTML documents in the character encoding of their choice, and the choice largely depends on the conventions used by the system software. 
These tools may employ any convenient encoding that covers most of the characters contained in the document, provided the encoding is correctly labeled. 
Occasional characters that fall outside this encoding may still be represented by character references. 
These always refer to the document character set, not the character encoding. 
--quote ends-- In simple cases it may be that the document doesn't utilise any characters that are outside of the repertoire of the encoding ("charset") that it uses: but it's perfectly valid for the document to contain some &amp;entity; or &amp;#bignumber; representations that lie outside of the repertoire that's defined by the document's encoding. 
To take a simple example, a document that's in a Cyrillic encoding, let's say koi8-r, can still validly include French or German employing &amp;eacute; &amp;uuml; and so forth, while a document that's in iso-8859-1 can validly contain &amp;#bignumber; references that represent Cyrillic characters. 
Which of the two representations to choose for, say, a bi-lingual document would be dictated by practical convenience: either is a valid document according to RFC2070 or HTML4.0. 
Provided that the reader is using a client agent that supports RFC2070 to this extent, the document will be displayed correctly. 
I'm sorry for not making my reasoning clear. 
I was referring implicitly to an argument elsewhere on this thread, that were some large site had already been created without language attributes in its markup, then it might be impractical to correct that. 
Well, processing a whole collection of files to do nothing more than change HTML into HTML LANG="value" for some fixed "value" is hardly rocket science, but I was suggesting an alternative solution that could be applied without editing the files, if that were preferred. 
I find this very sad: the HTTP protocol has many valuable features, it's a tragedy that it's being crippled in this way. 
And the most popular server, Apache, has no difficulty putting these matters into the hands of the document owners via their .htaccess 
files. 
But you could well be right that it's impractical to expect this part of the WWW to work as designed. 
Yes, that much is true enough, I have no dispute with that. 
I'm sorry, I'm rather conscious that this has addressed issues that are relevant to i18n in general, and not particularly specific to accessibility. 
However, they are issues that can have much more critical consequences in an accessibility context, so I thought it was worth trying to clarify the issues. 
all the best I think Alan has it, but for PF reasons I want to try the following recapitulation on y'all. 
To meet the needs of universal access, text needs to be fit for use with Braille and Text-to-Speech transliteration technologies. 
These technologies require that the character representation and language identification be rock-solid in order for them to function successfully. 
Therefore Universally Accessible documents must strictly adhere to the internationalization discipline as set out in HTML4.0 and identify the language of each document and sub-segments of the text which are not in the base language of the document. 
The actual impact of failing to follow this practice of course depends on the language mix involved and the likelihood that a given reader will assume or guess correctly the language and encoding of the document. 
Al Comments interspersed. 
Look for CMcCN:: or AJF:: Basically I am agreeing with things. 
and CMcCN:: Yes. 
It seems sad, but my experience is that it's true and becoming more so. 
I tend to use META HTTP-EQUIV pretty often. 
In an ideal world this wouldn't happen. 
But when I'm writing pages for people to give to someone else to put on their ISPs (unknown flavour of) server it's better to be safer. 
and I had written AJF:: CMcCN:: I agree that they are important to accessibility. 
I think the reasons have been explained pretty well recently. 
So it's nice to see the topic being taken up. 
cheers Charles McCathieNevile Since a significant proportion of authors/site administrators will be able to implement the HTTP response header, it should be recommended as a technique in the techniques document, along with the LANG attribute. 
