I suspect it doesn't show up that often because not that many people 
I suspect the reasons include: - most ? 
references are still implied through forms; - many sites have their CGI areas blocked in robots.txt; - most sites using non-trivial ? 
references in href attributes have broken URLs because they don't escape &amp; properly or use the alternative of ; suggested in an HTML specification appendix; - there may be a tendency to use meta to inhibit caching on such pages. 
I consider this particular, Cold Fusion, technique as an abuse of URLs which, by confusing the mechanics of creating the HTML with the naming of the resource, causes misoperation of things like proxies (Squid's default rules are to send ? 
URLs direct to the origin, rather than to an upstream cache, as it expects not to get a cachable resource back). 
I suspect it doesn't show up that often because not that many people 
I consider that proxy behaviour, if not an abuse of HTTP then a failure to implement it as well as is possible. 
Admittedly it's probably a failure based on adapting to practical experience with sites which in turn fail to implement 
HTTP as well as is possible. 
A URI containing a query string is of equal status to any other URI, though it may be weaker in terms of human-readable qualities. 
A GET to such a URI is just as capable of returning a cachable resource and developers should strive to assist this caching (setting Last-Modified, reacting appropriately to If- Modified-Since). 
I've used query-strings on numerous occasions (sometimes, in the case of searches, this was the most sensible way to go; sometimes it was due to the relative difficulty in generating data-driven sites any other way with certain tools). 
While I do avoid the use of query-strings I have not found their use to get in the way of caching. 
In particular there are some cases where the data-driven nature, combined with a knowledge of the mechanics producing that data, offers a reliable way of determining expiry dates, with a tremendous gain to caching efficiency. 
As for search engines, the reason that URIs with query strings are less likely to get indexed is that search engine people don't want their spiders to spend eternity indexing a site that is produced on the fly for which there may be an infinite number of URIs to be found in the generated pages. 
As an example of such a page I once wrote a joke version of RSS as a satire of the version numbers used by rival versions, each page would claim to have been obsoleted by another version which it linked to, with version 10234.0 pointing to version 10235.0 and so on. 
This would go on until version 2147483647.0 after which it would trigger an overflow error I couldn't be bothered checking for on what was after all a joke. 
If google had started indexing that page it would still be there :) Of course this can also happen with URIs that don't contain query strings, but such cases are rarer. 
Google will generally list a page if it is linked to from a page which doesn't contain a query string in the URI (I'd guess that includes HTTP redirects from such a page, but I'm not sure), having few parameters helps as well. 
Jon Hanna *Thought provoking quote goes here* 
It is not an abuse of HTTP 1.1 as it is based on a requirement of HTTP 1.1 (I'm not sure at what conformance level). 
It is in HTTP 1.1 because GET mode forms were abused for applications that made significant updates to the server state and should have been done with POST (the problem now tends to be that forms with pure function semantics are tend to be done as POST). 
I think the HTTP specification says that there must be explicit cache control information, not just a correlator (e.g. Last-Modified-Date) before a proxy can assume it is safe to cache a URL containing ? 
More precisely, bypassing upstreams proxies is an optimisation based on the likelihood that a non-cachable resource will be returned. 
As caches are never required to cache any particular page, it can't be a violation. 
HTTP 1.1 introduced workarounds for mis-use of GET forms for unsafe operations. 
It's a MUST NOT in RFC 2616 as far as caching ? 
URIs without explicit expiry information: We note one exception to this rule: since some applications have traditionally used GETs and HEADs with query URLs (those containing a "?" in the rel_path part) to perform operations with significant side effects, caches MUST NOT treat responses to such URIs as fresh unless the server provides an explicit expiration time. 
This specifically means that responses from HTTP/1.0 servers for such URIs SHOULD NOT be taken from a cache. 
See section [41]9.1.1 for related information. 
I was two steps ahead of myself and assuming that everyone explicitly sets as much cache information as they can in their applications! 
A stupid assumption on my part. 
Jon Hanna *Thought provoking quote goes here* 
