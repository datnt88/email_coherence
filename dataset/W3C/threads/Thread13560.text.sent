Secondly, an observation: most HTML documents are seriously broken. 
Trying to create a robust mapping from broken HTML to XML is a minefield we do not wish to step on. 
We are well aware of that, but it affects the practice, not the principle, of XPointers into HTML. 
Therefore the answer to the question "what should an XPointer into HTML look like?" is a very loud "it depends". 
Indeed. 
It depends on defining a canonical normalisation of HTML. 
If we can do that, we're fine. 
This has come up in ER and in Annotea. 
I wasn't involved in Annotea at the time of its introduction, so I won't comment on that now. 
The subject arose in ER because we wanted an xpointer-like mechanism available in Valet reports for client software - specifically Jim's stuff. 
Jim and I called them "Fuzzy Xpointers". 
We haven't properly formalised them, but rather we have an empirical working model: * Construct an XML normalisation of the HTML * Use a (simplified) Xpointer into that We called them "Fuzzy Xpointers". 
The mechanism works provided our respective parsers make compatible HTML- XML normalisations. 
After a couple of iterations, we were able to make it work with OpenSP, MSXML and Mozilla - the tools we were using in the applications in question. 
This looks like a reasonable starting point to define a canonical normalisation. 
Nick Kew Site Valet - the mark of Quality on the Web. 
"Nick Kew" nick@webthing.com 
Construct a normalisation of the HTML, no XML needed, just an agreed structure based on the SGML. 
no XML needed. 
I've tried to drop the X :-) Jim. 
From: "Nick Kew" nick@webthing.com 
Therefore the answer to the question "what should an XPointer into HTML look like?" is a very loud "it depends". 
And what I said is: that is a minefield onto which we [the HTML working group] do not want to step. 
Real-world HTML documents are jokingly called "tag soup" for a reason. 
You take a goodly collection of HTML tags, stir them up, put them into a file, and publish it on the web. 
style elements before the html tag; titles outside the head ; misspelled closing tags, misspelled opening tags, ul s with no enclosed li s; li s outside ul s. Imagine a combination of tags, you will find a document that contains that combination. 
Even Tidy throws up its hands sometimes, and instructs you to go back and change the source file! 
Finding a canonical normalisation of real-world HTML documents is not something the HTML WG feels inclined to spend its scarce time on. 
Best wishes, Steven Pemberton So one approach the RE group could take is to define a document namespace which is in fact defined as the Tidied version of something, where there is a reulst defined for when Tidy just gives up. 
A variation is to annotate a given docuemnt with an annotation type of "valid XML representation so we know what the xpointers refer to" or something, and make Xpointers refer to that (and define it, also, as the result of applying Tidy or something, so the actual thing can be autogenerated). 
Anyone want to make a server that does this? 
chaals From: "Nick Kew" nick@webthing.com 
Therefore the answer to the question "what should an XPointer into HTML look like?" is a very loud "it depends". 
And what I said is: that is a minefield onto which we [the HTML working group] do not want to step. 
Real-world HTML documents are jokingly called "tag soup" for a reason. 
You take a goodly collection of HTML tags, stir them up, put them into a file, and publish it on the web. 
style elements before the html tag; titles outside the head ; misspelled closing tags, misspelled opening tags, ul s with no enclosed li s; li s outside ul s. Imagine a combination of tags, you will find a document that contains that combination. 
Even Tidy throws up its hands sometimes, and instructs you to go back and change the source file! 
Finding a canonical normalisation of real-world HTML documents is not something the HTML WG feels inclined to spend its scarce time on. 
Best wishes, Steven Pemberton Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) Aaron Swartz runs an HTML-tidy service:- (Hmm... it seems to be down). 
I think it can be made to give source only. 
Kindest Regards, Sean B. Palmer @prefix : http://purl.org/net/swn# . 
:Sean :homepage http://purl.org/net/sbp/ . 
"Charles McCathieNevile" charles@w3.org namespace there is a Making something dependant on tidy is not an option AFAICT, and isn't necessary in any case. 
What we actually need is to stop thinking of the context as a URI-fragment. 
We can use a standalone property that we can define the semantics of without all these fragment references getting in the way. 
XPointer isn't even sufficient for application/xhtml+xml stuff as they're not robust over changes in the document - so we still need all the contextual stuff to identify a document. 
Jim. 
Oops, forgot to update the DNS. 
Fixed as soon as your DNS cache expires. 
It can. 
"Aaron Swartz" | Aaron: The Weblog http://www.aaronsw.com/ 
| what's on my mind is now online Yes, that's essentially what Jim and I are doing. 
Yes. 
A server doing exactly that is in the very-near-future plans, as a demo application of mod_xml. 
It'll be somewhat akin to tidy, but (unlike tidy) it is DTD-aware. 
Nick Kew Site Valet - the mark of Quality on the Web. 
Cool. 
I take Jim's point that this is not exactly a quick way to resolve the problem - but the basic problem is that people are using old systems when there are new ones available which work for almost all the applications, and work better. 
Getting people to upgrade is really a slow solution, so pointing out why some things run extra slow when people are waiting might help a bit. 
"Please be patient while we are checking the server to deal with your HTML code. 
Did you know that you could do the same things with XHTML and avoid making your clients wait for this service? 
Have a look at cheers Chaals Yes, that's essentially what Jim and I are doing. 
Yes. 
A server doing exactly that is in the very-near-future plans, as a demo application of mod_xml. 
It'll be somewhat akin to tidy, but (unlike tidy) it is DTD-aware. 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) "Charles McCathieNevile" charles@w3.org resolve the when applications, and pointing bit. 
There are systems which actually understand XHTML? 
I assume you're talking about the broken support in Mozilla, a beta browser which is not accessible to me, and my needs aren't particularly demanding (and doesn't support the SVG plugin...) HTML avoid That would be a simple lie, There are very few people on the web using clients that understand XHTML, and there are none which even claim to that are available for 2 of my platforms. 
Jim. 
Well, it depends what you want to do with the XHTML. 
Many people want stuff that renders in a browser, and XHTML written according to the compatibility guidelines can render in Lynx, Amaya, Mozilla beta (supports SVG plugin on Mac OS, although when it gets out of beta it is expected to support it natively), IE on various platforms, iCab, etc. (If you means correctly parses it according to spec that's different. 
But as far as I know there is almost nothing that does so with HTML either - one SGML parser and CSS solution is claimed to have done so.) The problem is that there isn't any very well defined way to point to arbitrary parts of HTML, and particularly for invalid HTML. 
Another alternative would be to use ed/vi syntax to get references into the source code, which can include doing searches to find things, if anyone can be bothered building a powerful enough engine for making references that are stable (searching for elements based on id attribute where available, etc... 
I wasn't intending to provide the one true answer. 
The point is that a specified answer that can be reproduced by other developers is what is really required, and one that can be readily converted to others, especially to those designed for XML (assuming people believe that XML will represent more content in the future), would be better. 
cheers Chaals "Charles McCathieNevile" charles@w3.org resolve the when applications, and pointing bit. 
There are systems which actually understand XHTML? 
I assume you're talking about the broken support in Mozilla, a beta browser which is not accessible to me, and my needs aren't particularly demanding (and doesn't support the SVG plugin...) HTML avoid That would be a simple lie, There are very few people on the web using clients that understand XHTML, and there are none which even claim to that are available for 2 of my platforms. 
Jim. 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) "Charles McCathieNevile" charles@w3.org stuff compatibility guidelines can render in Lynx, Amaya, Mozilla beta IE on various platforms, iCab, etc. but not IE on all platforms, and not the myriad of other browsers, including ones in embedded systems which aren't (easily) upgradeable. 
Suggesting XHTML 1.0 with some compatibility guidlines will make your content render appropriately in more browsers is clear bunkum HTML 4.01 has all the accessibility features of XHTML 1.0 and has the bonus of being supported in a lot more browsers. 
If XHTML is appropriate for future browsers, then the simple matter of content-negotiation on the XHTML mime-type is appropriate. 
I fear you've not been keeping up with Bugzilla... which says it applies to ALL OS's. 
I also understand there is no chance of native SVG making it into Mozilla 1.0, which has already been pretty much fixed (a Release Candidate is due on the 15th). 
The SGML normalisation is every bit as appropriate as an xhtml one, it's also simpler to implement as it's what the browsers and validators are already doing... Jim. 
"Charles McCathieNevile" charles@w3.org stuff compatibility guidelines can render in Lynx, Amaya, Mozilla beta IE on various platforms, iCab, etc. but not IE on all platforms, and not the myriad of other browsers, including ones in embedded systems which aren't (easily) upgradeable. 
Suggesting XHTML 1.0 with some compatibility guidlines will make your content render appropriately in more browsers is clear bunkum. 
At the moment we are talking in hand-waving generalities - I guess we would do well to get more clear facts. 
(I am not saying that what you say is untrue, but I am intersted in looking in some detail at the information. 
HTML 4.01 has all the accessibility features of XHTML 1.0 and has the bonus of being supported in a lot more browsers. 
If XHTML is appropriate for future browsers, then the simple matter of content-negotiation on the XHTML mime-type is appropriate. 
Yes, I think this is the key point. 
With a server like Jigsaw I think it is now possible to do something smart by validating content, and being able to serve one file as two different content-types. 
(XHTML should really be sent as xhtml+xml, but even browsers that do grok XHTML don't always know what content-type to expect.) I fear you've not been keeping up with Bugzilla... yep, true. 
which says it applies to ALL OS's. 
I also understand there is no chance of native SVG making it into Mozilla 1.0, which has already been pretty much fixed (a Release Candidate is due on the 15th). 
The SGML normalisation is every bit as appropriate as an xhtml one, it's also simpler to implement as it's what the browsers and validators are already doing... well, some browsers are. 
Some browsers are doing XML-based parsing, and don't handle SGML - as I understand it this is the minority case which is growing. 
And some browsers just have special magic processing for HTML... cheers Charles Metrics for Markup Change Detection Ideas for working towards an RDF Formalism When dealing with metadata about Web resources, we need to be able to identify unambiguously the subject of our metadata. 
THE PROBLEM Neither URI nor XPointer is sufficient for this, for several reasons. 
Consider an assertion: Nick asserts that the third word in the second paragraph of This kind of assertion is the starting point for metadata schemas such as EARL and Annotea. 
However, it has some obvious shortcomings: 1. 
It needs to be dated. 
In the absence of a date, the error may have been corrected, invalidating the assertion. 
Or it may still be there, but at a different point on the page due to changes unrelated to the error. 
2. If it is a content-negotiated page, then the assertion applies only to one instance of it: a misspelt English version doesn't affect the French, German, Russian or Arabic versions. 
We can deal with these using a more complex assertion: Nick asserts that on April 11th 2002 at 20:05 GMT, the third word in the second paragraph of the document at URL:... as represented in the English language version is incorrect. 
For completeness, we need to be more explicit about content negotation A second assertion is needed to complete the unambiguous identification: Valet asserts that on April 11th 2002 at 20:05 GMT, the server at negotiation other than language. 
This is getting rather messy. 
Furthermore, it is inflexible: If a page has no last-modified date, or a date later than Nick's assertion, we have absolutely no information on whether the assertion is still valid. 
On a more technical note, we identify a third problem. 
3. Our original assertion presupposes that "the third word in the second paragraph" is a well-defined concept. 
If we are dealing with well-formed XML then XPointer gives us this, but most pages on the Web today are neither XML nor well-formed. 
THE GOALS In order to overcome these problems, we set ourselves three goals, and seek a holistic approach to meeting them. 
(1) A generalised Pointer, applicable to HTML and tag-soup (2) A sensitive measure of content change, and whether change is "significant" (3) A means for recording dependency on Content Negotiation. 
The third is trivial housekeeping. 
The first and second are more interesting, and are the subject of this note A GENERALISED MARKUP POINTER Background: 1. 
In the WAI/ER working group, we needed a robust pointer for referencing an element in a document. 
Using an experimental approach, Jim Ley and I devised such a pointer, which currently enables his Client S/W to use pointers generated by Page Valet, even on tag-soup markup. 
2. The Annotea project uses what it describes as an XPointer, and which is in fact an XPointer into a normalisation of non-XML markup. 
In practice this is very similar to (1). 
3. The HTML WG has declined to consider defining a "canonical normalisation": SP = Steven Pemberton NK = Nick Kew SP Therefore the answer to the question "what should an XPointer into SP HTML look like?" is a very loud "it depends". 
NK Indeed. 
It depends on defining a canonical normalisation of HTML. 
NK If we can do that, we're fine. 
SP And what I said is: that is a minefield onto which we [the HTML working SP group] do not want to step. 
Now, we can actually step around that minefield. 
Instead of discussing a Canonical Normalisation, we postulate an Abstract Normalisation, of which our current implementations are instantiations. 
So the problem is now split into two parts: * To define an Abstract Normalisation * To define the metadata that will describe an instantiation Having done so, we can go ahead and use pointers into a normalised representation of any markup. 
The associated metadata will enable another agent to reconstruct the normalisation unambiguously. 
Abstract Normalisation I would suggest two candidates for this, both subject to the condition that the normalisation preserve all the original elements and their order, but not necessarily their structural relationships. 
Either (1) Normalisation to well-formed XML, or (2) Any normalisation having the property that every element can be referenced unambiguously by a path from a root element. 
(1) is probably the more generally useful, as it enables us to apply all the usual XML technologies. 
Although the weaker (2) is sufficient for the problem we have set ourselves here, I will assume normalisation to XML for the following discussion. 
Instantiation The fundamental problem is that for a given non-XML document - from valid HTML to tag-soup - a normalisation may not be unique. 
For example: * Implied elements in SGML. 
Does get normalised to (a) table tr td FOO /td /tr /table (b) table tbody tr td FOO /td /tr /tbody /table (b) is a simple normalisation, but a parser working from an HTML DTD might also normalise to (c). 
* Treatment of empty elements. 
Normalising do we get (a) p this br/ that hr/ the other /p (b) p this br that hr the other /hr /br /p or indeed other variants? 
Page Valet with an HTML DTD will normalise to (a), but in the absence of any DTD will give us (b). 
This is significant, because it materially affects the XPath to the hr element and the text around it. 
We can disambiguate this by describing our normalisation. 
For example, we can describe our normalisation as "the XML generated by tool X using settings Y", or we can define a precise spec. 
In practical terms, we should use a URL to describe a normalisation scheme. 
This gives us a third option: instead of *describing* a normalisation, the URL can be a webservice that *implements* the normalisation. 
Or in RDF we can offer two URLs: one to the scheme described in Chapter 4.5 of the mod_xml book; the other to a webservice implementing it. 
I'm not going to try and formulate this as RDF, or I'll not finish this evening, and lose momentum. 
But I hope the above at least conveys the basic idea. 
MEASURING CONTENT CHANGE In the absence of date information (including valid Last-Modified headers) to tell us when a resource has changed, we need to look at document contents to detect changes. 
The simplest measure is a checksum. 
However, we can do better than that. 
A checksum tells us nothing about the magnitude of a change, so that for example a document containing "todays" date might be updated daily without affecting the validity of metadata assertions. 
Since markup implies structure, we can improve on a simple checksum by computing hashes not on the document itself, but on a suitable representation of it. 
We can then refine our measure by considering only certain structural elements of interest, so that a mere date change is ignored, or (conversely) detected as distinct from a structural change - if we are looking for a spelling mistake to be fixed. 
A first experiment in this is described at and implemented at with source code at As you can see from the code, it works by filtering on an ESIS representation generated by onsgmls. 
This was found to be successful at tracking change at different levels of significance, and successfully detected structural similarity over changes in rapidly-changing news sites such as CNN. 
A variant on it (with an additional hash for all Form elements) is in use in Site Valet's Problem Reporting and Tracking database. 
In terms of determining the validity of an XPointer following a change to a document, we have proposed hashing on the document elements: and concluded that this can be implemented even in the restricted environment of a browser with scripting: Note that for this to be unambiguous presupposes normalisation to XML as described above. 
We can use any of the techniques described above. 
To formalise a measure of change detection, we must take the normalisation, and use additional metadata to describe our hashes. 
As before, we can use a URL to describe it and/or a webservice implementing it. 
Alternatively we can describe it directly in metadata: something like (sorry, I'm tired and I really want to get this don tonight; I'm going to stop now and post unfinished for discussion). 
Nick Kew Site Valet - the mark of Quality on the Web. 
