I therefore suggest that the section on %-escaping be the first section of RFC2396-Sensitive Comparison, and that it nails down that %-equivalence (e.g. ~ == %7E == %7e) is (as currently observed) and be (to be specified by specifications) the minimal (baseline) equivalence to be respected by resolution-related operations (as opposed to namespace-like equivalences). 
(with the exception of reserved characters) Except for, in EBCDIC, %61 is '/', a reserved character; so on an EBCDIC computer, what you see on the screen as /dir/a and /dir/%61 reference different URIs. 
Ah, now I see where your confusion is comming from. 
The characters in an URI (the ones that are compared character-by- character in namespaces) are just that, characters. 
URIs are defined independent of any particular representation. 
The URI spec says that /dir/a and /dir/%61 are equivalent, independent of the representation. 
They are equivalent if they appear in ASCII. 
They are equivalent if they appear on paper, on the side of a bus, and so on. 
They are equivalent when spoken over the radio. 
And they are equivalent when encoded as UTF-16 (as your Java example shows) or in EBCDIC. 
RFC 2396 gives three levels, condensed in the following line: URI character sequence- octet sequence- original character sequence In practice, there are two more layers, one on each side. 
We then get: a) substrate: paper, metal, audio waves, ascii, UTF-16, EBCDIC,... We don't want to limit that to a particular encoding. 
conversion depending on substrate representation V b) URI character sequence (just characters) conversion defined by RFC 2396 (always US-ASCII!) V c) octet sequence (just octets) conversion currently scheme/server dependent, moving towards UTF-8 V d) original character sequence (file names on server, query strings,...) conversion server-dependent V e) original octet sequence (e.g. UTF-16 for a filename on WinNT, EBCDIC on an EBCDIC server, and so on) Maybe this diagram should go into the new version of RFC 2396. 
And if you were writing EBCDIC software, you'd have to be careful. 
Yes, converting from 'a' to '%61' needs an ASCII table, which is implicit on an ASCII-based architecture. 
But apart from that, there is probably not much difference. 
Personally, if I were rewriting 2396 I would simply decree that all %-escaping be done on the UTF-8 mapping and only the UTF-8 mapping, I would be the first to agree with that. 
Unfortunately, it's not as easy as that, because there is some legacy out there. 
and be forbidden unless it's compulsory. 
But that's not what 2396 currently says. 
Exactly. 
I'd be glad to help with some actual text. 
Please. 
-Tim Will do. 
Can you give me few hours? 
Regards, Martin. 
No, 'a' and %61 are *not* equivalent in an EBCDIC environment. 
I just don't see where, in RFC2396, it says that the hex-encoding is necessarily that of the ASCII value of the character. 
I repeat: if I'm on an EBCDIC computer, and the URI reads out as /dir/a, that is *different* from /dir/%61. 
Yes, this is egregiously broken and stupid, but it's within the bounds set by RFC2396. 
Actually, the problem is that RFC2396 is just hopelessly unclear. 
The fact that you and I are unable to agree on what it says is incontrovertible proof of this fact. 
May I argue for a brief truce? 
As part of the revision process, I'm working on an essay whose subject is what RFC2396bis *should* say on the subject of data and characters and octets and %-escaping, so that we don't have to have these endless arguments. 
Frankly I would rather not waste any more time arguing about what the current revision of 2396 says. 
-Tim Martin Duerst replied to Tim Bray: Personally, if I were rewriting 2396 I would simply decree that all %-escaping be done on the UTF-8 mapping and only the UTF-8 mapping, I've spent many days thinking through this problem. 
My personal conclusion is that the only 'correct and complete' way out of this hole will have to meet the following requirements: 1) Any 'new' URI-RFC _has_ to mandate that: "All %-escaping be done on the UTF-8 mapping and only the UTF-8 mapping" 2) All 'New' schemes are distinguishable by some kind of syntactic tagging, 3) In URIs not containing that tagging the old 2396 rules (or lack thereof) continue to apply. 
Now, to meet these requirements we need a method of indicating that an instance of a URI conforms to the 'new' URI-RFC. 
I can think of three possibilities; Option 1 - URI tagging using a 2396-illegal character sequence In Option 1 URIs conforming to the 'new, common rules' are marked by placing a character sequence which is illegal in 2396 (such as %II) in a well-known place in the syntax. 
URI-receivers which are aware of the new RFC will detect the marker and interpret the URI accordingly. 
Those which do not understand it may break or corrupt the data in some way. 
Option 2 - A totally new URI syntax In this we use some syntactic formulation , such as http[URI/1.0]://host.... which cannot occur in 2396. 
The value in the brackets is the version number of the URI encoding scheme (making it possible to make future changes as well). 
Note that the [URI/n.n] 
would be part of the URI syntax, not something done on a scheme-by-scheme basis. 
Option 3 - Scheme-by-scheme changes Under this approach individual URI scheme users 'call up' the new rules on a scheme-by-scheme basis, e.g. for HTTP we introduce two new schemes: httpi: httpis: which are 'Internationalized' protocols and which mandate the UTF-8 encoding (and other internationalization features?). 
Other schemes ( ftpi: and so on ) would follow. 
All the individual new schemes need to do is invoke as 'mandatory' some rules defined in the new, common URI-RFC. 
In HTTP-land I think they would have to issue a new HTTP-RFC, defining a new version of HTTP (1.3?) which would accept all four scheme names Discussion The only advantage Option 1 has is that (in the RFC-world) it only needs a single RFC change to implement. 
Other than that I regard it as too ugly for words (but as it meets the requirements I included it). 
Option 2 is marginally more elegant, and is better 'engineering' in that we now have a protocol whose instances identify their versions. 
But, as this protocol is used directly by humans, it might be thought to be a little inconvenient (it's 9 additional, not very memorable characters to be remembered from the side of a bus). 
It would also be more of a 'shock' to legacy receivers which were not expecting it. 
My own preference is for Option 3 - new schemes. 
Rationale for chosing Option 3 The '%hh encoding is unspecified' disaster is so pervasive, and there are so many different, incompatible legacy adaptations and work-arounds that I think its inconceivable that we can find a way out without replacing in some way all the 'tributary' RFCs which call up URIs. 
My belief is that we have reached the limits of ingenious work-arounds and syntactic contortions - and have to face this Internationalization issue head-on across the whole of the IETF and W3C with a robust, properly-engineered solution. 
The 'communities' who make the different uses of URIs (browser developers, web server developers, FTP developers etc. ) are going to have to change their products to use the new URI, so having a new scheme name and new RFCs as part of that community change does not seem too onerous a task. 
The HTTP scheme names I have suggested (httpi and httpis ): a) Are reasonably human-friendly, b) Indicate their purpose (i for International) c) Maintain the visual significance of security (the terminating 's') Obviously, it is for the HTTP community (and the like) to decide on and register their new schemes; the URI-RFC community is above such detail. 
I only suggest names as part of this feasibility exploration. 
Implementation and Migration The most demanding test of the viability of this approach that I can think of (in the HTTP world) is to consider what happens if a 'new' browser sends a request to an 'old' web server. 
If the URI has characters which need to be encoded (or makes use of other features provided by the new URI-RFC) it first attempts a connection thus: GET httpi://www.host.com/user/D%FCrst 
HTTP/1.3 
Note that the HTTP spec is (the as-yet-fictitious) 1.3 - which, I assume, declares knowledge of httpi: and hence the new URI encoding. 
If the server understands HTTP/1.3, and hence httpi: and the new encoding it can cope and respond normally. 
If the server does not understand HTTP/1.3 it should reply with an 505 HTTP version not supported error. 
The browser must now decide what to do. 
There can be no single, correct behaviour if the requested URI contains characters which are unrepresentable in 2396. 
It might decide to try: GET http://www.host.com/user/D%FCrst 
HTTP/1.1 or it might tell the user it can't cope. 
Sundry thoughts and issues In HTTP practice , one could still use port 80 as the default for both Subsidiary scheme names like 'mailto' evolving to 'mailtoi' or 'imailto', would be dependent on an RFC821-replacement, and so on. 
Chris Haynes Retired Chartered Engineer Harvington, Evesham, UK A character (conceptually) never gets directly encoded into a %-escaping, you always have octets in the middle. 
I agree that it may not be extremely clear. 
But I disagree that your interpretation is within the bounds of RFC 2396. 
For example, in "2. 
URI Characters and Escape Sequences", we have: Within a URI, characters are either used as delimiters, or to represent strings of data (octets) within the delimited portions. 
Octets are either represented directly by a character (using the US- ASCII character for that octet [ASCII]) or by an escape encoding. 
This representation is elaborated below. 
Now let's take your example, "/dir/a". 
Let's assume that's a directory name 'dir' and a file name 'a' on a computer that uses EBCDIC. 
We don't have to care about the '/' here, because this is a separator that is part of the URI syntax, independent of local usage (see e.g. MSWin). 
So now let's look at how the ebcdic server exposes 'dir' and 'a'. 
It can either decide to expose them as EBCDIC (which makes server implementation easier) or to expose them as ASCII (which makes the URI more readable). 
If the server on the EBCDIC system decides to expose as EBCDIC, then this will give us the following octets: / 84 89 99 / 81 This then results in an URI of /%84%89%99/%81. 
There is no other choice, as we have in "2.4.1. 
Escaped Encoding" An escaped octet is encoded as a character triplet, consisting of the percent character "%" followed by the two hexadecimal digits representing the octet code. 
(Well, you could claim that instead of %84, it may also be %48, because the RFC doesn't say which order the digits go, but I hope you don't want to go there.) For an example that is a bit different, let's say '/d+r/a', we would get / 84 78 99 / 81 in terms of octets, and then /%84N%99/%81 in the actual URI (because the RFC clearly says that the octet 78 is encoded with US-ASCII, which results in an 'N'. 
We could also use /%84%78%99/%81. 
The other alternative is to expose the resource as US-ASCII, i.e. have the conversion work being done on the server. 
In that case, we have / 64 69 72 / 61 , which trivially results in /dir/a. 
It could of course also result in /dir/%61, because %61 is the escape for octet 61 . 
Please remember that it says: Octets are either represented directly by a character (using the US- ASCII character for that octet [ASCII]) or by an escape encoding. 
This representation is elaborated below. 
So overall, the server can make the choice of how to expose a resource name as a series of octets. 
But it doesn't have a choice to expose the resource name as one octet if the octet is escaped, an as another octet if the octet is not escaped. 
I't pretty unclear, I definitely agree with that, but it's not totally hopeless. 
Ok. 
Well, if your proposals result in some clarification, and is reasonably within existing mainstream practice, then that's a good thing. 
I'm definitely looking forward to your proposal, and I'm glad to help. 
Is that different from the "How to Compare URIs" doc? 
If you can include the diagram in my last mail, I think that will help. 
Regards, Martin. 
