Marc VanHeyningen mvanheyn@cs.indiana.edu 
(and others) write: MIME is not basically a text-based protocol, any more than HTTP is. 
I'd be the first to agree that MIME is a fine example of protocol design. 
But it has different design objectives, the chief one of which is to stuff all sorts of data types into (possibly broken) implementations of RFC822 messages and SMTP transport. 
These are clearly lines of text. 
The MIME standard does not really say much about the "binary" content transfer encoding that we say we are using, and says nothing about using it in the body of a message. 
It says that "binary" can contain any sequence of octets. 
What more does it need to say? 
It does not say how to embed binary objects on the same TCP connection as the headers without applying an encoding. 
This is where HTTP seems to have made an innovation, using Content-Length: (The spec refers to Content-Length: as an optional header in MIME external bodies but I haven't found this is in RFC 1521 or 1341.) SGML conformance clearly has particular benifits. 
What do you all think are the payoffs (or downsides) to sticking close to the MIME spec? 
It means that, as new things are defined for MIME (like how to represent Unicode, or how to transport objects with PGP signatures, or how to handle X.400 stuff) HTTP gets it "for free." 
There does seem to be some effort to re-do some of that kind of work in HTTP and do it differently from MIME, in some cases because of differences in design goals. 
Well, that's true, but a lot of that is localized in particular parts of MIME. 
If we can both transport the same types of body parts and we can logically map our multi-part structure onto theirs we ought to pick up most of the extensions. 
You don't need content-length to search for the boundary. 
You can go ahead and scan the binary data for CR LF --boundary-- CR LF . 
Doing so either using a simple scan or using a more elaborate boyer-moore algorithm should be computationally not significantly more expensive than merely counting bytes. 
Yes, you can do this, but to make it work for arbitrary binary files, you have to choose a boundary that is not in the file, either using a random string and hoping, or scanning the whole file (which generally costs more than just determining the size.) 
The MIME boundary mechanism is a complex device that we don't need on an 8-bit clean transport, and if implemented, could be sensitive to the EOL treatment issues, which we are more sloppy about than MIME. 
I think we either need to move closer to MIME conformance or a bit farther away (and I'm leaning toward the latter). 
You don't need content-length to search for the boundary. 
You can go ahead and scan the binary data for CR LF --boundary-- CR LF . 
Doing so either using a simple scan or using a more elaborate boyer-moore algorithm should be computationally not significantly more expensive than merely counting bytes. 
Yes, you can do this, but to make it work for arbitrary binary files, you have to choose a boundary that is not in the file, either using a random string and hoping, or scanning the whole file (which generally costs more than just determining the size.) 
I used to think that boundary-scanning was expensive, but given my experience that CPUs always get faster (I started my programming life using a time-shared PDP-11/40, after all), I think I can see Larry's point. 
A little experiment should help shed some light on this. 
On a reasonably fast machine (a DEC3000/600), I did (after first arranging for /vmunix to be in the filesystem cache): % time wc /vmunix 15102 97927 6351808 /vmunix 0.93u 0.14s 0:01 94% 0+1k 0+0io 15pf+0w a% time ngrep masinter@parc.xerox.com 
/vmunix 0.27u 0.19s 0:00 74% 0+1k 4+3io 47pf+0w ngrep uses a modified Boyer-Moore algorithm. 
Larry is right; Boyer-Moore with a properly chosen search string is actually three times faster than simply counting bytes (although the byte count might actually be available as a side-effect of some other operation, 6351808 bytes in 0.27 seconds is 188 Mbits/sec., so it's not going to be the rate-limiting step.) I did a similar experiment on the slowest machine I could find, an old DECstation-3100 (about 11 SPECmark, I think). 
This time, /vmunix didn't fit into the buffer cache. 
Anyway, I got: % time wc /vmunix 8821 50904 2120392 /vmunix 3.8u 1.4s 0:09 56% 40+71k 263+2io 0pf+0w % time ngrep masinter@parc.xerox.com 
/vmunix 0.3u 1.0s 0:02 67% 78+112k 16+0io 0pf+0w That's "only" 56 Mbits/sec for Boyer-Moore. 
But there is no way that this little CPU is going to drive any network interface at that speed, anyway. 
-Jeff I used to be on the other side of the fence (I thought scanning for boundaries was going to be too expensive), but Ned Freed convinced me. 
Now I'm a convert. 
Boundary scanning is also more *reliable* than content-length, because length calculations are unreliable. 
I was responding to Mitra's point that if you don't use some sort of encoding on the data, a file could conceivably contain CR LF --boundary-- CR LF (or whatever else you wanted to use). 
I can envision a situation where you couldn't download httpd using http because the file contained the boundary string. 
Using content-length to determine the end of a document would be 100% reliable, since the actual content couldn't possibly conflict with the protocol information. 
I only require 99.99999% reliability. 
(Besides, I'm in the market for a Pentium.) 
I'm sorry but I'm quite skeptical about your experimental design and it's applicability to the HTTP context as well. 
For starters finding split points in an inbound byte stream is a different problem than presented by a file full of bytes all present and waiting for processing. 
Secondly, I am suspicious that there is a marked difference in the quality of implementation of wc vs. ngrep interms of program organization. 
Dave Morris I'm sorry but I'm quite skeptical about your experimental design and it's applicability to the HTTP context as well. 
For starters finding split points in an inbound byte stream is a different problem than presented by a file full of bytes all present and waiting for processing. 
Well, I tried the same experiment, except with ngrep getting its data from a pipe: % cat /vmunix | /bin/time ngrep masinter@parc.xerox.com 
real 0.6 user 0.3 sys 0.1 I used /bin/time because the csh built-in "time" command won't accept piped input. 
So, within the resolution of /bin/time's measurement, the user-mode time is the same. 
The extra 0.2 seconds of "real" time may come from having to share the CPU with the cat command, which takes about that long when copying the file to /dev/null. 
Secondly, I am suspicious that there is a marked difference in the quality of implementation of wc vs. ngrep interms of program organization. 
Well, you caught me there. 
Wc is pretty darned slow; after all, "cat /vmunix  /dev/null" takes 0.0 seconds of user-mode CPU time. 
But that's largely irrelevant to Larry's argument; the absolute cost of the Boyer-Moore string search is still too small to worry about. 
-Jeff 
