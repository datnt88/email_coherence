When you do an HTTP method for a URL, the results may depend on things other than the method name and the path of the URL. 
To control 
caching, the source has to tell any client and proxy what things the results depended on other than the URI. 
Sometimes those things are the client's headers (accept:, accept-language:, authentication, state) and sometimes those things are external to what the client sent (date, client IP address, etc., whether the client has a license) A client/proxy that has a cached resource for a URL may want to invoke the same method for the same URL but with different parameters, or in a different context, e.g., when the date has changed, or a helper app has been added and the accept headers might be different, etc. 
In those cases, the client/proxy needs to ask the server for the results of applying the method to the URL, but also indicate to the server which objects it DOES have cached for that URL. 
If a site at Canada were to run a proxy cache for the site, it might have cached documents for both the French and English versions of a URL. 
Different clients should be able to retrieve the French and English versions from the cache without clearing or 'invalidating' the cache for the other clients who want the other language; modifying the English version shouldn't cause the French version to get dropped from the cache. 
Most of the current proposals for headers back and forth don't handle 
this situation correctly. 
Yet a straight-forward enumeration of 'depends-on' in the return from the server to the proxy, along with proxy services that preserve that information and also all headers that comprise the things the value returned depended upon, would be a good first step. 
For some headers, (accept, for example), you need 
more than merely knowing what it depended upon, but also the WAY in which it depended upon the original data, so that the proxy itself can decide whether a cached item is appropriate. 
I seem to have ended up as the leader of the caching subgroup (as well as of the persistent connections subgroup), mostly since nobody else has volunteered. 
Anyway, if you want to participate in the design of the HTTP caching model, please let me know ASAP. 
-Jeff 
Larry Masinter: 
Larry, most of the problems you write about are addressed to some extent by the 1.1 content negotiation mechanism, not the caching mechanism. 
You may want to read my 'notes on content negotiation', I believe they address some of your concerns about things currently missing from 1.1. 
The notes are in and 
In my notes on content negotiation, I proposed a new header Send-no-body-for: list of variant URIs 
for the purpose of indicating the variants the cache DOES have cached 
for a negotiated URL (called a negotiation port in my notes). 
It is interesting to note that the IF: (Unless) header allows one to implement the functionality of my proposed Send-no-body-for header. 
Roy's If: {ne {Location "http://blah.com/doc.french.html"}} would have about the same semantics as my Send-no-body-for: http://blah.com/doc.french.html . 
Reading your message, it seems to me that we have both independently concluded that `Send-no-body-for' functionality is needed. 
Koen. 
what i'm wondering is why content negotiation can't just return the actual URL of the negotiated resource, using the new "Content-Location". 
so if you requested /foo/bar/constitution.txt 
and preferred english, you'd get /foo/bar/constitution-english.txt (and would be silently redirected there), and the proxy would cache it under that name. 
similarly, the french request would get cached under the other name. 
granted, this would mean a "reload" would go to the specific one (and pre-empt negotiation). 
sorry if this is an obvious question, but just how *is* cacheing supposed to work with content negotiation? 
-=- sfw, canadian and former lurker 
At least one issue is the spoofing issue: If clients use the "Location" header as the cache key for the resource, then it is easy for a server to claim to be sending any old URI, and for that resource to get lodged in a cache under the false URI. 
This might not be *so* 
terrible for end-user caches, but it could cause some real trouble if it happened in a more public intermediate proxy cache. 
I don't recall anyone having mentioned a good workaround for this either. 
Shel Kaphan: 
It _can_ return the actual URL. 
From the 1.1 draft: |10.27 Location 
The Location response-header field defines the exact location of the resource that was identified by the Request-URI. 
For 2xx responses, if the Request-URI corresponds to a negotiable set of variants and the response includes one of those variants, then the response must also include a Location header field containing the exact location of the chosen variant. 
[Shel gives a reason why Location cannot work this way:] 
Eek. I had completely forgotten the spoofing issue, thanks for reminding me. 
The solution to this spoofing issue is simple, and I believe it has been discussed before. 
Include the following rule: 
Clients (including caching proxies) should disregard Location headers in 2xx responses if they do not point to the same server that generated the response. 
This restriction still leaves you with a negotiation mechanism powerful enough to handle the French/English example. 
Note that the above solution assumes that either the content providers on the same server either trust each other not to spoof, or that the server has some Location response header filtering mechanism that excludes spoofing. 
This spoofing issue is not addressed in the current draft 1.1 spec. 
I'm in the content negotiation subgroup, I'll make sure we will address it there. 
Koen. 
This is not a safe assumption. 
Numerous providers sell space to many independent people on single servers. 
For example: www.xmission.com 
serves on the order of 1000 independent entities, including many businesses and people, and allows CGI to be owned by the individuals. 
Clearly there is the opportunity for someone to spoof there under the rule. 
It is not significantly safer than unrestricted redirections when many (most?) people share common servers. 
Benjamin Franz 
If the cache key for the returned document were essentially the _pair_ (request-URI, location-URI), then this would be safe. 
Then even if a server returned a completely bogus Location URI, it would still be associated with the request URI, and so could not "escape" to spoof other requests where the request URI was the same as that Location URI. 
Furthermore, associated with this pair (and the object) could be the content-negotiation information that caused that request-URI to yield that particular Location-URI. 
However, in order to avoid duplicate objects in the cache, I think that an object returned with a given Location-URI must flush any other objects from the cache with that URI as either the request-URI or the location-URI. 
This is "spoofable" in a sense, but can only affect performance, and not correctness. 
But this should probably wait for the caching subgroup.... (sorry, Jeff) 
--Shel 
Benjamin Franz: 
The part `or that the server has some Location response header filtering mechanism that excludes spoofing' above is supposed to cover this situation. 
Not that I expect many providers to implement such a filtering mechanism, most would treat web spoofing like they treat news spamming and mail forging now: forbid it in the terms of service agreement and deal appropriately with any found violations. 
Anyway, here is how a Location spoofing filter is supposed to work. 
The HTTP server does some post-processing on all CGI output (except for nph- scripts, I'll cover them later). 
Part of this post-processing includes calculating the Content-Length for the CGI response. 
In the same post-processing stage, the server could check whether any Location headers generated by a CGI owned by Joe indeed point to locations on the server controlled by Joe. 
Depending on the layout of the web space maintained by the server, this test could be as easy as path prefix matching: i.e. the CGI under headers to http://www.xmission.com/joe/doc.fr.html, but not to unix-based servers, is to compare the used-id's on the CGI executable and the file or script pointed to. 
As for nph- scripts: a non-trusting server administrator interested in security would either disable them, or only allow them after auditing. 
Of course, Shel's idea of making the cache key of a negotiated variant be the pair (request-URI, location-URI) eliminates all spoofing risks, we could switch to such a scheme if the consensus is that Location header filtering is unfeasible. 
Shel's scheme is safe no matter how much the server administrator does about security, but has the disadvantage of allowing less cache hits: it would be much more difficult to let preemptive and reactive content negotiation share cache slots for the variants. 
[Note: an explanation of this last statement would require a level of detail only appropriate in the content negotiation or caching subgroups.] 
Unrestricted 3xx redirections are another issue entirely: unrestricted 3xx redirection will not allow Joe to fool a proxy cache into storing a response from his script under John's URI. 
I see the security issues connected to unrestricted 3xx redirects as equal to the security issues connected to unrestricted a href=.. tags. 
They are there, but there is nothing much we can do about them beyond making users aware that web link titles may not be telling the truth. 
Koen. 
Ummmm...Considering the immense magnitude of both spamming and forging today, this is not a convincing argument for leaving it to local option. 
Never-the-less, I believe this is the route that will have to be taken. 
The other route (local filtering) just places too much reliance on good security management at the local level. 
It amounts really to trusting all 
system admins to 'play nice and know what they are doing' - something the ever growing ever growing spam/forgery problems on the Usenet and in E-mail have shown just is not a good assumption in general. 
Just as the default reporting of people's email addresses with the admonishment not to abuse it proved futile (I routinely get requests from my customers to 'give them the email addresses of everyone who visits their web site so they can email them' - I fielded exactly that request not two days ago from one customer), it will prove impossible in practice to make local filtering work. 
Too many local system demands (and insufficient knowledge on the part of admins) will make it nearly impossible to maintain a secure system for many people. 
On large systems with thousands of customers with many special cases, it would be a logistical nightmare even for experienced admins. 
I did not phrase what I meant well. 
I meant 2xx redirections without the proposed rule. 
Benjamin Franz 
Benjamin Franz: 
One note: as long as the cache-validator of an object remains the same it is OK for copies of it to persist in the cache under separate keys, even though it may be the result of content-negotiation in several ways, or the result of a direct request. 
(Example: you have URLs A, B, and C. Requests for both A and B yield "location" C, as do direct requests for C. Though each request would result in a different (key,object) in the cache, ((A,C), (B,C), and (C,-)), if the cache validator for C is the same in all cases there is no problem leaving the other copies in the cache. 
The only performance hit occurs when C changes. 
Then the next similar request for A, B, or C must flush all other occurrences of C from the cache. 
So it really isn't so bad.) Never-the-less, I believe this is the route that will have to be taken. 
The other route (local filtering) just places too much reliance on good security management at the local level. 
It amounts really to trusting all 
system admins to 'play nice and know what they are doing' - something the ever growing ever growing spam/forgery problems on the Usenet and in E-mail have shown just is not a good assumption in general. 
Even if this route is taken, it still may be "robust" cache design to refuse to accept Location headers from an upstream server that do not at least match the hostname part of the request's URL. 
Unless there are exception cases I can't think of ... --Shel Kaphan 
Benjamin Franz: 
Hmm, forging does not happen that often AFAIK. 
Anyway, the kind of 
web spoofing we are talking about here does not have the same global impact as spamming and forging: with the rule that 2xx Location headers not pointing to the server that generated the response should be ignored, this kind of web spoofing can only harm users that share the host with the spoofer. 
As the impact of mismanagement would be limited to local content, I have little problems with this reliance being placed. 
Service providers with security management lousy enough to allow prolonged web spoofing will simply loose their customers and die. 
Not if the Location header filter is user-id based as described before. 
Experienced admins could create such a filter in a few hours, if it is not already a standard option of future 1.1 http servers. 
In other words, I don't share your pessimism. 
Koen. 
It does happen that often. 
I am engaged in cancelling a large (in excess of 2000 articles) combination spam/forgery (with the intent I think of mail bombing the forgery victim) right now. 
Drop into news.admin.net-abuse.* to appreciate just how bad it has gotten. 
We are getting daily reports of forgeries with intent to cause harm. 
I think we will just have to agree to diagree on this. 
Among other things it does not address the practice of 'sub-letting' web space. 
A number of sites (including www.xmission.com) 
allow this as well. 
Benjamin Franz "_Never_ underestimate the power of human stupidity." 
I have added "Spoofing using Location headers (prevention thereof?)" to my list of issues for the caching subgroup, although this is not a commitment that we will actually solve the problem. 
I tend to agree with the view that this is not exactly a protocol design issue, but rather is a problem for people who are implementing shared web servers. 
No matter what criteria we put into the HTTP 
protocol, if www.webcondo.com 
has sold service to both "The Good Guys" and "The Bad Guys" without providing some security barriers between them, then nothing we can do in the protocol spec will solve everything. 
But it may be that we can include some recommendations that will improve security without significantly compromising performance. 
And some of these may be necessary to provide correct caching even without the threat of malicious behavior. 
-Jeff 
I agree that this is not exactly a protocol design issue. 
However, there are a number of aspects to caching that are not exactly part of the communication protocol. 
Larry Masinter wondered (though I think it might have been just to me) whether we shouldn't consider doing a separate I-D to cover caching, presumably to address the kinds of issues that are not strictly part of the communication protocol, but that need to be, or at least would be far better off being, nailed down in any case. 
But we can talk about this after the caching sub-wg gets going. 
--Shel 
Jeffrey Mogul: 
For the record, I feel that the spoofing using Location headers issue is really a sub-problem of content negotiation, not of caching. 
But I'd be happy to deal with this problem in the caching subgroup instead of the negotiation subgroup. 
Two of the four members of the negotiation subgroup are also in the caching subgroup, so there is litte chance of these groups getting too much out of sync. 
In my opinion, considering the impact of the protocol design _is_ a protocol design issue, even if this consideration leads to a review of shared web server security mechanisms. 
We can only afford to introduce a web security problem and pass it to the people who are implementing shared web servers after we have established that these people can actually provide a solution to the security problem. 
As long as we do not have consensus that they can solve the problem, we had better not put this security problem in the HTTP protocol. 
I have been thinking up a negotiation header structure that would not allow any form of cache spoofing _and_ provide nice cache efficiency (preemtive and reactive negotiation sharing cache slots). 
The downside of this structure would be the introduction of a new request header with rather unusual semantics. 
I'll try to post a description in the near future. 
Koen. 
Absolutely. 
A Vary: 1#(http-header-name) header returned by the server would help solve this problem. 
The 1.1 content negotiation mechanism is only of use for the caching problem if the server provides the client/proxy with sufficient information for it to do the document selection itself. 
The 1.1 draft supports this in the limited cases where the variance can be expressed by the URI: header. 
The 1.1 mechanism is not of use when 1. The server does not provide individual URLs for all the possible documents that could be returned when accessing a URL. 2. The server may have URLs for the individual documents, but does not return an exhaustive list of possibilities (maybe it would be too large) 3. The document returned depends on a parameter in a manner that cannot be described by a URI: header. 
(e.g. available in multiple character sets.) Without a Vary: header, servers will have to mark the response as not cacheable. 
David Robinson. 
I like this syntax. 
The URI header would work, but this is short and to the point. 
Dan 
