(This was Re: Unverifiable Transactions / Cookie draft, but I think the topic has drifted far enough to merit a new Subject). 
From: Patrick McManus mcmanus@appliedtheory.com 
In a previous episode Ted Hardie said... :: Just to clarify, the proposals are to standardize a method :: to *allow* proxies to report this kind of data. 
Nothing in the :: proposals *makes* anyone do anything. 
Jeff and Paul :: were very clear about that from the beginning, and it :: keeps the hit-metering draft out of the scary :: "big-brother" category. 
Right on.. and to clarify a little further when serving to a proxy the origin server is told whether or not the proxy pledges to return this information at a later date.. if it doesn't they can cache bust.. the weakest point of the hit-metering draft IMHO is that it doesn't try and provide any other methods of determing proxy reliability wrt this pledge to base the "to cache or to bust" decision on.. It is true that there is no technical mechanism in the hit-metering proposal to prevent a proxy from agreeing to hit-meter a response, and then not doing so. 
The proposal states MUST-level requirements, but provides no means to verify that they are always observed. 
But this is not any different from any other HTTP protocol requirement. 
For example, an origin server can send Cache-control: no-store to a proxy that identifies itself (in the request header) as compliant with HTTP/1.1, but there is no way for the origin server to verify that the proxy actually obeys this directive. 
If anyone can suggest "other methods of determining proxy reliability with respect to this pledge" (or any other pledge implied by an HTTP/1.1 version number), I'd be interested. 
But in general, this reduces to the problem of copy-protection in fully digital representations. 
The only way that I know how to solve this, in a network that spans administrative boundaries, is to use both end-to-end encryption and tamper-resistant decryption hardware at the client end. 
But this doesn't seem either feasible or desirable. 
There are non-technical means to verify compliance (auditing, planting fake information to trick people into exposing copyright violations, etc.), but these are beyond the scope of a protocol specification. 
-Jeff 
In a previous episode Jeffrey Mogul said... :: It is true that there is no technical mechanism in the hit-metering :: proposal to prevent a proxy from agreeing to hit-meter a response, :: and then not doing so. 
The proposal states MUST-level requirements, :: but provides no means to verify that they are always observed. 
:: But this is not any different from any other HTTP protocol requirement. 
Jeff, I'm starting to see this in a new light, your argument about protocol trust is a good one. 
In summary non reliable worries me more than non compliant, read on. 
What I'm still hesitant on is what I feel will be a very strong content-provider hesitation to this proposal because it's accuracy is so unbounded. 
While solving that problem precisely is extremely difficult I think that something needs to be done to reduce it.. consider this: A content provider implements hit-metering instead of doing their current cache busting technique.. they're most pleased as the server load drops by 50% (as does potentially WAN traffic) but they also take a 10% drop in usage numbers (after aggregation of all the proxy reports).. if they get paid on a linear model they could lose 10% of their income.. on quota systems it could be even worse, so they immediately revert back to cache busting and their numbers return to their expected values. 
It's a shame really, because we should all win by them using hit metering.. not only is net traffic reduced but I would think that their access numbers (if reported properly) could actually go up as site availabilty isn't always at the whims of WAN reliability as local caching enters the picture. 
What they'd like to do is not allow caching to non compliant or non reliable caches and do so for the rest.. 
I think that non reliable here is more important than non compliant.. 
I'm not worried about the proxy's algorithm as much as I am machines that are perpetually rebooted or have their proxies restarted by itchy sys admins.. or even machines that devote a fixed amount of resources to storing these counts and when those resources are exhausted can't report them to the origin server because of network unreachability and drop the report on the floor instead.. certain environs are prone to being chronic with this sort of thing. 
I made a proposal months ago about being able to (at the origin servers option) force the return of 0/0 counts.. at least this would allow the construction of deterministic audit trails and therefore some notion of reliability.. it doesn't account for outright fraud by the proxy of course (they could misreport the numbers) but it does close the case of any open ended transactions.. 
I'm not sure that it is enough, but I do think it helps considerably in establishing 'good faith and a reliable history' which is something to go on.. -Pat, not feeling bad about bringing this back up when it's still in ID and considering we can do 50 messages a day on cookies that are nearing last call.. 
I'm starting to see this in a new light, your argument about protocol trust is a good one. 
In summary non reliable worries me more than non compliant, read on. 
What I'm still hesitant on is what I feel will be a very strong content-provider hesitation to this proposal because it's accuracy is so unbounded. 
It's not clear that the accuracy is really that unbounded. 
Assuming that non-compliance is an orthogonal issue, the three things that could lead to inaccuracies are (1) perturbations of access patterns, due (as Koen has argued) to the potential for more cache-busting outside the metering subtree (2) failure (or reboot) of a proxy before a report is delivered (3) loss of a report message before it reaches the origin server (i.e., through network failure) If there are other sources of inaccuracy that I've missed, please let me know. 
Item #1 is, for now, unknowable. 
Perturbation could just as easily improve the situation, since, as you observe, if hit-metering increases caching, then more users might be accommodated. 
Item #2 is addressed in the latest draft, by adding an optional timeout to the Meter response-directive (i.e., to the server's request that the response be hit-metered). 
This can't eliminate the problem of proxy crashes or reboot, but it can bound the likelihood of report-lost-due-to-proxy-failure. 
E.g., if the timeout is set to 10 minutes, and the mean time between reboots for the "average" proxy is (say) 60 minutes, then there is a 1/6 chance of report loss. 
Since I suspect that MTBF for proxies is probably on the order of days, not hours, the actual loss probability is likely to be lower. 
This leaves #3, loss-in-transit. 
My experience is that the most common way for servers to lose HTTP requests is due to internal congestion (i.e., the SYN_RCVD problem), so if hit-metering improves caching, the reduction in congestion ought to help this. 
But loss due to network partition is also a problem, and (according to Vern Paxson's SIGCOMM '96 paper) it's getting worse. 
This has inspired me to change the text in the next version of the draft from "The proxy is not required to retry the [report] if it fails" to "The proxy is not required to retry the [report] if it fails (but it should do so, subject to resource constraints)." 
This is still "best-efforts", but the specification now encourages more effort. 
The next draft will also say: Note that if there is doubt about the validity of the results of hit-metering a given set of resources, the server can employ cache-busting techniques for short periods, to establish a baseline for validating the hit-metering results. 
(with a citation to James Pitkow's WWW6 paper for more discussion of such sampling techniques). 
Given that this gives each origin server a way to answer the question "is hit-metering making my counts inaccurate?", it seems to avoid the question of whether hit-metering is accurate in general. 
(Clearly, a server that discovered this way that hit-metering is giving bad results would simply stop using hit-metering, at least for a while.) I made a proposal months ago about being able to (at the origin servers option) force the return of 0/0 counts.. at least this would allow the construction of deterministic audit trails and therefore some notion of reliability.. it doesn't account for outright fraud by the proxy of course (they could misreport the numbers) but it does close the case of any open ended transactions.. 
I'm not sure that it is enough, but I do think it helps considerably in establishing 'good faith and a reliable history' which is something to go on.. I tried putting support for 0/0 counts in a version of the proposal, but I took it out in favor of the timeout mechanism. 
James Pitkow's paper points out that the lack of a time-bound on the reports was a serious flaw of the original proposal. 
I think if the origin server can say "send a report within X minutes, if you have anything to report" then this effectively does the same thing as a request for 0/0 reports, but without the additional message overhead. 
(Remember, lots of studies have shown that most cache entries are never used more than once.) A 0/0 report also doesn't solve the "proxy rebooted before sending a report" problem, but the timeout "solves" it (probabilistically). -Pat, not feeling bad about bringing this back up when it's still in ID and considering we can do 50 messages a day on cookies that are nearing last call.. 
Your comments have been quite valuable. 
-Jeff 
My comments are based on the belief that draft-ietf-http-hit-metering-00.txt dated 1/21/97 is current.. let me know if that's not the case.. Jeff makes some references to 'latest draft' that had me confused, but now I think I realize that he just means an unreleased version.. In a previous episode Jeffrey Mogul said... :: Item #2 is addressed in the latest draft, by adding an optional :: timeout to the Meter response-directive (i.e., to the server's :: request that the response be hit-metered). 
This can't eliminate :: the problem of proxy crashes or reboot, but it can bound the :: likelihood of report-lost-due-to-proxy-failure. 
E.g., if the :: timeout is set to 10 minutes, and the mean time between reboots :: for the "average" proxy is (say) 60 minutes, then there is a 1/6 :: chance of report loss. 
Since I suspect that MTBF for proxies is :: probably on the order of days, not hours, the actual loss :: probability is likely to be lower. 
This does address my issue nicely.. My first reaction was that it complicates even further construction of the 'non-naive algorithm for scheduling the deletion of hit-count entries' that the proxy is strongly encouraged to use by section 3.5 but the capacity to bundle together and schedule reports for a single server is far more predictable with respect to a max time value than with respect to a max usage value so the penalty appears worthwhile. 
:: This leaves #3, loss-in-transit. 
My experience is that the most :: common way for servers to lose HTTP requests is due to internal :: congestion (i.e., the SYN_RCVD problem), so if hit-metering :: improves caching, the reduction in congestion ought to help this. 
:: But loss due to network partition is also a problem, and (according :: to Vern Paxson's SIGCOMM '96 paper) it's getting worse. 
This :: has inspired me to change the text in the next version of the :: draft from "The proxy is not required to retry the [report] :: if it fails" to "The proxy is not required to retry the [report] :: if it fails (but it should do so, subject to resource constraints)." 
:: This is still "best-efforts", but the specification now encourages :: more effort. 
A strictly editorial comment.. 
I like the content, how about the slightly firmer language: The proxy SHOULD retry the [report] if it fails but MAY abort it if resource constraints dictate. 
:: I think if the origin server can say "send a report within :: X minutes, if you have anything to report" then this effectively :: does the same thing as a request for 0/0 reports, but without :: the additional message overhead. 
sold me. 
-P 
My comments are based on the belief that draft-ietf-http-hit-metering-00.txt dated 1/21/97 is current.. let me know if that's not the case.. Jeff makes some references to 'latest draft' that had me confused, but now I think I realize that he just means an unreleased version.. Yes, draft-ietf-http-hit-metering-01.txt is currently wending its way through the Internet-Draft editor's queues. 
Sorry for the confusion. 
:: This leaves #3, loss-in-transit. 
My experience is that the most :: common way for servers to lose HTTP requests is due to internal :: congestion (i.e., the SYN_RCVD problem), so if hit-metering :: improves caching, the reduction in congestion ought to help this. 
:: But loss due to network partition is also a problem, and (according :: to Vern Paxson's SIGCOMM '96 paper) it's getting worse. 
This :: has inspired me to change the text in the next version of the :: draft from "The proxy is not required to retry the [report] :: if it fails" to "The proxy is not required to retry the [report] :: if it fails (but it should do so, subject to resource constraints)." 
:: This is still "best-efforts", but the specification now encourages :: more effort. 
A strictly editorial comment.. 
I like the content, how about the slightly firmer language: The proxy SHOULD retry the [report] if it fails but MAY abort it if resource constraints dictate. 
The text I put into draft-ietf-http-hit-metering-01.txt says: - The proxy is not required to retry the HEAD request if it fails (this is a best-efforts design). 
To improve accuracy, however, the proxy SHOULD retry failed HEAD requests, subject to resource constraints. 
Your wording is perhaps a little crisper; I'll think about using it in a subsequent draft. 
Thanks -Jeff 
