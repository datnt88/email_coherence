Is there a proposal to add globally unique id's to HTTP? 
There is a noticeable lack of unique object identifiers. 
Large objects are often duplicated on the Web....and UID's assigned to largish objects would reduce traffic. 
These UID's could comes through as response-header fields. 
"Content-Origin: url | some-other-distinguished-name" "Content-Identifier: md5-coded-hash | some-other-authenticatable-id" so when an indexer sees that....it can say "well...i already have that...so i'll store it as an alternate location of the same thing" or whatever.... or maybe even a little dumb authentication protocol: -client uses a HEAD method -server X's response contains the header: "Content-Identifier: purportedly-unique-content-id" -client already has a copy of this document given to him by Y -....asks if server X if Y's copy is to be trusted OR some such nonsense......... 
While the idea is sound, some time ago I did some measure on our proxy cache: out of ~300 MB of files in the cache, only about 7MB were duplicates with a different name. 
I only considered files with the same size (after stripping metadata), so I might have missed something, say text files with different end-of-line conventions; also, this test should really be repeated on a larger data set. 
Anyways, I am not very convinced that the saving are worth the effort of handling multiple headers for the same object (while I was *before* doing this test). 
Luigi Luigi Rizzo Dip. 
di Ingegneria dell'Informazione 
I got curious about this a little while back, and wrote a little Perl program to calculate MD5 checksums of the objects in our (local/regional ?) cache, so we could see how many were dups. 
The results weren't very encouraging... URL:http://www.roads.lut.ac.uk/lis ts/ircache/0202.html Martin 
