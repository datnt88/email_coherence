It occurs to me that this has been discussed a little on IRC, but that posting here will reach a wider readership. 
So here goes. 
I hope this makes sense, though it may very well not :-) Some of us have been using the W3C annotea server to store EARL. 
I recently completed a posting agent that enables Valet to post directly to the server. 
One thought that occurred to me was to make this operational, so that Page Valet would save its own results and use a cron job to submit to Annotea. 
This could rapidly build a corpus of test results, that could be matched to evaluations on the pages concerned by other agents - in particular human agents. 
Now, EARL and Annotea are different, and as things stand, embedding earl in annotea is a clumsy hack. 
Furthermore, ericP (of the Annotea server) doesn't think it's very sensible, and I take his point insofar as lots of EARL evaluations might be seen as "noise" in his database. 
OTOH the server and client technologies developed for Annotea could certainly be adapted for EARL, and become the basis for building a corpus. 
The exercise of doing so will of course also be valuable real-life experience with EARL in a larger-scale project than has hitherto been undertaken. 
If I were to hypothesise a corpus-building project, what support might it expect: * Set up a new EARL server, based on the Annotea server. 
Ensure that it properly tracks changes in pages, as this is more important to us than to Annotea. 
* Adapt existing Annotea Clients to work with it. 
* Document the differences between us and annotea. 
* Use automated processes (such as Page Valet users, and/or spiders) to assemble a corpus. 
There is an issue here of unrepresentative sampling, but that's probably not important. 
* Use this infrastructure to collect human evaluations, either "blind" (just evaluate a page) or in the context of existing evaluations. 
* Develop a methodology for evaluating tools against the same corpus. 
I believe this will help developers like me to improve our tools, but the more important result will be new insights into the WCAG itself. 
This latter might for example arise where human evaluators disagree with Valet, but it is clear that Valet is right according to WCAG. 
Nick Kew Site Valet - the mark of Quality on the Web. 
"Nick Kew" nick@webthing.com 
Tracking changes isn't an Annotea issue certainly, but we need some way of looking at changes, I've been trying to find time to look at this from my own link maintenance needs but haven't got anywhere really yet. 
(link monitoring in usenet FAQs after I got caught out with pages changing subjects entirely without changing urls) This is an important issue I think, and one that whilst we've often touched on in meetings haven't fully researched, although again real world experience is likely needed which a corpus would also give (perhaps it would be a good idea to cache pages we evaluate somewhere. 
My two agents will do this as soon as is practical, and I'll possibly have graphical querying of RDF in my RDF Editor which will also be given an Annotea interface (it'd be the same annotea interface as in Snufkin and FillyJonk so no skin of my nose.) I'll also look at either a server side, or client-size in Mozilla solution so as not to be windows IE specific. 
One thing before this is an agreement on a single namespaces to use, to make the evaluation tools simpler, I'd propose either rapidly agreeing a 1.0 namespace or formalise the 1.0-test. 
Jim. 
I think tracking changes in web pages is more of a TestSubject issue (see the thread above). 
It should be the EARL-producing client's responsibility to compute a hash of the page contents (or whatever) that will uniquely identify a page's content with a high probability - the EARL database just needs to be able to tell the difference between two hashes if you ask for a specific one from a certain date. 
One problem with the current namespace (this has probably already been discussed, forgive me if I'm repeating anything) is that there are too many ways to say the same thing. 
Example: earl:passes vs. earl:validity earl:Pass; earl:confidence earl:Certain. 
If someone asserts a Likely Pass we should be able to query for any passing state and return that along with plain old "passes." 
Algae queries can't do that because they don't "know" what "passes" is supposed to represent. 
I'm also worried by the 0.95 examples allowing :Joe earl:asserts{ :SVGTool :passMedium :CircleTest} As far as I can tell (correct me if I'm wrong here), Algae will think that's different from :Joe earl:asserts {rdf:type :Assertion; rdf:subject :SVGTool; rdf:predicate :passMedium; rdf:object :CircleTest;} So either we need to make EARL stricter so that there is only one way to say anything so that we can know what the exact relationship between any bits of triples we want to query against each other (like Annotea), or the database needs to be able to somehow look up the canonical form of any statement and convert when something is submitted. 
It'd also be kind of cool to have a database that allows queries to go "triple-surfing" so you could find the relationship between any two bits of triples by walking up a chain of identical subject-object pairs. 
It seems like something that would be potentially really slow, but has probably already been implemented somewhere. 
Nadia 
