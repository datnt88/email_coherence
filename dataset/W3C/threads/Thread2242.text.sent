Agreed. 
But I think if one looks at the long run, which is what is most important for a standard, all the proxies will work properly. 
I just find it kind of weird that the RFC specs nondeterministic behavior -- sometimes cookies work, sometimes they don't, and you can't always know when they will without a lot of complexity. 
Dwight From: M. Hedlund hedlund@best.com 
montulli@netscape.com; 
yarong@microsoft.com 
Subject: Re: Unverifiable Transactions / Cookie draft Date: Monday, March 17, 1997 12:34 PM Designers of web sites (at least the large percentage who will advertise on the web) will have to take into account that cookie assignments on their home page may fail a large percentage of the time. 
If they wish to measure number of unique visitors to their site, they will get a highly inaccurate reading since often multiple cookies will be assignied to a single user before one "sticks". 
This is reportedly already the case to due to faulty proxies. 
I believe Hotwired keeps an extensive database of which proxies fail to properly pass-through cookie assignments as a result. 
My point is simply that other system faults already require the complexity you mention. 
M. Hedlund hedlund@best.com 
Sure, but then soon to come are cookie monsters (well actually I'm aware of at least one already). 
And all proxies I know of can easily block sites converting the banner ad to a small broken image icon. 
I know of one personal web tool based on proxy technology which has this ability as well. 
The more people who are offended by hidden profiling, the wider the deployment of such tools. 
I think the question is from whose perspective the behavior is non-deterministic. 
The behavior is quite deterministic from my perspective as a user. 
In the single very limited involvement I had with advertising, the revenue was associated with the click thru on the banner ad, not the bannner ad. 
That would, as I understand the DoubleCLick model would be a click to the DoubleClick site from which a set-cookie wouldn't be considered an unverifiable transaction. 
If I were advertising, knowing when a user actually asked to see more of my message would have tremendous value compared with having my ad where they might have seen it. 
The bottom line is that WWW advertising is based on a business model which didn't even exist a couple of years ago. 
It is based on leveraging technology which is new and evolving. 
I am confident that it will evolve in response to improvements to the technology.. David Morris For some models this makes sense - I even know ad models where content sites are compensated only as a percentage of online sales generated! 
But there is a compelling argument that compensation models based on clickthroughs or more are flawed because it encourages the content developer to focus exclusively on pushing people through the ad, rather than actually providing useful content. 
I'm sure many content sites would refuse to partake in this. 
The business model of sponsored content has been around forever. 
So have these privacy issues - how do you know that donation you made to the Sierra Club last year didn't lead to that Democratic National Committee soliciation you got yesterday? 
Looking to technology for a solution to the privacy problems is as misguided as blaming technology for causing them. 
I have come to believe over the last year that this is not a problem for which technology can provide a complete solution. 
Intent is everything - and even browsers that obey this spec and folks who don't use cookies are still ripe for abuse by parties with intent. 
Fortunately abuse is not a requirement to make a profit in this model - doubleclick doesn't have to be able to do a credit check on you or know your email address to do what they do. 
Two companies which have similar privacy issues - FireFly and Narrowline - backed up their claims of consumer protection by paying Coopers &amp; Lybrand (one of the Big 6 accounting firms) to perform a personal information security audit. 
Perhaps DoubleClick should do the same. 
I think in the long run a solution like one suggested by "Jaye, Dan" DJaye@engagetech.com in is the one which most closely semantically matches what's going on here. 
The issue is one of trust - and trust brokered by a certificate authority (be it Coopers&amp;Lybrand, ETrust, Better Business Bureau, an organization of the EC, whatever) makes sense. 
But in the short term... let's not kid ourselves. 
This will not significantly increase the privacy of users on the web. 
This is not similar to the "opt-in/opt-out" debate amongst direct marketers - of which I am firmly in the land of "opt-in". 
This is a small hurdle - even just using the IP number and User-Agent as the key to the profiling database would be sufficient for many uses, and slightly more elaborate schemes can be used to make it much more precise. 
That it is a small hurdle to work around is *not* a justification for putting it in the spec. 
I am somewhat loath to enter this conversation as I was periphery to the discussions early on which led to this, and I now have a slightly different opinion, and we know this RFC has been in development for a long time. 
The fact that my opinion can be different today suggests it may be too early to try and coerce the protocols into implementing policy... just a thought. 
I think Dwight's proposal might be a little overcomplicated - something like the below might address most concerns out there? 1) By default, user agents are configured to prompt for confirmation on receipt of all cookies - unlike today's defaults which is to accept all cookies. 
2) Upon receipt of a cookie, UI must have the options: Accept this cookie Do not accept this cookie Accept all cookies from this domain Never accept cookies from this domain 3) UI allows for some indication of pages with content inlined from other servers. 
Perhaps specially flag cookie requests for inlined content too. 
4) Remove the restriction on unverified transactions. 
Show me this leads to /less/ privacy, particularly in combination with all the other existing provisions of the current spec. 
I really wish this was a more clear-cut situation - that cookies really did make the difference between being "database-able" or not. 
It doesn't. 
Brian brian@organic.com www.apache.org 
hyperreal.com 
http://www.organic.com/JOBS 
[I believe my earlier comments make clear that I disagree with some of Brian's positions. 
I will try to limit these comments to new or particularly murky points.] 
Brian, I would (in all seriousness) like to have a long discussion with you on this assertion sometime in the future, but I don't believe it is relevant to the draft we are discussing. 
I don't think anyone is trying to solve the Web's privacy problems with this draft; and if we were, I would agree with Yaron that such a goal would be way out of the scope of this WG. 
Instead, I believe we are trying to avoid _creating_ privacy problems in the HTTP spec. 
What's "policy"? 
I don't consider this "policy" discussion to be all that different in _scope_ from the discussions we had last year, in which many WG members asserted that creating a scalable caching mechanism was the most important work before us. 
Is the HTTP spec implementable between one browser and one server without a solid caching mechanism? 
Sure -- but to leave it at that would ignore the realities of bandwidth depletion and network congestion that HTTP's implementation has, in part, caused. 
So the group let a "policy" question -- in this case, resource depletion -- guide its work. 
My point is not that caching (bandwidth conservation) and cookies (privacy) are indistinguishable -- obviously, there are many dinstinctions. 
Instead, I am arguing that it is specious to dismiss privacy protections as "implementing policy." 
We should not retreat from a privacy issue simply because it seems a broader concern than the specification of working code. 
[proposal:] I think in the above you are doing what Yaron is asking that we avoid: specifying a UI in a wire protocol. 
I would agree with him that the above is too restrictive. 
Less privacy than what? 
The current implementations from the Netscape cookie proposal? 
Perhaps not. 
The implementations that would arise from compliance with the draft RFC? 
I would argue that (4) above would substantially reduce privacy protection for the user if compared to the current draft. 
You are arguing for an "after-the-fact" notification (3), which I don't think is sufficient. 
I agree that it doesn't _solve_ the problem of "databasability." 
I disagree that anyone is trying to do that. 
M. Hedlund hedlund@best.com 
I think that this debate has been revisited sufficiently that we're no longer making good progress. 
I am looking for ways of wrapping up the discussion, rather than repeating (endlessly) arguments made and remade again. 
There is a significant divergence of views, and many remain steadfast that the original tradeoff in 2109 between privacy and capabilities are well considered. 
I think there's also a significant counter-opinion developing. 
I would suggest -- as an alternative to continuing on the mailing list -- that we ask those who would prefer to see a different resolution on the issue of unverifiable transactions to write a separate internet draft, outlining what the protocol should say instead and addressing the issue of user privacy to the same detail as RFC 2109. 
Perhaps if we can see a complete design which adequately protects user privacy, we can consider the alternative with sufficient technical detail. 
Is this OK? Can we close down discussion on this point until we have a fully worked out alternative from someone? 
Thanks, Larry 
