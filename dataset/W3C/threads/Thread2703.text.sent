Hi! I am currently working with a traffic model for an IP network. 
I have a question about the amount of overhead involved when transfering binary files over http. 
As fas as I understood, http handles binary files so no encoding has to be performed (as for example files over SMTP uses MIME). 
So, how many header-bytes does http add in each packet when sending a binary file from a server to a client? 
I hope someone can help me with this. 
Thanks, Patrik Depends on the server, but typically around 256 bytes. 
E.g. this instance of Apache response uses 279 bytes: ab -v 4 http://www.foo.com:81/poweredby.png 
HTTP/1.1 200 OK Date: Thu, 30 Nov 2000 10:21:44 GMT Server: Apache/1.3.12 (Unix) (Red Hat/Linux) PHP/3.0.15 mod_perl/1.21 Last-Modified: Wed, 01 Mar 2000 18:37:44 GMT ETag: "33303-482-38bd6378" Accept-Ranges: bytes Content-Length: 1154 Connection: close Content-Type: image/png Cheers, /Patrik. 
Patrik Winroth pwinroth@alteon.com 
You have an HTTP response header (around 250 bytes) which goes in its own segment and then all succeeding data is sent in TCP segments as a stream of bytes. 
No overhead other than the TCP/IP overhead. 
The latest standard (HTTP 1.1) has provisions for compression and "chunked" transfers which change this, but I haven't seen these used in any real-world situations yet. 
Doug................... Chunked encoding is frequently used in situations where the server might not know the size of the response body at the time it is generating headers. 
This is often the case for CGI and other dynamic interfaces. 
If chunked encoding is used, there is a short chunk header that preceeds each chunk - no more than 10 bytes; the chunk size is up to the server, but is typically either the record size of the buffer it is using to read the dynamic content, or the buffer size of its network buffers (depending on which memory it is optimizing for). 
Scott Lawrence Architect lawrence@agranat.com 
Virata Embedded Web Technology http://www.emweb.com/ 
Apache will recognize a file with a ".gz" extension as gzipped, and send the Content-Encoding: x-gzip line. 
Netscape will recognize Content-Encoding: x-gzip, and uncompress the file. 
Unfortunately, at least in my installation (Apache 1.3.14, Red Hat 7), Apache doesn't look at the extension before the ".gz" to get the content-type; "foo.txt.gz" gets marked as Content-Type: application/x-gzip. 
|John Stracke | http://www.ecal.com 
|My opinions are my own. 
| |Chief Scientist |================================================| |eCal Corp. |But how do we know destroying the Van Allen belt| |francis@ecal.com|will kill all life on Earth if we don't try it? 
| Many apache servers send data chunked. 
Takes a couple bytes (average of 4) for every block transfered. 
Maybe a total overhead of an additional 20-40 bytes per transfer (maybe less). 
This is just a guess... - Joris Almost all modern browsers support content-encoding (gzip, for example), and most servers can be pummelled into serving compressed content; actually, I've seen a lot of activity around this recently, with vendors releasing products to faclilitate compression on the server, or to move it to an intermediary. 
While not many Web sites are using compression, the number is growing, and I've heard rumors of some Big sites playing with it on home pages, etc. (haven't checked recently). 
I've also heard rumors that browsers are starting to support transfer-encoding. 
Mark Nottingham Many apache servers send data chunked. 
Takes a couple bytes (average of 4) for every block transfered. 
Maybe a total overhead of an additional 20-40 bytes per transfer (maybe less). 
This is just a guess... - Joris From: francis@localhost.localdomain 
[mailto:francis@localhost.localdomain]On Behalf Of John Stracke Sent: Thursday 30 November, 2000 16:45 Subject: Re: Http overhead Yes, but it's actually better than that: AFAIK, Apache uses chunked transfer-encoding only for dynamic resources, where it can't predict the content-length. 
The alternative would be (a) buffer the output before sending it down, or (b) defeat persistent connections. 
Either (a) or (b) would increase; (b) would actually cost extra bandwidth, and (a) would cause bandwidth consumption to come in spikes. 
So, most likely, the cost of chunking is lower than the cost of not chunking; it's certainly lower than the nominal overhead of the encoding. 
(Sorry to those to whom this is obvious--probably including Joris--but I didn't want to leave anybody thinking they could save bandwidth by turning off chunking. 
:-) |John Stracke | http://www.ecal.com 
|My opinions are my own. 
| |Chief Scientist |================================================| |eCal Corp. |In the country of the blind, the one-eyed man is| |francis@ecal.com|in therapy. 
| You are right, I did know this. 
Indeed that's why I favor chunked encoding for dynamic resources. 
MSN's (search) server (MS-IIS) transfers the data by just closing the connection at the end (not sending any indication about the size of the document) what also decreases the transfer 'reliability' - or better, you don't know wether you have all or not... HTML or JPEG/GIF you can guess it, but with many other sources you don't (like TXT).... Also MSN's server has thus more overhead for the server and waisted time between the requests for the client... - Joris From: francis@localhost.localdomain 
[mailto:francis@localhost.localdomain]On Behalf Of John Stracke Sent: Friday 01 December, 2000 16:04 Subject: Re: Http overhead It did was a while ago, when I did some experimenting (personal purpose) with HTTP. 
Your server (MSN Search) didn't return a content-length field or send the data chunked (this is a good while ago)... Also another server from CNET did the same thing. 
At least this was about a year ago, or maybe even a longer time a ago, this wasn't recently.... 
I don't know how your search servers work, but maybe the server just forwarded a response from another server. 
The URL I gave to the server (using telnet) I had from the search area in IE 4. So maybe actual data came from someone like Infoseek or Aslavista.... The server however did turn, at that time, "Server: Microsoft-IIS[...]". 
At that time I was developing a simple proxy server (for myself), that ran into trouble with sites not giving a content-length or using chunked tranfer-encoding. 
The first trouble I then developed support was chunked tranfer encoding, what did turn out to work correct. 
But at this time I still couldn't download from MSN search of CNET. 
Possiblity that it was this error. 
Also the proxy didn't support caching (yet) and did turn out to be quite unreliable, especially with subsequent requests on a connection. 
But don't forget I is a long while ago, what I actually should have put in the previous mail (my fault). 
Software and systems evolve, and you probably don't have the same stuff you had a couple years ago... - Joris -----Original Message----- From: francis@localhost.localdomain 
[mailto:francis@localhost.localdomain]On Behalf Of John Stracke Sent: Friday 01 December, 2000 16:04 Subject: Re: Http overhead 
