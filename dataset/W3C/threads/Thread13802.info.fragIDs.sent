Do any of the evaluation and repair tools place a meta data tag in the html source of a web page after the page is evaluated and/or repaired? 
Bobby 4.0, PageScreamer 4.1, and Lift (to name a few) do not seem to have that option. 
Some have the option to add an icon to the page (Lift and Bobby for example have "approved by" icons). 
Should the tools have the option to add meta tag? 
Does Dublin Core provide a standard or reserved "name" for E R T's? Would/should a meta tag ever point to an EARL file? 
For example, 
perhaps EARL url here" / 
V6.0.2 for Windows" Regards, Phill Jenkins IBM Research Division - Accessibility Center 
Good question. 
It would be a five-minute update to AccessValet to do that for its automatic-fix options, but that wouldn't really add any more value than Tidy's meta. 
I think I'd prefer to amend your suggestion to separate URLs from inline info That would fit nicely with work-in-progress on AccessValet 1.1. 
Nick Kew 
A-Prompt does not place EARL or a meta tag in the page. 
We store the evaluation/repair results in a separate file in EARL format. 
I like Nick's suggestion of adding meta info that points to the evaluation/repair report: I wonder what users would think of a tool that added a couple extra lines to 
their files? 
Chris 
DC.Relation.conformsto 
would be the obvious choice from teh Dublin Core vocabulary that I know. 
But I don't think that inside the page is the best place to put this kind of information - apart from anything it restricts the ability of people to decribe information they cannot directly edit - which cuts out the possiblity for third parties to provide valuable information taht would help users (one of the neat features of PICS...) cheers Chaals 
SWAD-E http://www.w3.org/2001/sw/Europe ------------ WAI http://www.w3.org/WAI W3C, 2004 Route des Lucioles, 06902 Sophia Antipolis Cedex, France 
I have some thoughts on DC.Relation.conformsTo 
at [1]. 
Another idea is to do something like: link rel="meta" href="myearl.earl" 
/ [1] http://lists.w3.org/Archives/Public/w3c-wai-er-ig/2002Oct/0015.html 
wendy a chisholm world wide web consortium web accessibility initiative 
For people who have control over a document, and are interested in publishing the EARL metadata, the HTML link element is great. 
But a couple of other possiblities arise. 
People may not want to publish externally their internal assessment data. 
In this case they are unlikely to point out where it is in a document they serve publicly. 
Also, third parties may be interested in rating the accessibility (or other conformance characteristics) of documents or things with URIs. 
In each of these cases something like annotea would be useful - using the concept of a ratings bureau that was introduced for PICS as a way of looking for RDF information abd a document. 
The annotea infrastructure allows for an annotea server to require authentication before giving out results, or for making results public, which would support the two use cases listed above. 
In general it isn't a problem for someone who produces metadata to find it - even if they don't have any control over the original document their data is about. 
The difficulty is how another third party finds metadata in the web at 
large - and this is something that is true of all RDF, not just EARL (it is true of XML in general...) Cheers Chaals 
SWAD-E http://www.w3.org/2001/sw/Europe ------------ WAI http://www.w3.org/WAI W3C, 2004 Route des Lucioles, 06902 Sophia Antipolis Cedex, France 
And what happens if you produce a second or third report. 
You need to control all the documents (the original, any existing EARL reports) to make this work. 
WHich is only sometimes the situation. 
CHeers Chaals 
SWAD-E http://www.w3.org/2001/sw/Europe ------------ WAI http://www.w3.org/WAI W3C, 2004 Route des Lucioles, 06902 Sophia Antipolis Cedex, France 
Thanks for the discussion. 
I need to focus back on the original scenario, which I didn't include in the original post. 
The questions I posted were asked because I was asked how would I know if/when the pages were tested and repaired. 
Independent of where the files were stored, for example in a "fixed" directory, I needed to leave a piece of meta data in the file itself that said the file/page had been tested and repaired. 
Many authoring tools leave a string of meta data saying which version of the product was used to edit or convert the file. 
I think this was done initially as a marketing tool to leave vendor product names lying around inside meta data in web files, but has also been useful in debugging tool specific problems by knowing the possible heredity of the file. 
So if the owner of the file wants to leave some indication that the file has been tested and validated, via some tool or process, what is the best way to specify that piece of meta data. 
I agree that most owners won't want to leave a trail to their dirty laundry (test reports), but in just the same way others recommend visible logo's on pages, I would like to leave a "good housekeeping seal of approval" somewhere in the file. 
Regards, Phill Jenkins IBM Research Division - Accessibility Center 
Well, there are a few approaches. 
The simplest is to know when the document was last updated - EARL requires that you keep a date of when the resource was tested (and assumes as far as I can tell that this is the date on which the document was read. 
So all you need to do is know that certain assertions are for a resource dated prior to the last change, and others have been validated or added since the last change. 
(If you follow the work that Nick and Jim did on hashing documents in various ways then in many cases you could determine whether a particular result you have needs to be retested or not.) An example scenario: I produce a document, and I do a complete evaluation against WCAG, which I publish as an annotea annotation on a private server. 
It find lots of faults. 
I repair the page, with my repair tool accessing the private server via my password information. 
It then publishes the results of P1 and P2 evaluations to http://earl.w3.org (a public server that is currently available, for people who like playing with alphas). 
It includes a link to the earl.w3.org data in a meta element. 
So the public know how to find the latest version of the data. 
But for my private use there is some more data, and some obsolete data. 
What do I do with that? 
For the Priority 3 checkpoints where the result hasn't changed, I replace the with the same result and the new date (i.e. later than the last-modified date of my document). 
Where the results have changed I post a reply with a type that says it obsoletes the thing it replies to. 
In this way it is possible to follow the thread for my own audit purposes. 
(For the cases that haven't changed I could do instead post a reply that simply reconfirms the existing result). 
For the copies of what is now public data on my private server I post a reply that obsoletes it, and that points to the public information as the now-current version. 
So I have now published a document and evaluation information, and a way to find it from the document. 
Let's say that Phill looks at my document and claims that my evaluation of 14.1 is wrong. 
What does he do? 
He could just write a page on his website and be done with it. 
Maybe I will find that, maybe not. 
Alternatively, he could post a reply to the annotation I put onto Marja could do a full conformance evaluation, including testing for triple-A conformance, and post the new results for P3 checkpoints to earl.w3.org. 
If he does this, the next time I edit the document, my authoring tool will go to my private server for information, and find pointers to some public information as well as some private information (the claims for P3 checkpoints). 
When it asks for information on my document from the public server, it can either ask for the things I said, or ask for all the relevant EARL assertions. 
I would like to take advantage of other people's work, so I ask for all the relevant claims. 
So I am now editing my document, and have all my evaluation information, plus some provided by third parties. 
(There may be more that other people have made available but which I haven't found. 
After all, I didn't use a semantic-search engine to see if there are other things on the Web, I just looked in places I knew already). 
I could decide to trust the latest result, irrespective of who made it. 
But in this case I think I have a better evaluation tool than the one Marja used. 
I do choose to trust all Phill's results as if they were my own. 
I ask fora report on conflicts between what I said and what Marja says. 
I can then select which of Marja's results I believe against my own, based on 
her commments. 
I am now ready to set about repairing the problems. 
But before I begin I decide to update the information in the document itself. 
So some of the results will refer to an old version of the document - in particular, Phill's claim that the content isn't clear enough. 
No problem. 
Each claim identifies not just the documentthat it is about, but the location of the problem itself. 
It turns out that I wasn't going to touch the pieces Phill said were particularly bady written, but I was going to change some of the things that Marja commented on - after all I had done my own private check of the P3 items and thought about how to make my page better already. 
After I have made the edits, my tools checks things the set of claims I already have to find out which ones are still known to be valid and which need re-testing. 
Then it follows up with the testing process (naturally this includes asking me about some results) to bring the results it has up to date. 
It posts all the results to my private annotation server, noting that they are replies to various different annotations, some that were public, some private, and dating them. 
I go away for the weekend, leaving the editing uncompleted, and the document unpublished (I am keeping a copy on my private server for editing), with the record of data on my private server. 
On Sunday night I miss my train, and decide to stay an extra day, working in the internet cafe. 
Now, if my editing tool is available via the Web, I can simply tell it to look for my private copy of the document, and the earl data I am keeping, and continue working, publishing my new version and my results as I did the first time I made some information available. 
I still keep my P3 thoughts to myself, but I reply to Marja's by noting that they refer to an old version of the document. 
(Not strictly necessary, but it was quick and easy, and Marja gets notified when her annotations get a response). 
As an alternative, I might use a different tool for editing - perhaps even one which doesn't understand accessibility or earl as a tracking mechanism. 
It simply edits the page. 
Again, when I get back to work, I can check whether the earl results I have are affected by changes in the page, and I can do the testing, repair, and publishing. 
What are the implications here? 
Beacuse I use a public server, people can post direct responses, which I can find and use or ignore. 
(Maybe its a semi-restricted server, where certain people can post, and certain people can read...). 
It was easy to find this because I pointed to it in the document. 
I could also have relied on people harvesting the semantic Web, as some people will point to a document by describing search terms to use in a search engine. 
This is more flexible and more powerful, but may be less reliable. 
I can keep information to myself as well. 
Nothing requires me to share my private data - I can acknowledge the annotations by someone else that I 
disagree with, or I can publicly post why I disagree, or I can ignore them 
completely. 
When I update my document I can post my new conflicting claims, or I can just note that the document has changed, or I can rely on people checking whether the evaluations are still relevant. 
A lot of this relies on the annotea protocol as the technology for storing the information. 
Although there are alternatives, I chose annotea because it is native-RDF, and compatible with a lot of the tools I will use to process ERAL itself - shorter development time because I am always working with RDF information). 
However not all these features are clearly implemented in the real world? 
Cheers Chaals 
SWAD-E http://www.w3.org/2001/sw/Europe ------------ WAI http://www.w3.org/WAI W3C, 2004 Route des Lucioles, 06902 Sophia Antipolis Cedex, France 
Yep. 
Well, that's only half the story. 
They could publish internal data with appropriate access restrictions, regardless of whether a URL to that data finds its way onto a public page. 
That's another approach, but I don't think it really adds much value. 
It adds an accessibility bar on the client side (which systems such as HyperNews don't), and it is only effective within a closed community who can all use the same Annotea database. 
(talking of which, I wonder whether Annotea might benefit from pinching anything from Hyper-G to work over multiple collaborating servers?) 
Indeed. 
I think the main use for this within an organisation is in the context of a QA process. 
If you require every page to have a link rel="accessibility" type="application/rdf+xml" href="..." - and perhaps integrate this with a CMS, you can enforce Good Practice. 
Nick Kew 
An example of using this would be to generate a Level 2 report report.rdf, 
and link to it from your page using link rel="accessibility" type="application/rdf+xml" href="report.rdf" . 
This tells agents such as search engines and QA tools that your page meets accessibility standards. 
Nick Kew 
Good work Chris Initial dilemmas. 
Re: All IMG elements must have an ALT attribute. 
In WCAG 1 I seem to remember an exception to this rule - were the alt 
tag had text that was all repeated in the text. 
Wendy - do you remember this one? 
Re IMG element cannot have ALT attribute value of null ("") if WIDTH attribute value and HEIGHT attribute value are both greater than 25. 
ID Number 4 In most case - but always? 
I think that is a general comment of mine, that I am not sure that there are no exceptions to the rules 
Maybe we should have more maybes? 
Same comment for assuming that a large picture is important (it could still be background and have a div with absolute positioning super imposed on top. 
(ID Number 5 ) 
This description I really did not get : IMG element must contain a LONGDESC attribute. 
ID Number 8 
Same idea for id:8 Any text in image is also in ALT text. 
- as a pass criteria Often, if there is a lot of text, and it is important it belongs in the long desc If there is a lot of text but it is repeated in the true text we do not need it at all. 
If there is a lot of text but it is vastly irrelevant, I would say forget it (for example a site on dyslexia may have the word "dyslexia" misspelled in list of different ways) 
ID 34: If FRAMESET element contains 3 or more FRAME elements then FRAMESET element must contain a LONGDESC attribute that is a valid URI. 
-isn't that only if the relationship is complex - otherwise a title on each frame will do 
ID 90: Does a script that just changes the shading need a no scripts? 
Etc Objects- is there such a thing as an object that is just visual - with 
out semantic content? 
If so then will a blank title do? 
Let's take an animation/object of an dot getting bigger and smaller. 
Does the text equivalent need to be larger then the words "dot getting big and small". 
do we need it at all? 
What if a description of the object follows in the text. 
Does an object described above need both a title and embedded text? 
Applet - Will embedded text do for an applet or do they _need_ an alt as well - And the same kind of issues as with object This is a great exercise in defining what we think. 
All the best Lisa Seeman Visit us at the UB Access website UB Access - Moving internet accessibility 
hi, 
in my opinion the answer to this question is no, the author does not need to provide a no script tag for such a script. 
my rationale is the analogy to images: if the script conveys _relevant_ information then the experience should be represented in text. 
by the way, in the case of mere shading, maybe the extension of the css pseudo classes and voice synthesis properties will provide a better solution to this problem in the near future but a NOSCRIPT tag would still not be necessary. 
i'm not sure what the correct way is to provide an alternative text for an object and whether the title attribute is actually the intended feature. 
as to the animated dot, it depends on the context. 
if the dot is the logo of the company or and advertisement campaign etc then it is relevant information and probably a short text would do the job. 
on the other hand, if the description of an object comes before the object tag itself and the description highlights the visually conveyed information in such a comprehensive way that an additional text would be redundant then i would agree that it should be omitted. 
but how often does that happen? 
would a false positive be a better approach for such a test? 
first of all, i believe the applet tag is deprecated and should be replaced rather than be fixed. 
secondly, i'm inclined to believe that the embedded text was intended to be sort of a NOSCRIPT feature. 
again, depending on the context sometimes the alt attribute will be enough and often it won't. 
i can't recall seeing any applets where an alt attribute was not required. 
regards, shadi 
Lisa, Thanks for your comments and questions. 
My replies... 
[1] I can't recall any exceptions to this rule. 
ALT can be null or spaces but it has to be there. 
[2] Not always. 
This is a potential problem. 
It's perfectly OK to have large images with null ALT but in some cases it's wrong. 
This may be a useful check for programs, or people, that insert empty placeholder ALT text. 
There's an important distinction between these 2 checks. 
The first check is for a known problem. 
The second check is a potential problem and requires human intervention to figure out it there's really something amiss. 
Ideally, all the checks would work perfectly and there would be no potential problems. 
Currently, more than half of the checks are for potential problems. 
Human intervention is still required but at least you're pointed in the right direction. 
The checks are better if they can detect with certainty the problem. 
If we add more "maybes" then it means the checks will get less use. 
People will just ignore them. 
[3] This is a check for a potential problem. 
Likely the image does not require a LONGDESC but you have to check for it. 
I'm thinking that this check would be better if there was a size restriction. 
If we keep asking people if their spacer gifs and bullets require a longdesc then the check will be ignored. 
[4] How do we determine if the relationship between frames is complex? 
I've guessed that this occurs when there are 3 or more frames. 
Can anyone provide a clearer definition of frame complexity? 
Or, when does a frameset require a longdesc? 
Chris [1] [2] [3] [4] 
I am not getting the er list mailings again - have I been bounced off? 
All the best Lisa Seeman Visit us at the UB Access website UB Access - Moving internet accessibility 
hi lisa, we read you! 
you're on the list and i wouldn't know why you would be bounced off... regards, shadi On Behalf Of Lisa Seeman I am not getting the er list mailings again - have I been bounced off? 
All the best Lisa Seeman Visit us at the UB Access website UB Access - Moving internet accessibility 
