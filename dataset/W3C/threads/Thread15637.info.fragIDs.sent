Hello everyone, Guideline 1.1 currently has an exception: If the non-text content is part of a test (for example, a spelling test) and providing a text-equivalent would defeat the purpose of the test, a text-equivalent would not be required. 
In my work, I have encountered another business case in which it is hard to meet the level 1 success criteria for this guideline, which we could fix by generalizing this exception. 
We work a lot for organizations that handle historical information. 
In this domain, more and more old documents get digitized. 
Some of the archives have expressed the wish to publish the digital documents or photographs on their websites, and ask the visitors to help them to make transcriptions of the documents (or describe the content of the photographs). 
Until the visitors have created a transcription or description, no text equivalent is available. 
I'm currently involved in a project that involves 500,000 poorly described images, where the public is asked to help improve the descriptions. 
With numbers such as these, simply telling them to fix the 
descriptions before they publish the content isn't an option because that would take several man years. 
How do we want to handle cases like this? 
It would be sad if archives had to choose between either complying to WCAG 2 or making historical information available through the internet (a service that is greatly appreciated by people with disabilities who cannot always go to the archives). 
My proposal (needs some word smithing) is to generalize the current exception for other situations than tests as well: If the purpose of the non-text content is to let the user provide the text equivalent, for example in a spelling test, a text-equivalent would not be required. 
This formulation would apply to the transcription business case as well. 
Yvette Hoitink CEO Heritas, Enschede, The Netherlands E-mail: y.p.hoitink@heritas.nl 
Off the list people have asked me why the organizations does not use OCR for this task. 
Because of the bad legibility, this is not possible. 
For a real world example look at http://www.dutchgenealogy.nl/test/document.jpg. 
This is a 16th century document which 99% of the people in the Netherlands would have trouble reading. 
Only obsessed genealogists like me can. 
I do not believe any OCR software will be able to transcribe this in the next 10 years. 
Especially since every clerk had their own handwriting and different areas in the Netherlands had different ways to write the letters. 
In tonight's telecon, a remark was made that a site which puts up these images without alt isn't accessible. 
I agree, it isn't accessible. 
One might argue that the documents are inaccessible to sighted people too, since 99% can't read what it says. 
So that's a level playing field :-) But what could the archive do to make these documents accessible at level 1 that isn't an undue burden? 
They rather spend their budget on digitalization than on transcriptions, because that's something the audience can help with. 
Shall we just forbid them to publish these documents if they want to meet level 1? 
That's against everything level 1 should be about. 
We don't even want to tell them they can't use black text on a black background at level one, after all. 
I hope to discuss this at a more appropriate time, when we are not so busy with the upcoming TR. 
Yvette Hoitink CEO Heritas, Enschede, The Netherlands E-mail: y.p.hoitink@heritas.nl 
A few comments: First, one of the unresolved issues surrounding the conformance scheme is the extent to which those making conformance claims should be free to define which parts of their content meet WCAG, either in written conformance claims or metadata. 
This is still on our long-term agenda to be resolved. 
Concerns have been expressed that conformance claims which encompass all of a Web site except the interface through which a transaction is to be completed, could be very confusing, as most of the content is accessible but the crucial aspect of it (the transaction completion) is not. 
Depending on what we decide, it might be possible to claim that all of the content at a site meets WCAG 2.0 except certain historical documents. 
Secondly, there have been issues raised about the value of WCAG 2.0 metadata as a descriptive tool, with which to inform users of what content is, or is not, likely to be accessible to them based on conformance/non-conformance with WCAG 2.0 at a minimum level, or with specific guidelines/levels that correspond to a user's particular needs. 
To the extent that policy-oriented exclusions are written into the guidelines, this diminishes their value as a means of reporting and classification. 
It has been sometimes suggested that different rules should apply to conformance logos and other directly human readable claims of conformance, on the one side, and metadata-based conformance claims on the other; though again, this has the potential to be misleading, a problem which must be balanced against possible advantages. 
Comments on any of the above issues are encouraged, on the mailing list or at the point at which they are placed on a meeting agenda. 
