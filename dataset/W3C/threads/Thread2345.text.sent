I am in the process of designing an HTTP 1.1 server which provides JPEG Images when a GET method with a certain URL is received, but am unsure about the use of entity tags. 
Should an entity of this type respond with an ETag field ? 
In turn, should I support If-Match/If-None-Match? If so ,How? 
Sorry , I sent this message to the wrong group....... 
I am in the process of designing an HTTP 1.1 server which provides JPEG Images when a GET method with a certain URL is received, but am unsure about the use of entity tags. 
Should an entity of this type respond with an ETag field ? 
In turn, should I support If-Match/If-None-Match? If so ,How? 
I am in the process of designing an HTTP 1.1 server which provides JPEG Images when a GET method with a certain URL is received, but am unsure about the use of entity tags. 
Should an entity of this type respond with an ETag field ? 
Generally, an HTTP/1.1 server would return Etag fields whenever there is a straightforward way of generating them. 
It has nothing to do with the precise data type; that is, it doesn't matter if the images are JPEG, GIF, MPEG, or drawings made up of ASCII characters. 
If your server does not have a way to generate an ETag field that meets the specification, then don't send one. 
In turn, should I support If-Match/If-None-Match? 
It's almost entirely pointless to send an ETag response header unless you also support the request headers that use entity tags (i.e., If-Match, If-None-Match, and If-Range if you support subranges). 
If so ,How? 
We tried to describe this in the specification. 
If there is some part of the specification (draft-ietf-http-v11-spec-07.txt) that is unclear to you, please describe the specific problem. 
The purpose of this mailing list is to produce a good specification, and we need to know if people are confused after they have read it carefully. 
-Jeff I'm having a moral dilemma with regards to my recommendation to the server team about what we be a good means for generating an ETag: Warning: Everything that follows is an implementor's issue, and not technically a protocol specification issue. 
Initially, I felt a hash of {the full pathname (to distinguish variants), the size of the file, and the OS-reported last-modification time} would be sufficient to be a strong validator. 
But, technically, that doesn't guarantee that a file that has changed twice in the same second has a different ETag value, which the spec says is a must. 
So now I have a dilemma. 
On NT, I might have an easy solution, as supposedly FILETIMES are stored as 100-nanosecond intervals. 
On UNIX do I have to use a W/ weak validator? 
I'm strongly concerned that every time someone aborts a GIF download, and returns to the containing page, smart browsers of the future will be using If-Match with Range GET's, and the Spyglass Server will be left out in the cold with its wimpy weak validators. 
Certainly reading in the file and doing an MD5 or checksum of its contents each time we serve it is out of the question for performance reasons. 
I would think with our file-based web server, numerous changes per second aren't as much of an issue as a database back-end that was generating content dynamically. 
I'm thinking that said dynamic application was the focus and intent of the restrictions on the strong validators, so maybe I can slide by? Here's everyone's chance to express an opinion. 
Daniel DuBois, Traveling Coderman http://www.spyglass.com/~ddubois/ 
A polar bear is a rectangular bear after a coordinate transform. 
Initially, I felt a hash of {the full pathname (to distinguish variants), the size of the file, and the OS-reported last-modification time} would be sufficient to be a strong validator. 
But, technically, that doesn't guarantee that a file that has changed twice in the same second has a different ETag value, which the spec says is a must. 
So now I have a dilemma. 
On NT, I might have an easy solution, as supposedly FILETIMES are stored as 100-nanosecond intervals. 
On UNIX do I have to use a W/ weak validator? 
I'm strongly concerned that every time someone aborts a GIF download, and returns to the containing page, smart browsers of the future will be using If-Match with Range GET's, and the Spyglass Server will be left out in the cold with its wimpy weak validators. 
First of all, on many UNIX systems, the file modification time actually has sub-second resolution. 
Here's a little test program: #include sys/types.h 
#include sys/stat.h 
main(int argc, char **argv) { struct stat statbuf; static char buf[1000]; sprintf(buf, "cat a.out  %s", argv[1]); system(buf); stat(argv[1], &amp;statbuf); printf("%d.%06d\n", 
statbuf.st_mtime, 
statbuf.st_spare2); 
system(buf); stat(argv[1], &amp;statbuf); printf("%d.%06d\n", 
statbuf.st_mtime, 
statbuf.st_spare2); 
If I compile this (as a.out) and run it on either ULTRIX or Digital UNIX, I get results like: % a.out foo 839976508.783384 
839976508.904470 
i.e., the st_spare2 field is really the tv_usec part of the file modification date. 
Now, I'll immediately admit that this is not true for all UNIX variants. 
For example, the version of the 4.4BSD code that someone left here a few years ago explicitly sets the st_spare2 field to zero. 
And even on DEC's UNIX systems, the resolution of this field is actually one clock tick (1 msec on Digital UNIX, 4 msec on ULTRIX) and so it is not 100% proof against collisions. 
But it's pretty hard to update a file, hand out a copy via the HTTP server, and update it again all in the space of 1 msec - our best SPECweb96 results so far show only 809 ops/sec. 
I'm not a POSIX expert, so I don't know if the POSIX spec for stat() says anything about st_spare2. 
Perhaps people blessed with UNIX systems from other major vendors, or other POSIX-compliant systems, can run my little test program and report what they find. 
(Please, not all at once!) More to the point, the requirement in the HTTP/1.1 spec for strong validators does not preclude the use of 1-second timestamp resolutions under the appropriate conditions. 
For example, if your server is integrated with some sort of authoring tool that simply delays its update if the new version is being written within a second of the old version, then you can make the appropriate guarantee. 
The guarantee is not based solely on the resolution of the timestamp; it's based on what the server knows about how the timestamp has been used. 
Even more useful, probably, is the fact that a file modification timestamp T of any resolution R is only "weak" during the period R+|E| after the file has been modified, where E is a bound on clock skew. 
To make this more specific, suppose that you know that (1) file F was modified at time T (2) the timestamp T has a resolution of R seconds (3) the clock used to generate the timestamp T is at most E seconds away from the clock used by the server to find out the current time. 
Then, between time T and time T+R+|E|, you would have to generate an entity tag of the form W/"encoding-of-T" but after time T+R+|E|, you could generate an entity tag of the form "encoding-of-T" because you are now sure that you have not given out that tag for a previous version of the resource. 
[The slight rub here is that the server might have to lock the file against modification between the time that it reads the timestamp and the time that it finishes reading the data, so that these two values are effectively read in one atomic operation. 
But this might be necessary in any case, to avoid sending out a chimera with pieces from two versions of the file.] 
The rules in section 13.3 state that these two entity tags cannot be treated as equal. 
This prevents any errors on GETs, and if a client does a conditional PUT using an entity tag, we've already insisted that this only be done with non-weak tags. 
The downside of this weak-and-then-strong approach is that if the file is changing rapidly *and* is being given out more frequently than it changes, then one may miss a lot of opportunity for optimizing If-Match and Range GETs. 
But then this is an incentive to use an operating system that minimizes T, and a system design that minimizes E, or makes E = 0 (e.g., don't have the server read its files via NFS unless you also use NTP). 
I would think with our file-based web server, numerous changes per second aren't as much of an issue as a database back-end that was generating content dynamically. 
I'm thinking that said dynamic application was the focus and intent of the restrictions on the strong validators, so maybe I can slide by? 
I think it's not that difficult to abide by the rules even for file-based servers. 
And it would be sad if we discovered after a while that we can't safely do conditional Range GETs, for example, because server implementors didn't take this seriously enough. 
-Jeff Hmm. 
I must admit, I never even *thought* of this. 
In implementing ETag support for Apache 1.2 (which should be released sometime before the turn of the century - I promise!), I used a hash of inode number, size and last-modified date, which amounts to the same thing as yours, and I didn't see anything wrong with calling it a strong validator, either. 
But... I dunno. 
What precisely are the odds that someone will modify a file more than once per second, and leave the file the exact same length? 
Pretty low, one would think. 
Actually, it brings up an interesting point: when matching ETags for ranges, one must use a strong validator comparison. 
But you can also use dates. 
Namely, this is perfectly valid: GET /filename HTTP/1.1 Range: bytes=30-500 If-Range: Thu, 15 Aug 1996 06:35:59 GMT Surely a date is a much weaker validator than the hashes that we are implementing? 
Yet as I read the spec, it is valid in this application. 
Agreed there. 
I would think that a database application would have to come up with a better way of making ETags, yes. 
But presumably that's something the database is capable of doing. 
For a file-based web server such as Spyglass Server or Apache, I think that we can certainly "slide by". 
But that's just my opinion. 
Alexei Kosut akosut@nueva.pvt.k12.ca.us 
The Apache HTTP Server URL: http://www.nueva.pvt.k12.ca.us/~akosut/ http://www.apache.org/ 
But... I dunno. 
What precisely are the odds that someone will modify a file more than once per second, and leave the file the exact same length? 
Pretty low, one would think. 
Danger! 
Danger! 
I think it would be foolish to make this assumption. 
Certainly the part about leaving the file length the same, since many frequently-modified files only change a few bytes. 
For example, imagine a stock-quote page where the quoted price of, say, EDS goes from "52 7/8" to "51 1/8". 
That doesn't change the length of the file at all. 
OK, this is bad example; nobody in their right heads would cache stock-quote values, but my point is that relying on file-length is a really bad idea. 
Actually, it brings up an interesting point: when matching ETags for ranges, one must use a strong validator comparison. 
But you can also use dates. 
Namely, this is perfectly valid: GET /filename HTTP/1.1 Range: bytes=30-500 If-Range: Thu, 15 Aug 1996 06:35:59 GMT Surely a date is a much weaker validator than the hashes that we are implementing? 
Yet as I read the spec, it is valid in this application. 
"Surely" not. 
If you are doing this comparison at a proxy cache, and you have the Date of the response as well as the Last-modified time, then 13.3.3 
explains rules for both the client and the proxy to follow so that there is no actual "weakness." 
And, if the server is actually able to determine that it not handed out the same last-modified time for two different versions of the resource, then you can also support your example at the origin server. 
When you write, of your example, "Namely, this is perfectly valid" you are only correct if you also know the Date: value for the cached responses, and if that Date value is sufficiently later than the Last-Modified value. 
Read the spec. 
Of course, if we discover that client implementors aren't reading section 13.3.3, then we'll have to prohibit servers from doing this. 
But 13.3.3 is cross-referenced from several places, so there is really no excuse for not reading it. 
-Jeff Isn't there some confusion about the issue of file modification dates as etags? 
The possibility that the file might be updated more quickly than the clock resolution doesn't matter, as long as the server never sends a date that is the same as 'now'. 
As long as the modification date is in the past, then you can use that date as a validator; even if the file gets modified 100 times in a nanosecond, it doesn't matter since the new times will be different than the time you sent before. 
Here's how to write a safe server: a) lock file from modifications b) check last modification date compared to current time c) if last modification date is the same as current time, then wait one clock tick d) send file with file modification date e) unlock file If you can't lock your files from modification while they're being sent, you're already in trouble. 
(What if it's updated in place while you're sending it??!?). 
The only alternative would be to copy the file somewhere else (into memory, for example): a) note modification date of file. 
b) copy file somewhere else. 
c) check that the file wasn't modified while you were copying it. 
If it was, go back to (a). 
(If you do this too many times, you might send an error message, since there's no reliable way to save a file that gets updated more quickly than you can copy it.) d) send copy of file with original modification date Just to put a real stake in the ground, I've received two *.dll-s from a vendor. 
The second includes fixes/changes from the first, but the length is identical (5xx120 bytes, I don't remember the xx). 
In this case, the only improbability is generation of two 500K dlls within 1 second ;-:) Bottom line, you can't assume the length won't change. 
Dave Isn't there some confusion about the issue of file modification dates as etags? 
The possibility that the file might be updated more quickly than the clock resolution doesn't matter, as long as the server never sends a date that is the same as 'now'. 
As long as the modification date is in the past, then you can use that date as a validator; even if the file gets modified 100 times in a nanosecond, it doesn't matter since the new times will be different than the time you sent before. 
Yes, this is correct. 
If the server is using the modification time to create Etags which are encodings of that timestamp, then it is possible for the server to tag them as "weak" or not based on your algorithm. 
That's why I wrote in this: a file modification timestamp T of any resolution R is only "weak" during the period R+|E| after the file has been modified, where E is a bound on clock skew. 
To make this more specific, suppose that you know that (1) file F was modified at time T (2) the timestamp T has a resolution of R seconds (3) the clock used to generate the timestamp T is at most E seconds away from the clock used by the server to find out the current time. 
Then, between time T and time T+R+|E|, you would have to generate an entity tag of the form W/"encoding-of-T" but after time T+R+|E|, you could generate an entity tag of the form "encoding-of-T" because you are now sure that you have not given out that tag for a previous version of the resource. 
see that URL for more details, including the issue of locking. 
But a separate question arose regarding the use of Last-Modified dates (i.e., the strings passed in a Last-Modified header, not the opaque strings passed in an Etag header) in context where a strong validator is required. 
And in that case, the server has no way to tag the Last-Modified value as weak or strong, so the rules in 13.3.3 
are (probably) the best that we can do. 
-Jeff 
