This discussion continues to be useful, if a bit theological in its flavour now and again. 
I'd like to address these issues: 1. 
Why namespaces anyhow 2. Web engineering perceptions 3. Validation, DTDs, and schemata 1. 
Why namespaces anyhow Here's the canonical scenario that I think we're trying to make possible. 
People want to stick together a chunk of XML that gives a PICS rating, another one that applies some Dewey Decimal classifications, another one that explains copyright status, pack them up with a chunk of content (HTML for now, but soon XML), and jam the whole shooting match down the wire in one blob. 
They want to do this ad-hoc. 
For one or more of these chunks, validity may be an issue. 
For most of them, there will be a requirement to map them to a formal schema, which may or not be a DTD. 
This compound blob will be well-formed but probably nobody cares if it's valid, as a whole, with respect to any particular DTD. 
The goal is that you can cook up a chunk of meta-content, stick it at the front/end of the document, and somehow not worry about GI collisions confusing a simple-minded app that is reading the transmission. 
Probably there will be multiple independent downstream apps, most of which care only about (a) the content or (b) one of the blocks of meta-content. 
[corollary: this scenario *presupposes* that most web metacontent is going to have XML syntax, one way or another. 
Smiles.] 2. Web community perceptions I mentioned in an earlier note that the people who are our customers for this facility are up to this point in time very hostile toward the idea that we do it all with AFs (er, fixed attributes). 
Said mention drew down a storm of abuse, and as a result the first draft of this note was very nasty. 
My best friend walked by the computer, glanced at the screen, and said "don't send that out tonight, OK?". 
So I've confined the nastiness to the following well-marked excursion: I don't think that it is a good idea to throw away what we know to be useful (DTDs) based on so-called "massive resistance" from people who don't know what they are talking about. 
Gimme a $#%!# break. 
Uh, how many pieces of DTD-driven software were downloaded voluntarily and used on a daily basis by 50 million people because they like it and they get useful work done without training? 
In fact, I gasp happen to think that some of these Web guys actually do, on a regular basis, know what they are talking about, when they are on the subject of the kind of software that people like to use and developers like to interface to. 
Fortunately for the AF partisans in this debate, people like Jean and Jon and myself are insulating the web engineers from the above kind of militant ignorance; which means that 8879 conformance, for the moment, still has a fighting chance. 
Sheesh. 
OK, let's try again. 
I and Jean (and I'm sure Jon at Sun and Dave at HP etc etc etc) are having a lot of trouble with really smart and really senior people who at the moment look at us like we're nuts when we try to convince them that Architectural Forms are a viable way to go, because they exist and are standardized, etc... especially because we *really* don't want them to go and look at the relevant standard, if they do we're toast. 
These people see what looks like a self-evident self-explanatory no-brainer syntax and say "why won't this work?", as opposed to ...much later in the document, or maybe even a different entity... Things they don't want to do include: 1) loading up every element with form attributes, or 2) figuring out, as they append successive chunks of XML, what things have to be surgically implanted in the internal subset to avoid problem #1, and 3) ensuring each little lightweight application includes a DTD syntax parser [yes, I know it's not that hard, I know first-hand, Lark processes !attlists from the internal subset precisely to support constructions like the above, but they DON'T WANT TO and are going to have to be given good solid reasons why they should]. 
What is the correct answer? 
Saying "we know what we're talking about and you don't" isn't good enough. 
At the moment, I have not heard a good enough argument to convince me that AFs are better for the purpose described above than the colon syntax. 
3. Validation, DTD's and schemata The discussion has suffered a bit from the assumption that when we say namespace or schema or whatever, this necessarily means we are tied to some DTD and we are worried about content models and the like. 
A lot of the schemata that people want to write are concerned with things that DTD's don't care about: data typing, domains and ranges of properties, inheritance of one kind or another. 
Yes DTDs are schemata. 
But the traditional SGML mind-set, that metadata means markup declarations, and that markup declarations means a DTD, and that the DTD must necessarily constrain the whole document from end to end, just doesn't apply any more. 
The concept of "validity" in XML isn't going to go away, but it's going to be subsumed into the much larger concept of "piecewise conformant to the rules for the schemata that apply to various document components". 
Lots of applications that are very stringent about what their data can contain just don't care about, for example, content models. 
Nobody is trying to *discard* DTD's; we need more validation, not less. 
It seems pretty clear that it's probably not realistic at this point to extend the 8879 syntax in 8 different directions at once to meet the needs of all the meta-content group requirements - so for now we're preaching that they should at least use XML, not S-expressions or tab-delimited files or whatever other proprietary weirdness they may have been considering. 
Perhaps in a future revision process DTDs will transmogrify, and son-of-8879 will have a much more ambitious world-view as to what constitutes a markup declaration. 
In the meantime, these guys are convinced that they need namespaces and they need them well-defined by Q4 '97, and we shouldn't tell them that they can't have them. 
-Tim Thank you Tim. 
I have been following the thread with great interest and find myself distracted at some of the rhetoric that has been posted. 
I hope everyone will respond to your posting as a cry for precision. 
Regarding validation I would like to understand better just what type(s) of correctness testing may be possible with the dc:name approach. 
As I see it, we can: 1. perform well-formed testing 2. validate...which portions would be able to be validated if only the "base" DTD is available? 
3. apply application specific tests to fragment content I suspect these are sufficient for the example that was given. 
Regards, Dave Hollander PS. 
This is so true. 
The web's success provides it the high ground. 
I must argue that the SGML lessons will help the web, not the web must help SGML. 
Since we are all moving toward a new technology, any point will lose if it is pressed based solely on the premise that the SGML community knows better. 
If that argument is used enough, the SGML foundation of XML may become suspect then challenged. 
Dave Hollander Hewlett-Packard Intranet Architect 3404 E. Harmony Road, MS. 6U68 TIS/WebCOE Fort Collins, Colorado 80525 dmh@corp.hp.com 970-229-3192 The presumption here is that SGML's concept of validation is not really applicable to these other fields (too strong or too weak) but that its instance syntax is fine. 
But we already know that that isn't the case from the arguments about empty end tags. 
I could go into dozens of other things I would like changed about SGML instance syntax to handle non-document data (some involving minimization, some involving semantics). 
Even SGML's DTD concept becomes horrifically verbose and difficult to read when it uses SGML's instance syntax. 
What is the point (technically) of foisting this inferior database-describing format on the database world? 
Is this all political? 
About unifying the whole world under our banner? 
About keeping SGML consultants employed advising database consultants about how to use SGML instead of comma delimited files? 
I'm not accusing anybody of cynically trying to misdirect the industry, but of committing the "I know alot about hammers..." mistake. 
We may achieve industry success -- "the only data modelling format you need", as HTML did as the "only DTD you need", but we will have failed in the sense of Doing the Right Thing in the way that HTML has failed. 
I, personally, would rather not make another HTML. 
If we want XML/SGML to be used across all fields, then we should rethink it from scratch. 
It should have an abstract syntax that can adapt to the job at hand (more abstract than SGML), a reorganization of concepts around more fundamental data-modelling principles and either a more general, powerful concept of validation or none at all. 
Paul Prescod Eliot has also been addressing these issues over in his work with the Medical people, and sent me the following, which at his request, I'm passing on to the WG. -T. Eliot from here down: I've been silent on this issue (except for one post presenting my AF usage proposal) largely because I've been too busy with getting the HyTime standard out and client work to think about this very difficult issue. 
Here is my feeling at the moment [Tim, feel free to share the following as you see fit, including on the WG]: 1. Architectural forms do in fact solve most (if not all) of the name space problems. 
They do it in what I think is a pretty elegant way considering that they depend on existing SGML methods and limitations. 
They offer a number of advantages, not least of which is that the whole architectural tower rising over a document can be completely ignored by unaware processors. 
Combined with subdocuments, most (but not all) of the requirements I've seen coming out of the name space discussion can be satisfied. 
I know this in my heart and mind. 
Convincing others that this contention is true is difficult and time consuming, and I don't fault anyone for being initially daunted by the abstact and formal nature of the mechanism. 
I've made it my life's work to educate the world about these technologies, which are, at their core, fairly simple and natural to use. 
2. There seems to be a legitimate requirement for direct schema munging of the sort reflected in most of the non-AF proposals I've seen. 
This has some qualities that AFs can't provide. 
It also has some drawbacks and limitations that AFs avoid. 
3. I agree with David Durrand that the initial negative reaction to AFs is short sighted. 
However, I don't think it's because the people having the reaction don't know what they're doing. 
I think it's because they haven't had 15 years to wrestle with the problem. 
They are, by and large, short-term thinkers, for good or ill, with short-term requirements that need to be met. 
Proposals with a lot of conceptual overhead require time for reflection, time they don't have. 
Applying what they know from other domains is a sensible and natural reaction. 
While we might disagree with their technical solutions, there's no profit in simply naysaying them out of hand. 
4. AFs and more direct name munging can be used together (the munged names are, after all, ultimately just element types, which can always be derived from architectures). 
I have a number of serious concerns with the various schema munging proposals, to the degree I've had time to think about them at all, in particular: 1. 
The requirement that all name spacing be exposed in the instance. 
I think this is ultimately self-defeating, because it builds in dependencies on document authors/generators that you don't have with AFs (at least with explicit DTDs). 
Semantic derivation can't be retrofitted onto documents without modifying the instances. 
2. Multiple levels of derivation all take up space in the instance. 
With AFs, multiple levels of derivation are visible only to architectural processors. 
3. Multiple inheritance is difficult or impossible. 
With architectures, multiple inheritance, both of the same element from forms in different architectures and of the same element from different forms in the same architecture, is possible (although this second case is non-obvious and a bit tricky, although inherent in the general architecture mechanism). 
4. Addressing of elements may be significantly complicated, because there may be multiple ID or entity name spaces within a single instance. 
Not impossible to manage, just a complicating factor. 
Note, however, that HyTime's general name-based addressing facility is perfectly capable of dealing with such name spaces because the mechanism for defining name spaces is completely generalized. 
However, simple ID/IDREF may no longer be sufficient. 
5. It seriously confuses the syntax/semantic issues that SGML (and HyTime) are careful to keep separate, primarily through the use of subdocuments to construct compound documents. 
However, from my experience in evaluating the Japanese proposal to WG8, it appears that all of the above concerns can be addressed by reasonable technical approaches. 
Therefore, it seems likely that a useful namespace approach that is consistent with existing SGML designs and design principles can be developed and probably should be. 
I'm certainly not competent to accept or reject any proposal at the moment. 
I don't think there's been nearly enough thought or experimentation yet and seriously doubt the ability to define a mechanism by YE97 that will stand the test of time. 
However, I wouldn't impede the development of approaches that didn't require egregious violation of SGML syntax rules--obviously the experimentation has to happen and I don't see why we shouldn't let the people with the requirements do it. 
I'm not so arrogant as to think that the SGML world can provide all the answers to these issues and I'm certainly willing to listen to and learn from people outside our little world. 
I know that advances in any well-entrenched discipline must come from outside. 
As for the issue of medical records, I just don't know the answer. 
My personal feeling is that we should explore the architectural approach until we break it. 
In the mean time, other name space approaches will have had time to mature. 
We should carefully capture our name space requirements so that we can inject them into the discussion in the most productive form possible (i.e., devoid of implementation suggestions). 
My confidence in the architectural mechanism makes me think that, as the name space issue is explored in more depth, people with both come to appreciate the benefits of architectures and better see the limitations of name munging approaches. 
I say, lead by example. 
If you build it, they will come.... Cheers, E. I'd like to make a comment about #2 above. 
Suppose I shoot you the following well-formed XML document: p Some text that contains a mathematical formula here: written using the a href="www.aap.org/dtds/math.dtd" 
AAP math DTD /a . 
/p [I made up the aap.org URL, but the formula markup is valid AAP.] Now the point is that mydoc is well-formed, but can't be valid because there is no DTD. 
But there is a DTD for the subtree defined by the f element, and it might very well be important that that subtree does validate against its DTD (because my equation processor requires only valid AAP-marked up input). 
What you can have are what I call "islands of validity" in a well-formed document. 
Tangent This leads to another interesting (to me) concept, though I'm not sure of its usefulness: suppose you have a document instance that "looks like" a 12083 article consisting of two chapters, but the second chapter--while well-formed--has sect3's containing sect2's and title's containing paragraphs. 
Also, the document is missing the required front matter, so the overall document is not valid. 
But chapter one *is* valid when compared to the DTD. 
Now suppose you define two namespaces: one of which is the ISO 12083 article DTD and the other of which is a special DTD that consists of declarations for all the elements in 12083 but where each element's content model is ANY. 
Now, if you attach the ANYDTD namespace to the overall document instance and attach the 12083 namespace to chapter one, you have a completely validatable document. 
(If you were a smart editing tool trying to fix a well-formed but invalid document, you could start by giving the document an overall "ANY" namespace and then switch the namespace of subtrees to the real DTD as each subtree becomes valid. 
When there are no more elements in the ANY namespace, you're done fixing the document.) /Tangent paul Summary: Tim makes some good points, and addresses my concerns (without fully allaying them). 
I ask a lot of questions to try to find out what's really up. 
I at least see some signs that there may be a justification for jumping the gun on standardizing new stuff. 
I'd also like to note that if the XML project had been taking a more radical approach to redefining SGML I would be more positive about the current debate, but the conservative design of the current language makes me less willing to take chances, especially at the last minute. 
OK, as I have said, this ammounts to either a loophole in the notion of validity, or a case where it needs to be extended. 
One of the reasons that I have been a bit nasty is that no-one has just admitted this up front. 
We are talking about a way to add stuff to instances without worrying about a DTD. 
I am a bit dubious about this assumption... Especially since we _need_ a notion of validity or at least ignorability that will let simple minded applications work. 
This sounds like a good application of MIME/multipart, but that's got a lot of unnatractive overhead too, so I'm still listening. 
This is good as long as I can count on XML syntax to actually be a meaningful notion... Maybe meta-data in XML is worth a jump into the unknown. 
I'm not on the ERB, so I don't make that decision, but if that's the argument, let's not deny that we're doing it. 
And let's hear why having all meta-data in XML is a wonderful idea. 
Well, as I said, the content of one of my objections seems to be true -- we want an escape hatch from DTDs, and we _are_ throwing out, or at least radically extending existing SGML practice. 
Having one's points acknowledged (if not acceded to) goes a way to reducing overall spleen level. 
Given the performances that I've seen by some web engineers on this list, and on the net generally, you must admit that I am not without a point. 
I obviously did not mean, and don't think, that all the people associated with the web are idiots, but there's clearly a gap in cultures, and a lot less esperience with markup systems and the good and bad points of DTDs on the web side. 
HTML has not been an unalloyed success, though success it certainly is. 
After living with the HyTime document I agree that it should be kept under wraps. 
I have only advocated the use of #FIXED attributes now because I worry that we will standardize something that will not work in the long run. 
I'm not opposed to the idea of namespaces, but the last minute rush is very worrisome, and the reasons behind that rush are _not_ being clearly presented to this group -- nor are the requirements and technical tradeoffs being presented. 
Given the choice between something that is workable but clumsy and invokes no new mechanisms and a pig in a poke, I then to go for clumsy... 
This doesn't sound like it's that hard, actually, but I can see the point of view clearly enough. 
In fact, the parsers make the case for element syntax much better. 
That was not the argument that I'm making. 
I'm saying that if AFs are a solution _at all_ then we should be careful about standardizing a new, only partially understoo mechanism in a big hurry. 
If we don't get the semantics of the colon syntax right, then we've tied a long-term millstone around XML's neck. 
I'm saying that we're talking about standardizing something that may be _much_ less good that it should be because we don't understand it well at the moment. 
The stakes may be worth it, but I haven't seen a refutation of the adequacy point. 
That's why I'm taking a conservative line here. 
I can believe that for political reasons it is imperative that we choose a new mechanism because it's impossible to sell the one that we have -- but if that's the case, I'd rather see it up front. 
Maybe all meta-data in XML is so important to the future of XML that a chance taken is worth it. 
I'm partly convinced that it might be, but only partly at the moment. 
I'm also worried about the inverse -- do namespaces force my DTD-loving application to deal with arbitrary foreign-namespace gunk that I can't anticipate? 
If DSIGs don't want to impose DTD stuff, that's fine by me, but if I want to fix a DTD, will namespaces force me into writing code to deal with "unexpected markup". 
This is a big step back from the reasons I usually want to write a DTD. 
Well, if we change the notion of validity, we are sure making a big change in the infrastructure at the last minute. 
I'm more concerned that if DTDs become optional, replaced by informal tag-salad specifications for diverse namespaces that we will be worse off than with HTML, because now it won't even be easy to format the sloppy tagging that will come in. 
I just want to know how I get a DTD that describes my document when the server jams a DSIG in there... 
Or be told straight to my face why not being able to get that is such a good idea. 
I certainly agree -- though we may prefer that once we understand the requirements, and their implications for SGML. 
I can see this as a fine strategy. 
Why can't the use the "." character and a suitable naming convention to uniqify tags for use with XML 1.0 and leave extending the language for later. 
This is essentially the : proposal, but avoids making the claim that we're adding a feature, and thus perhaps tying our hands once we know what we really want. 
I can believe that conviction, but by itself it's just an assertion. 
I feel that I'm being asked to comment on something where the requirements are not in the open. 
And that frustration may be making me a bit nasty... Are the requirements (singly or in combination): 1. attachment of abstract semantics 2. unquification of names 3. attachment of unique names to short names in an instance 4. parsability by a one-pass parser 5. generatability on the fly with no discontinuous dependencies that would require buffering attlist declarations and the instance in progress while you do generation. 
6. Conformance to notational prejudices of the engineers involved. 
7. ??? ... And are the payoffs/risks: 1. XML if not used for meta-data will not be included in commercial web-browser software (eg MS, NS). 2. XML if not used for meta-data will be in browser software, but not anyplace else. 3. XML, if lacking namespaces, will be rejected by the W3C as failing to meet their needs. 
4. ??? ... I think there is a potential case that XML for meta-data will enhance its acceptance so that it's worth the risk of adding semantic or syntactic conventions. 
But I would like to see the case. 
-- David David Durand dgd@cs.bu.edu \ david@dynamicDiagrams.com 
Boston University Computer Science \ Sr. Analyst --------------------------------------------\ http://dynamicDiagrams.com/ MAPA: mapping for the WWW \__________________________ "islands of validity": An interesting concept. 
I have often observed that in the past I have tried to impose validity on entire corpus when there was only a small part that I was really interested in being valid, as extended beyond the well-formed requirements. 
Indeed, when I look at html and validation, it really is only a small portion of our web pages that I am really concerned about, specifically forms and meta-data, the rest I just want to parse cleanly. 
What about the other side of the coin, if there is a DTD for mydoc, but not for formula, can you have a valid document with an opaque bubble? 
Is there any way to make this a SGML valid concept? 
If the mydoc DTD had formula as any would be be ok? 
How is validation with ANYDTD interesting? 
Only for the smart editor? 
Dave Dave Hollander Hewlett-Packard Intranet Architect 3404 E. Harmony Road, MS. 6U68 TIS/WebCOE Fort Collins, Colorado 80525 dmh@corp.hp.com 970-229-3192 The overall document would be valid if either: 1. the mydoc DTD had an ANY declaration for formula *and* all the elements that could be inside forumula, or 2. if formula changed its content's namespace to some other DTD and that DTD had an ANY declaration for all the elements. 
If you want validity, you do have to have a declaration for every element. 
An ANY DTD is basically an 8879-compatible way to give a list of elements names that can be combined in any well-formed way. 
Validation against such a DTD is effectively equivalent to just validating against a list of allowable element names. 
As such, it's only 'interesting' in that it is as close to well-formed as you can get and still be able to validate. 
(And by the way, it's nice that this feels like a discourse again, rather than a barroom brawl). 
To be fair, in my posting of May 19 entitled 'Re: SD4 - Schema Format [fmt]' I find the following: The new types of schema that are being proposed include a lot of things that are not covered by DTDs. 
Thus we have three choices: 1. use XML syntax 2. radically extend DTD syntax 3. invent a new syntax Of these, the first seems desirable, the second questionable, and the third idiotic. 
That was a whole month ago... any older and it'd be in Cuneiform. 
I agree that this does seem related to the problem that MIME/multipart is trying to solve, and also agree that that's probably not the right path for us. 
That decision is not really on the whim of the ERB. 
We are a creature of the consortium, i.e. mostly the vendors and information providers; said vendors &amp; providers have looked at XML and said "document delivery yes, when the browsers evolve a bit, but metadata right now!" 
This process must react to demand from the W3C base - right now the demand is for metadata. 
Now, we are not required to go along slavishly with the flavor du jour, but the metadata application seems like a good one and I at least see no reason to resist. 
BTW, if you do an "XML" search on DejaNews, you'll see there's a big XML brouhaha going on over in EDI-land. 
So far, they haven't come in asking for changes... 
As for why metadata in XML is a good idea, if it was just XML syntax that alone would be good; cheap parsers, and fewer syntaxes is always better than many. 
But I at least want more - check out the recent MCF proposal that Guha and I cooked up (or better, wait until next week when we'll have a redraft taking into account early reviews) - we think it's possible for lots of radically different metadata to share not only a syntax but a deep underlying data model and big chunks of vocabulary, the benefits of which will probably not be lost on this group. 
I agree it's a rush, but then everything about XML has been done in a rush. 
So far, it's worked pretty well. 
About the requirements, I can only disagree; I thought that Layman's original write-up was pretty good; I took it on myself to poke around with my contacts, and other ERB &amp; I assume WG folk have been doing the same, and the picture seems pretty consistent. 
People want a wide variety of schemata (including but not limited to DTDs) and they want to bundle chunks governed by different ones together in a delivery unit. 
That's about all there is to it. 
This concern is highly valid. 
There is indeed a chance of going off the rails; and I think if there's some fatal flaw in the colon-prefix approach, this group is about the most highly qualified in the world to spot it. 
Looks like a win to me, but that's a judgement call. 
Uh, may I point out that the world is not at the moment exactly bursting with commercially successful architecture-driven applications? 
We're taking a bet either way. 
This is a real problem. 
I think that the chances of finding a traditional SGML covers-the-whole-class-of-documents centrally-maintained laboriously-updated DTD that can parse everything that's going to come over the wire are pretty poor. 
Now I could indulge in 8879 sophistry and say that what comes over the wire is just a collection of individual entities, and as long as you can validate the part you care about, it's the task of some entity manager thingie to pull that part out and treat it as the document entity and the others as SUBDOCS or NDATA or whatever. 
But I won't; I just think that the DTD is going to have to take its place as one of the schemata that can be used to validate certain portions of blobs of data coming over the wire. 
Yes, it's different from what we have now. 
That's a fair concern; I don't share it, because well-formedness helps quite a bit, and as I keep saying, people *like* the concept of validation and of stringent schemata - they just want different things than 8879 provides. 
That was actually the initial proposal from Microsoft; I suggested adding the ':' just because lots of people already use '.' in GIs. 
I think this is a good fair summary. 
Although I would change "one-pass" to "simple and lightweight". 
I think SP is one-pass, and we'd like to be able to use simpler technology. 
Also for #5, it's worse than that, because different chunks of metadata will be poured in by independent modules, so each would have to read the subset to see what's already there, munge it in nontrivial ways, and write it back out. 
And I would put "(perhaps well-founded)" in front of "notational" No. XML *will* be used for metadata. 
Hence, because of the nature of metadata, XML *will* have namespace/schemma wrangling machinery. 
We couldn't stop that now if we tried. 
That's why we're spending time on this. - Tim That was precisely the argument Henry Ford gave his managers for sticking to the Model-T. 
It was wildly successful, everyone had one, cheap, easy to fix, and who needed anything other than black. 
Call it rhetoric if you like. 
It is history. 
GM took over the leadership in the market with a component-based design to stratified markets of buyers. 
Ford Motor Company never recovered its dominance. 
Henry took to shooting at his workers. 
The SGML community knows SGML. 
That made HTML trivial to learn and use. 
It is also the reason we are here trying to improve the potential for markup applications on the Internet. 
We defend tools and approaches which we understand, use and use well and have for a number of years before the Web. 
We understand the Web as well. 
It is HTTP: the poor man's protocol which for the most part, is a DEC 9600 tape format a la 1840 put in a wire with two methods and MIME: types by file extensions. 
Wow. 
If the only argument being presented was that "we know better", you might have a point, but it isn't. 
The SGML foundation of XML is why many of us agreed to work for free at night and in our spare hours on this worthy project. 
To eliminate that reason is to also betray the trust of this working group to those whose names do not even appear on the membership list. 
That is without honor. 
This discussion will lead to schism. 
One group of consultants will build something XML-like for one of the two browser vendors, and the other will try to fix SGML. 
XML without the legitimacy of SGML will fail. 
SGML without a subset for the Internet will yield to PDF. 
I think we can do better. 
Len Bullard Well, then *this* is the fundamental requirement, then, isn't it? 
I buy David Durand's plea for a conservative approch to the namespaces issue. 
Wouldn't it be possible to enable the ":" to be added to XML names, and then enable namespaces themselves at the "application specific instructions level"-equivalent in XML, which I would take to be a set of processing instructions in the Misc section of the Prolog? 
That way, the Q4 guys are happy, experimentation with namespaces can proceed apace, and any failures wouldn't bring XML or SGML down. 
(Actually, the worse case scenario in my mind is where XML is *not* SGML -- but is marketed as if it were....) It certainly appears so -- an essentially external process has deemed a protean notion yclept "namespace" to be a Good Thing To Have On Hand ASAP. 
The issue, then, is the syntactic machinery to permit this. 
I'll add my Me Too. 
Assuming that *some* syntactic device will have to be invented, I would prefer that it (a) be least invasive in terms of the appropriate changes in SGML-bis, (b) not be *limited* to 97q4 "namespaces" in its overt syntactic function. 
In fact, the syntactic requirements appear to be relatively orthogonal to the whole business of why namespaces are so urgent. 
If it were just a matter of associating a "wider context" to an element -- typically for semantic purposes -- the AF solution via attributes *is* an answer. 
This can't be the problem. 
Rather, the problem seems to be how to "uniquify" GIs in contexts where name clashes can't be ruled out. 
Quite apart from the serious validation issues raised (Paul Grosso's AAP example comes to mind), there's also the chance that the app downstream from the parser, i.e. the "semantic engine", will get confused too. 
Hey, waitaminnit. 
Preventing that is supposed to be a raison d'etre of SGML... So, what are we *really* talking about here when we say that "name clashes can't be ruled out"? 
IOW, *why* can't they ruled out? 
It appears that the Canonical Problem is not inclusion of data from multiple domains. 
It's such inclusion *on an ad hoc basis*; this is what forces the need for syntactic distinguishability in the instance. 
We're talking about a syntactically explicit Cut'N'Paste mechanism. 
It may help, then, to work backwards from a standard SGML answer to this, notations and (external) entities, and cast this as what happens when you inline such a notated entity. 
Clearly, the requirement is to preserve the notation information: the data content of the entity will still need to be in a portable or transferable form (on General Principles: some day we'll need the external reference mechanism anyway, and then it will do no good if the actual data content has to assume different syntactic forms depending on where it gets to be plunked.) The argument against the AF answer is that the naming attribute even when included isn't distinguishable in the instance: it requires extra DTD machinery to work. 
In my own long-winded way, I've arrived at the point where I believe the three kinds of (nominally sans-DTD) proposals can be understood in terms of their different approaches to distinguishability: 1. Lexical - add a character to the set of name characters and use it as a name-compounder. 
HTML:A, TEILITE:NOTE, etc. 2. Syntactic - use a PI to stuff the disambiguating information. 
3. Structural - use a newfangled marked section as an explicit scoping device. 
I would argue against (1) on the grounds that (a) it is overkill when the problem context is (or rates to be) essentially ad hoc in its incidence, (b) unnecessarily verbose, if not goofy, when the content might need to be reusable as an external entity (indeed, why couldn't it have started out that way?), and (c) it doesn't scale to the situation where multiple domains/namespaces/whathaveyous might need to be encoded (cf. 
two or more AF attributes.) I have no strong argument against (2). 
It works like a DTD in absentia, in that the information has to be separately parsed *and* buffered while the data content is parsed. 
That is, it's not syntactically (more accurately, lexically) explicit in its scope; we need a full parse even to start. 
Nevertheless, my push-come-to-shove preference is for something like (3). 
(a) the scope of ![ .... ]] is lexically distinct in a reasonably opaque fashion (sort of like figuring out the extent of an IGNORE MS), (b) like a PI, we keep the "notation" information separate from the actual data content, and (c) also like a PI, it focuses on a general purpose syntactic mechanism that can be specialized for the needs of namespaces. 
Currently, only status keywords are allowed between the DSOs. 
I would propose a variant on Henry Thompson's proposal with syntax like this: Assuming PEs aren't dead in the water, hiding the namegroup also becomes possible (and in extreme cases, the PE might be redefinable to CDATA and the buck of parsing the content passed to the application, which could invoke another parser instance ... hey, ad hoc come, ad hoc go.) Indeed, the entire marked section can also be stuffed into an entity declaration if need be. 
Name-munging certainly looks like an easy way out. 
But it smacks too much of forcefitting a solution whose essential appeal derives from a different paradigm (C++?) Sure, the programmers will grok it and love it. 
But it messes with the content (the need to "resolve" GIs gives them a data quality beyond their markup function) when what we need is just markup. 
Perhaps, but any solution that won't scale and won't work with other mechanisms such as declarations for external entities and notations rates to be penny wise and pound foolish. 
IMHO, of course. 
Arjun In SGML terms, what you are asking for is, I think, * inline SUBDOCs, * tagged using "formal marked sections". 
I disagree. 
Making identifiers meaningful to computers is a good thing. 
Rick Jelliffe Not only that, but why must the metadata be in a single file? 
If we had subdocs I would suggest that there is a strong case for metadata being incorporated as subdocument entities - this is really what people need: a means of associating structured metadata with a file which does not conform to the same structure as the data in the file. 
Why invent a convoluted and restricted mechanism for doing something which is easy to do in SGML already? 
(DSIGS that need checksums might be a problem here though!) Tim says the deadline is Q4 of 1997: this makes much more sense than July 1st. 
7. Ability to build compound documents (with and without validation) Martin Bryan, 29 Oldbury Orchard, Churchdown, Glos. 
GL3 2PU, UK For details about The SGML Centre contact http://www.sgml.u-net.com/ 
For details about the Open Information Interchange initiative contact I suppose so, except I was really thinking in terms of the MODULE proposal and Paul Grosso's example (that MODULE could address at the DTD level): the point was an inline alternative syntax for DTD-challenged situations. 
I like that name:-) Perhaps this can be argued as a natural direction in which to extend MS syntax. 
CDATA/RCDATA MSs basically warn the parser that normal parsing rules don't apply, in particular that what may look like tags really aren't. 
OK, so extend that notion to offer some form of lexical scoping: what look like tags aren't tags for the "current" validation/parsing context -- refer to some NOTATION declaration instead, etc. etc. 
That is, generalize "status keyword specification" to something like "context treatment specification". 
I think I said what you just said; I must have chosen my words poorly. 
The issue, I think, is how to make identifiers meaningful to computers. 
I'm still not convinced that delimiter-based compound names in instances is the way to go. 
Arjun From: Arjun Ray aray@q2.net 
WG8 is ahead of you! 
(Why is W3C so slow??? :-) Both "formal marked sections" and "formal processing instructions" were in the drafts for the WebSGML TC. 
The idea is just what you say: that they both should start with NOTATION identifiers. 
They were taken out at the last minute, I think because WG8 did not sense any groundswell of understanding or support for them at that stage, and did not want the WebSGML TC to be derailed. 
If you want some details, see under "Extension to FORMAL". 
I hope that they may be considered as part of the Namespace TC that we are working on now. 
And I hope that XML will adopt the formal PI proposal anyway. 
All XML needs to say is "The first token in a processing instruction is a notation identifier, which should be unique to avoid name clashes". 
Everything must be labelled with its type/notation. 
If a thing cannot self-label, the things that link to it must be able to label. 
And this includes external entities/resources, prolog entities/resources, and inline entities/resources. 
Rick Jelliffe 
