Hello all, first welcome to these two lists. 
(they are both archived off http://lists.w3.org/Archives/Public) - WG (Working Group) is for active participation in design, coding, and writing specification. 
- IG (Interest Group) is for following the activity in terms of meeting agenda, presentations being made, results, but also exchange ideas, and give input to the working group. 
I'll post a more detailed charter for both groups later on. 
The purpose of this message is to give you the agenda for the first WG meeting in Sophia-Antipolis, May 22. 
After some thinking, I decided to remove the sessions on guidelines from this first meeting, partly because Gregg Vanderheiden is not going to be there and also because we're short in time as is and lastly because it's logical: first things first, let's put the effort in the technology and then we can work out the msg about the technology (but since Mike Paciello is going to be there, we will take the opportunity to talk a little about education/awareness process over lunch). 
It's at Mike Paciello told me that it might be possible to participate in this meeting remotely (teleconference?). 
Is this true, and how does one participate? 
I am especially interested in discussing text-to-speech standards and interfaces. 
Jim Fruchterman jim@arkenstone.org 
President Arkenstone, Inc. 555 Oakmead Parkway 1-800-444-4443 Sunnyvale, CA 94086 USA 1-408-245-5900 From: jim@arkenstone.org 
I am especially interested in discussing text-to-speech standards and interfaces. 
Jim, is there anything up on the web that would pass for a situation survey? 
Something about what is in practice in the field, and possible opportunities to improve things by defining standards? 
For example, lots of Lynx users have screen readers that don't even distinguish the flavors of highlighted text that the VT-100 is capable of. 
This is a real drag in composing accessible HTML. 
For that sort of a screen reader, I basically have to write Gopher screens. 
I would prefer hypertext, i.e. links embedded in naturally flowing prose with just enough hint as you drive over the hot spots. 
Al Gilman I'm not sure, because there are several directions to explore. 
1. 
What is the current state of the art in text-to-speech technology? 
This is not as nice as people would like, because the humanlike quality falls short of what is desired. 
Our speech synthesizer interface library ("SSIL"), the leading standard used in applications developed for the blind, is a good example of the state of the art five years ago. 
Unfortunately, the state of the art in TTS hasn't changed much except for the need to live in 32 bit environments! 
2. What should we plan for in the future? 
My impression from reading the briefing package is that we're discussing aiming quite high, trying to mark up material with very rich information. 
The question also exists of where the primary responsibility for natural speech output lies: in the document creator or the output device. 
Are we aiming for the least common denominator browser client in content or a richer one? 
The answer is usually yes, both. 
As someone who is mainly interested in developing talking client applications, I'd like to be able to deliver all of what is possible. 
However, that's a high bar! 3. What should a talking browser do in handling different kinds of objects? 
Some applications call out different objects with a "LINK" statement. 
I agree with you that a slight change in the TTS would be desirable for some users. 
I've done an application for reading scanned material aloud that has five different voices (reading, menu, bold, italic and underline), and think that this would be a valuable way to convey information. 
I think the underlying information should be there and each user make the choice of how it is conveyed. 
I'm just feeling my way in this group, trying to figure out where we're going technically. 
Jim Fruchterman jim@arkenstone.org 
President Arkenstone, Inc. 555 Oakmead Parkway 1-800-444-4443 Sunnyvale, CA 94086 USA 1-408-245-5900 
