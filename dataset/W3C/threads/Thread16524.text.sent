How would one go about providing alternative content for maps? 
Yahoo Maps does a good job at providing text directions alongside a visual map. 
However, directions from A to B are not the same as an entire map. 
Some map alternatives that I know about are: -Topographic maps with actual rises and falls in the material. 
These are expensive, and I don't know that they would provide necessary useful information to a blind user. 
And of course, this isn't an electronicly transferable media (tho I could imagine braile displays conveying something like this). 
-Descriptions of regions. 
These mostly occur in literature - writing them is an art, and I've been able to get very accurate information from them. 
But I don't think web developers can be expected to write these. 
Thanks for your input, Benjamin J. Simpson Education Associate, Web Development Group NASA Ames Research Center (650) 604-3292 This is a very interesting question. 
I think, though, that we should keep in mind that the -map- is rarely the content we are conveying; instead it is the -information on the map-. 
So when we consider the options, we need to focus on conveying the content of the map, not the form. 
You could print off, with a braille printer, a crude map -- I've seen a map of the US, for example, at CSUN printed in this way. 
But it may not be the best way to convey the info in the map. 
What is the map conveying now, and how? 
(Rhetorical question.) 
That could provide some clues. 
Kynn Bartlett kynn@idyllmtn.com 
You're at Ames.? Have you met the Moose?? 
This is a haptic mouse that one of the programmers at Project Archimedes at Stanford uses to good effect.? There is the hypothesis that one could present map information with the outline graphics sensible via haptics and the cities, mileages, etc. textual annotations presented by coordinated speech or Braille. 
With the advent of a commercial haptic mouse under $100, this gets increasingly interesting. 
There is also a new attempt at commercializing a tactile mouse -- sort of an Optacon on a puck -- that was showing at CSUN this year.? 
This can give you the illusion of a dynamic and electronically sharable Braille Graphic region. 
Maps are certainly an area where it is desirable to generate some capability for presenting at least rough line-drawn figures and perhaps a bit of texture in the plane under the user's touch or muscular feel.? And use that as a base for verbal annotations at key points. 
Al them is But I AG:: "There is the hypothesis that one could present map information with the outline graphics sensible via haptics..." WL: The hypotheses in the area of "help the blind" technologies are (and have been for my decades in the "industry") legion. 
This one is very popular among: 1) sighted engineers for whom this is an intuitively obvious solution; 2) grant seekers who recognize the "sexy" nature of high-tech gadgetry; 3) granting agencies who've seen ink/airtime devoted to publicizing awards to test the hypotheses; 4) entrepreneurs who can raise money to build/exhibit/discuss said gadgets; 5) blind people who although they have yet to find much use for any of these things don't want to discourage people who are "only trying to help"; and lastly 6) an old geezer who delights in puncturing dream balloons. 
The most fortuitous outcome of all this is that some blind guys get token employment, usually as subjects. 
Hardly a year has passed without yet another attempt at a "vision substitution system" involving haptic display of manipulated output of video (in some cases the display is auditory instead). 
All of these efforts seem to this cynic to be based on the notion of a "cure" for blindness which is traceable (pun intended) to at least Biblical times but as Mike May's experience makes clear, the "real" cure (restoration of some vision) isn't as important as sighted (blindless) folks posit. 
The big advantage of working for accessibility to both the built environment and the World Wide Web is that it actually produces usable results. 
As a recent press release (showing one U.S. presidential candidate's Web site failing Bobby/WAVE tests while the other's passes) indicates, our work is reaching its intended goal - inclusion has become fashionably PC. 
As I visited the sites Al mentioned (CSUN had a few exhibits dealing with feelie mice) with blind pals (all right - Bill Gerrey, Jason White, and Gregory Rosmaita) all of this was brought resoundingly home once more - for about the twentieth time. 
The devices are briefly amusing and the idea behind them might even be provocative, but... Entire careers have been based on these things yet we *never* see folks using the gadgets (possibly [probably?] 
excepting the OptaCon) on which the millions were spent. 
Sticks and dogs for mobility, a handful of Talking Signs for orientation, and LOTS of screen readers and braille displays for accessibility. 
The latter's effectiveness as we move into a "new economy" depends to some measure on our efforts to improve accessibility and we must keep on keeping on with our eye on the donut, not on the hole. 
Love. 
ACCESSIBILITY IS RIGHT - NOT PRIVILEGE you are absolutely correct. 
Some of this work might enhance our arsenal of tools and provide us with info that is useful but the doughnut is the most important thing for we are what we eat so if we eat the hole, how long can we live? 
Thanks! 
Kynn, I agree completely with your statement (snipets below) and is what I too emphasized in the "How To describe flowcharts, ..." thread back in Aug. of 1999. 
I'd like to see developments in an interactive XML-based database driven system where I could ask questions like "What are the states North of Kentucky?" or "How many miles is it from city1 to City2?" just to pick a few rather trivial questions. 
As Len Kasday said in the thread on describing flowcharts, the system should go beyond requiring the user to formulate questions and give the user suggestions as to what kinds of relationships exist or what kinds of information can be retrieved. 
Of course, those with disabilities affecting comprehension of text would need a visual interface, so both are really required. 
The haptic mouse is an interface which gives me yet another non-linguistic way to "ask questions". 
In all cases, though, it seems to me a solid, high level database driven system behind the scenes is needed, independent of the input and output modes. 
On analogy with CSS, there probably needs to be Device Interface sheets describing the mapping from the operations of the device to database querries (already under way - e.g. xml- query? 
and WAP). 
That is, semantic content and device/mode are not mutually exclusive but a two-part whole. 
If there are researchers out there who want to give another mode of access a try, as long as I can get the same information out, great. 
Dr. Raman's EMACSpeak and ASTER give another interface (speech) but at a much higher level, with the ability for the user to customize higher levels (in effect allowing the user to create his/her own query language). 
that's what really needs to be done - customizable, extensible, user-agents and devices and the languages that a user uses to communicate with them. 
"Computer, I'd like to map all word based queries into a sequece of graphical icons" - Of course, there will also have to be a graphical way to "say" this command as well. 
I actually believe, technically speaking, this could be done today. 
(Any more developments from the Voice Browser Activity http://www.w3.org/voice ?, and, is there an analogous Visual Browser Activity which coordinates efforts as to the types of questions asked or clicked on ) I mean, is there a mapping from spoken query to icon-based query to Haptic-based query? 
Since haptic based interfaces would only give relationships based on physical contiguity meaning what feels next to where I am currently, it is not clear whether querries based on physical nearaness can capture all the advantages of seeing the "global" view of the map. 
There was some research a few years ago, (I havent' seen recent work), on Earcons and 3D Audio and Sonification. 
Again, all very interesting and I think worth pursuing but the semantic database back engine needs to be there and mappings from devices/user-agents need to be created. 
This is a very interesting question. 
I think, though, that we should keep in mind that the -map- is rarely the content we are conveying; instead it is the -information on the map-. 
So when we consider the options, we need to focus on conveying the content of the map, not the form. 
Yes! 
You could print off, with a braille printer, a crude map -- I've seen a map of the US, for example, at CSUN printed in this way. 
But it may not be the best way to convey the info in the map. 
Could be, it would depend on what is being conveyed. 
I have not read the article, so I don't know the intended scope of the application. 
-Steve Steve McCaffrey Senior Programmer/Analyst Information Technology Services New York State Department of Education (518)-473-3453 smccaffr@mail.nysed.gov 
Member, New York State Workgroup on Accessibility to Information Technology Web Design Subcommittee In the drarft note on accessibility features in SVG - to provide information such as the relationships that you describe (well, our example is a lot simpler, but it is there as an example). 
Since SVG can create images from components, and useful things can be known or deduced about them (like their relative positions, how they are connected, etc) it may serve this kind of goal very well (and it is already XML, with the added bonus of being able to be represented graphically). 
Cheers Charles McCN Kynn, I agree completely with your statement (snipets below) and is what I too emphasized in the "How To describe flowcharts, ..." thread back in Aug. of 1999. 
I'd like to see developments in an interactive XML-based database driven system where I could ask questions like "What are the states North of Kentucky?" or "How many miles is it from city1 to City2?" just to pick a few rather trivial questions. 
As Len Kasday said in the thread on describing flowcharts, the system should go beyond requiring the user to formulate questions and give the user suggestions as to what kinds of relationships exist or what kinds of information can be retrieved. 
Of course, those with disabilities affecting comprehension of text would need a visual interface, so both are really required. 
The haptic mouse is an interface which gives me yet another non-linguistic way to "ask questions". 
In all cases, though, it seems to me a solid, high level database driven system behind the scenes is needed, independent of the input and output modes. 
On analogy with CSS, there probably needs to be Device Interface sheets describing the mapping from the operations of the device to database querries (already under way - e.g. xml- query? 
and WAP). 
That is, semantic content and device/mode are not mutually exclusive but a two-part whole. 
If there are researchers out there who want to give another mode of access a try, as long as I can get the same information out, great. 
Dr. Raman's EMACSpeak and ASTER give another interface (speech) but at a much higher level, with the ability for the user to customize higher levels (in effect allowing the user to create his/her own query language). 
that's what really needs to be done - customizable, extensible, user-agents and devices and the languages that a user uses to communicate with them. 
"Computer, I'd like to map all word based queries into a sequece of graphical icons" - Of course, there will also have to be a graphical way to "say" this command as well. 
I actually believe, technically speaking, this could be done today. 
(Any more developments from the Voice Browser Activity http://www.w3.org/voice ?, and, is there an analogous Visual Browser Activity which coordinates efforts as to the types of questions asked or clicked on ) I mean, is there a mapping from spoken query to icon-based query to Haptic-based query? 
Since haptic based interfaces would only give relationships based on physical contiguity meaning what feels next to where I am currently, it is not clear whether querries based on physical nearaness can capture all the advantages of seeing the "global" view of the map. 
There was some research a few years ago, (I havent' seen recent work), on Earcons and 3D Audio and Sonification. 
Again, all very interesting and I think worth pursuing but the semantic database back engine needs to be there and mappings from devices/user-agents need to be created. 
This is a very interesting question. 
I think, though, that we should keep in mind that the -map- is rarely the content we are conveying; instead it is the -information on the map-. 
So when we consider the options, we need to focus on conveying the content of the map, not the form. 
Yes! 
You could print off, with a braille printer, a crude map -- I've seen a map of the US, for example, at CSUN printed in this way. 
But it may not be the best way to convey the info in the map. 
Could be, it would depend on what is being conveyed. 
I have not read the article, so I don't know the intended scope of the application. 
-Steve Steve McCaffrey Senior Programmer/Analyst Information Technology Services New York State Department of Education (518)-473-3453 smccaffr@mail.nysed.gov 
Member, New York State Workgroup on Accessibility to Information Technology Web Design Subcommittee W3C Web Accessibility Initiative http://www.w3.org/WAI Location: I-cubed, 110 Victoria Street, Carlton VIC 3053 Postal: GPO Box 2476V, Melbourne 3001, Australia 
