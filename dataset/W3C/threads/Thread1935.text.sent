Here are two modest suggestions for HTTP/2.0 with their rationale. 
1. Add a header to the client request that indicates the hostname and port of the URL which the client is accessing. 
Rationale: One of the most requested features from commercial server maintainers is the ability to run a single server on a single port and have it respond with different top level pages depending on the hostname in the URL. 
Service providers would like to have multiple aliases for a single host and have URLs like all return appropriate (and different) pages even though all the hostnames refer to the same IP address. 
This is not currently possible because there is no way for the server to know the hostname in the URL by which it was accessed. 
2. Require (or request) that clients support relative URL's in redirects (status 301 and 302). 
Rationale: This is important for small special purpose servers (e.g. gateways). 
Such a server is simpler to write, more robust, and more portable if it doesn't need to know its hostname or port. 
At present there are two reasons a server needs to know its hostname and port. 
First if it supports CGI it is required to supply this information and second for sending 301 and 302 redirects. 
Normally a small special purpose server would not support CGI and wouldn't send redirects. 
However, in order to use relative URL's, a server must deal with requests like GET /dir1/dir2 which should be GET /dir1/dir2/ i.e. with trailing '/'. 
The accepted (and perhaps only) way of handling this is with a redirect from the first to the second. 
Since clients must handle relative URL's anyway there is little cost in having them handle them in redirects. 
On the other hand the cost for special purpose servers of needing to know both their hostname and port is significant. 
John Franks Dept of Math. 
Northwestern University john@math.nwu.edu 
Re the first proposal, to incorporate the hostname somewhere. 
This would be cleanest put into the URL itself :- GET http://hostname/fred http/2.0 
This is the syntax for proxy redirects. 
This suggestion conflicts with the aims of the second I'm affraid. 
I don't think that its a good thing for a server to not know its name. 
Proxying is far too prevelant now and a server that doth not know its name shall be called a LOOP. 
Phill According to hallam@axal04.cern.ch: 
The only thing objectionable about this is that it is a substantial change from the HTTP/1.0. 
I suppose we could say the syntax is GET URL HTTP/?? and that HTTP/1.0 only allows relative URLs (i.e. relative to the host being queried). 
Obviously a proxy server would have to know its own name. 
But a small gateway that speaks HTTP on one side and accesses a local service on the other side shouldn't need to. 
(Am I wrong about this? 
How would that create a loop?). 
The proposal was to make it possible for such a gateway to use file system names in URL's without being required to know its own name. 
Parenthetically, IMHO proxy servers and regular servers should be different programs. 
Their purposes are quite different. 
John Franks If the server needs to know its own name, it seems more appropriate that the info be made part of the request header and not the request itself. 
If you want to have any hope of easily maintaining backwards compatibility with slack clients, the header is a much safer place to put this info. 
A more general purpose solution might be to have clients send the complete URL that they used to make their current query. 
Something like: From-URL: http://some.host/some/path.html 
Since there will be backwards compatibility issues with old and new clients, it matters little where this information is passed as some clients will send it and some won't. 
In order to prevent wholesale overhaul of HTTP request processing, it seems much easier to add a new header field than to substantially change the syntax of requests. 
Relying on something as weak as a domain name for differentiating the roles a server is to perform is an extreme hack. 
There are MUCH better ways to accomodate this that won't be subject to the whims, vagueries, and failures of DNS. 
Path arguments, header fields, and any number of other techniques can be used already to help a server determine its "role". 
Simply depending on the DNS name of the server is not sufficiently robust. 
I agree! 
Proxy servers perform a completely different function from "regular" servers and in my opinion, are more properly termed "proxy clients". 
With the advent of caching clients like NetScape, making allowances in HTTP for proxy servers may become less of requirement except at sites where there are evil firewalls that only allow the proxy in and out. 
Chuck Shotton \ Assistant Director, Academic Computing \ "Shut up and eat your U. of Texas Health Science Center Houston \ vegetables!!!" cshotton@oac.hsc.uth.tmc.edu (713) 794-5650 \ I disagree very strongly here. 
Security proxies such as the TIS proxy are rather different to what the CERN proxy server provides. 
Here there is a primitive version of an item I beleive represents the future of the Web, a caching relay server. 
The name proxy is a misnomer. 
Client side caqches help but only to a small extent. 
Client side caches cannot be safely shared in most circumstances. 
Phill H-B According to Chuck Shotton: Everything you say is true from a technical point of view. 
However, the issue here is a political/commercial one. 
When a company contracts with a service provider to create a WWW presence they want the URL for their company to be something like They don't want the service provider's name in the URL and they don't want any path or port stuff at the end. 
It's a PR thing. 
It may seem silly but it is important to them. 
(As you no doubt know there are lawsuits now over the ownership rights to DNS names.) On the other hand, the service provider does not want to have to have a different computer for each client since most clients put minimal load on a server. 
It seems to me that both the desires of the company and the desires of the service provider are reasonable and it ought to be possible to accomodate them with a very minor change in the protocol. 
John Franks I think allowing GET url HTTP/2.0 makes sense just in terms of cleaning up the protocol, independently of the motivation of helping people who want to serve maultiple host-names from the same host. 
Servers don't really need to know their own names, as much as they need to be able to discover their own addresses, and, after doing the name lookup on a new hostname first ask "is this me?". 
Servers will also need some way to discover their own port, though. 
This is an apples and oranges discussion. 
An alias name in the DNS for a computer has very little to do with Web servers or HTTP. 
There is NO change needed to HTTP request syntax to accomodate this. 
As I said before, clients are going to need to send the info to the server one way or another. 
My proposal is that they adopt a standard HTTP header field that specifies the complete remote URL used to access the server. 
Since there will be a mix of clients, some supporting host name reporting and some not, it just doesn't matter how this info gets to the server. 
Since it doesn't matter, the easier to implement solution is a new HTTP request header field. 
It allows all clients and servers to operate as they do now with NO code changes. 
Clients and servers that actually need host name information can have tiny mods made to send the extra header field containing the URL and process it. 
Leave the standard alone on this issue. 
It is robust enough to do what you want using the mechanisms built into it now without completely convoluting the syntax of a request. 
Companies will still be able to use whatever domain name they want and servers will get a LOT more info than just the host name with this scheme. 
In any case, the client must conform to a new standard, whatever it is, or this won't work. 
All I'm suggesting is that there is a better way to implement the delivery of host name info to the server that doesn't involve hacking the request syntax and can be backwards compatible with ALL clients and servers. 
From-URL: http://host.name/file/path/info.html Chuck Shotton \ Assistant Director, Academic Computing \ "Shut up and eat your U. of Texas Health Science Center Houston \ vegetables!!!" cshotton@oac.hsc.uth.tmc.edu (713) 794-5650 \ The issue in question is not that of using CNAME aliases (which provide different names for the same service), but one of providing different services on the same machine, all with (vanity) addresses of the form above. 
I _think_ this is currently done at a few sites with a feature of the BSD ifconfig that allows one interface to accept traffic on multiple IP addresses on one interface, then hacking the server to serve up different web pages for the different IP addresses. 
It's an ugly hack, but there is a demand. 
I personally doubt this can be "fixed" in the HTTP protocol because of the problem of supporting old clients, and because this is, in effect, trying to subvert the meaning of the DNS in the context of URLs. 
Albert Lunde Albert-Lunde@nwu.edu 
This is only true of servers on Unix implemented to run under inet. 
It isn't the case on any other server on any other platform including stand-alone Unix servers, because these servers already know what port they are listening on. 
Servers DO need to know host name and port info so they can pass it to CGI applications which may need to generate self-referencing URLs. 
They just don't need to find it out by forcing a wholesale change on the way clients make requests to the server. 
Imagine all of the software that will have to change, from clients and servers to dedicated scripts, applications, etc., if the syntax of a GET request changes to require a complete URL. 
Information contained in the URL is redundant, given that servers already know their IP address, the protocol they are communicating with, and the port number. 
The ONLY missing piece of information is something that has NOTHING to do with HTTP, HTML, or the WWW and everything to do with some strictly commercial needs - namely the actual DNS name that was used to access the server. 
As I said before, using the domain name to determine server function may (or may not) be considered a hack, but it doesn't really have anything to do with HTTP, per se. 
It has to do with some configuration "tricks" that some server administrators feel they need to do to make customers happy. 
I'm all for that, but I think that the appropriate mechanism should be chosen and munging the HTTP request syntax isn't it. 
Bottom line is that it would be a lot easier to look for a new request header field than to have to add a bunch of conditional code to process a different request syntax for HTTP/1.0 vs. HTTP/2.0. 
The two protocols will not be forward/backward compatible if a syntax change is made to the request, causing a lot of headaches for everyone. 
I suggest avoiding the headaches altogether and simply define the new request header field. 
Can someone point out a good reason NOT to accomodate the need for sending a host name by putting it in a required header field as part of a complete URL? 
If there's something I'm overlooking, I'll gladly stop whining. 
Chuck Shotton \ Assistant Director, Academic Computing \ "Shut up and eat your U. of Texas Health Science Center Houston \ vegetables!!!" cshotton@oac.hsc.uth.tmc.edu (713) 794-5650 \ Yes, I understand the problem. 
It is identical to the technique used by some C programs to examine argv[0] and perform different behaviors (serve different home pages in the WWW case) based on the name, like compress/uncompress or sendmail/newalias. 
But instead of using the name of a program to determine behavior on a host, admins want to use the name of a host to determine behavior of a single server (with multiple names). 
This is a legitimate technique to use. 
I just question the logic behind altering the syntax of HTTP requests when other mechanisms exist. 
The real issue is that if clients don't send the name, servers have no way of knowing which of many names was used to contact the server. 
SO, clients ultimately have to support sending this info. 
Since clients need to change, there will be a non-trivial period of time where some clients support the new method (whatever that may be) and some don't. 
In order to ease the transition (strictly from a software developers' perspective), the servers should easily be able to accomodate requests from both types of clients. 
The best way to do this is to try and leave the ways that clients communicate with servers relatively untouched and enhance the amount of info sent from client to server using features in the HTTP protocol designed for this purpose. 
Namely, HTTP request header fields. 
New clients will send the field, old clients won't. 
New servers will understand the field, old servers won't. 
New clients will still be able to talk to old servers with the SAME syntax, and old clients can talk to new servers, too. 
Changing the request syntax to include a full URL will preclude NEW clients being able to talk to OLD servers. 
The client has NO way of knowing whether or not the server it is about to talk to can understand HTTP/2.0 until it talks to it. 
This is the single biggest reason to avoid radical changes to the syntax for the request. 
I don't know of any servers now that break if they get a HTTP request header field that they don't understand. 
But I bet every one of them will fail if they get a complete URL in a GET request. 
This is different than using CNAMEs and doesn't present the same problem since you DO know which ip address was contacted and can equate this directly to a host name. 
It also isn't widely supported on many Unix workstations. 
Another thing to consider is that a VAST majority of Web servers aren't even being run on Unix servers. 
There are MANY, MANY more servers running on PCs and Macs than Unix. 
So continuing to adopt a Unix-centric approach to implementing new HTTP features is not necessarily the best idea. 
In so far as the HTTP protocol equates to the actual request/response method syntax, I agree. 
However, there is an unlimited ability to modify client and server interaction using other parts of the request/response data (the header fields). 
Chuck Shotton \ Assistant Director, Academic Computing \ "Shut up and eat your U. of Texas Health Science Center Houston \ vegetables!!!" cshotton@oac.hsc.uth.tmc.edu (713) 794-5650 \ Are you really proposing that HTTP/2.0 be kept compatible with HTTP/1.0 such that old HTTP/1.0 servers could ignore the "HTTP/2.0" in the GET request and respond as if it were a HTTP/1.0 request? 
Any protocol change for HTTP will have to be staged by first getting most of the servers to upgrade. 
If there are no changes proposed that would actually require some different response, then why bother calling it 'HTTP/2.0' at all? 
Actually, this gets me to a point where I want to stop talking about HTTP/2.0 at *all*: we need a specification/standard for HTTP/1.0, as an IETF RFC, either an "informational" one or as a "draft standard". 
Is anyone willing to volunteer to put such a beast together? 
Yes. 
This is pretty important, since most servers will handle this already. 
(Most apparently ignore the HTTP/1.0 tag or don't care if the version number is off. 
Try it by telnetting to port 80 and sending GET / HTTP/2.0 to any server) This means that forward/backward compatibility can be maintained between new clients and old servers NOW, with no changes. 
Only if the syntax of the request/response changes. 
Otherwise, the status quo can be maintained for old servers while new clients and servers get the benefit of new HTTP additions. 
A question of semantics, I suppose. 
If all that changes are the header fields, leaving the syntax of methods, requests, and responses alone, then there is no fundamental change required of old servers as they can just ignore the new headers. 
If radical change is a requirement for increasing the protocol version number, then there's no reason to change the version number for this "hostname" proposal. 
But if substantial functionality is added in the context of the existing HTTP/1.0 standard, there's also no reason that it can't be termed a new draft of the standard, or a new version altogether. 
What's in a number anyway? 
There's already a draft RFC for HTTP/1.0. 
Were you thinking of something beyond the current draft that's available from info.cern.ch? 
Chuck Shotton \ Assistant Director, Academic Computing \ "Shut up and eat your U. of Texas Health Science Center Houston \ vegetables!!!" cshotton@oac.hsc.uth.tmc.edu (713) 794-5650 \ There is no Internet Engineering Task Force 'RFC' for HTTP. 
There may be a document that CERN put on the web that describes its use, but it hasn't been published as an RFC. 
You might want to check out RFC 1310, "The Internet Standards Process" for more details. 
I thought we were here (in html-wg, rather than on www-talk) for the purpose of creating Internet Standards for HTTP. 
If that isn't the purpose of this mailing list, would someone please correct me? 
(and take me off the list; I'm on enough 'random chatter' mailing lists, thank you). 
Henrik and I, with the help of Bob Denny, have been working on it steadily over the past month. 
The draft should be available sometime early next week. 
We anticipate that it will go through at least one iteration before the San Jose BOF, where it will be the main topic. 
......Roy Fielding ICS Grad Student, University of California, Irvine USA Two points :- 1) Running httpd under inetd is definitively not recommended. 
Under many UNIX implementations the inetd daemon breaks very badly when it gets large number of simultaneous requests. 
So one netscape file upload and your whole system is hosed. 
2) The proposal to allow specification of the whole URL at the method prompt would be an option, not mandatory. 
The point being that proxies should be intelligent enough to identify requests to themselves. 
Phill H-B As agreed at the WWWF'94 HTTP BOF, Henrik Nielsen and Roy Fielding are working on this and will report at the IETF meeting next month. 
Simon Spero will report on work on HTTP-NG. 
Best wishes, Dave Raggett United Kingdom 
