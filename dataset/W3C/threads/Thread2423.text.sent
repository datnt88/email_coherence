I was hoping to polish this proposal a little more before floating it externally, but alas, with the meeting on Monday, time did not permit. 
I hope that I have at least stated my perspective well enough to stimulate discussion. 
Problem statement: The existing HTTP authentication model does not allow authentication realms to be distributed across servers. 
To protect user credentials. 
HTTP browsers associate each realm with a single IP address, and will not pass user credentials to multiple servers even if they claim to belong to the same realm. 
This security measure, built into the browser, has created the undesirable side effect of requiring users to re-type their user names and passwords for each protected server within a multiple server site. 
Abstract: A proposal is made to: Allow the distribution of protection realms across servers Protect user credentials from "imposter" servers which claim to belong to a realm but do no Pass user credentials securely to content servers which identify themselves as members of those realms To re-use user credentials throughout protection realms so that users are challenged for user name and password only once within the context of a single session. 
Create secure, trusted relationships between servers Centralize authentication, authorization, and directory services for one or multiple websites Centralize directory security Simplify or eliminate directory services on distributed content servers Make it scalable I propose that this new authentication scheme be named "remote authentication". 
Theory of operation: When access is first attempted to a content page which is protected by remote authentication, the browser is redirected to the "remote authentication" server for that realm. 
This redirection should be done via SSL for security. 
The user is then challenged for user name and password by basic authentication. 
The server then encrypts the user name and password with a secret (symmetric) key and returns the user name and password to the browser where they are cached for the session. 
The browser is then re-directed to the original content server and the browser passes the encrypted user name and password to the server. 
The content server, which shares the same secret key as the authentication server, is able to decrypt the user name and password. 
When the browser is challenged by another server which claims to be in the same realm, the user credentials are served. 
If the server is trusted, it will share the same secret key as the authentication server and the user name and password will be decrypted. 
If the server is an imposter, decryption will fail and the user credentials remain secure. 
The way I see it, the browser will associate each realm with an encrypted user name and password, which are simply opaque strings. 
In addition, I would like the browser to allow the server to cache an additional string of arbitrary length to pass state to the other servers in the realm. 
It could be used to store additional authentication and authorization semantics, such as an expiration time for the authentication, authorization information such as a list of groups to which the individual belongs, an index to identify the identity of the encryption key, etc. 
This information could be signed with an MD5 MAC or encrypted or just plain cleartext. 
I think that we could do the web a great big favor if we eliminated the need to replicate directories to content servers. 
The authentication piece is easy, but if we can solve the authorization part I believe that we could simplify the common registration puzzle at the same time. 
Thank you for your support, -e I think that the spec for "domain" is broken -- it specifies a list of URIs, but doesn't say that these can be _prefixes_ of URIs that may also use the same credentials. 
Without that, it is pretty uselss, IMHO. 
From: Scott Lawrence[SMTP:lawrence@agranat.com] Sent: Friday, December 05, 1997 10:53 AM Subject: Re: Proposal for new HTTP 1.1 authentication scheme Digest authentication already includes a mechanism (the 'domain' attribute; see section 3.2.1 of draft-ietf-http-authentication-00) to specify that credentials may be used on multiple servers, and through the 'digest' attribute allows for mutual authentication. 
There is also the model of Kerberos to consider - developing a ticket-based authentication scheme (with the advantages and problems of any third-party mechanism) would be another area to explore. 
Could the spec be fixed without interoperability trouble emerging? 
(Query to digest implementers???). - Jim Most of the suggestions by Paul and Dave seem to be clarifications of the original intent. 
They should not cause problems. 
The one significant change is Paul's suggested change of the algorithm for calculating the "entity-digest". 
If implementations exist they will be incompatible. 
I don't think I would describe this as fixing a "bug" in the entity-digest algorithm. 
It might be an improvement though. 
On the other hand, if I recall correctly it was Paul who wrote the entity-digest algorithm, so he may have a right to call it a bug. 
John Franks john@math.nwu.edu 
I still feel my one objection about proxy-added headers is substantive and unresolved. 
Briefly, an origin server might omit headers that get figured into the entity-digest calculation. 
A proxy might subsequently add those headers. 
The client sees a message *with* the headers, calculates an entity-digest that figures them in, and gets a different answer from what the origin server calculated. 
Dave Kristol I agree that there is an issue here. 
The current spec says the proxy MUST not add these headers. 
If I recall you suggested the MUST be changed to SHOULD. 
I am not sure how this helps beyond making the proxy technically "legal." 
It doesn't materially affect the problem. 
What should a proxy do in this situation? 
It seems it must either not add headers or break the entity-digest. 
John Franks john@math.nwu.edu 
Ummm... I think my "MUST - SHOULD" had to do with a proxy's changing the content of headers. 
I think I see the words to which you're referring (end of p.13), and they mention Content-Length explicitly but don't mention Date. 
And there's a potential problem with Content-Length: suppose a proxy eats chunked data and wants to create a complete entity *with* Content-Length. 
Is it hereby forced to forward the entity as "chunked" because it's forbidden to add Content-Length? 
I agree it's a dilemma. 
An option is to require that clients send Content-Length and (perhaps) not Date, and forbid proxies to add either within this context. 
Dave Kristol Sorry, my mistake. 
...snip.. Here is what the spec says: The entity-info elements incorporate the values of the URI used to request the entity as well as the associated entity headers Content-Type, Content-Length, Content-Encoding, Last-Modified, and Expires. 
These headers are all end-to-end headers (see section 13.5.1 of [2]) which must not be modified by proxy caches. 
The "entity-body" is as specified by section 10.13 of [2] or RFC 1864. 
The content length MUST always be included. 
The HTTP/1.1 spec requires that content length is well defined in all messages, whether or not there is a Content-Length header. 
I was remembering "which must not be modified by proxy caches" as "which MUST NOT be modified by proxy caches." 
I guess I don't see a problem with this. 
On the question of length it says the content length must be used in the digest even if there is no Content-Length header. 
This seems fine and should cause no problem if a proxy unchunks a chunked response. 
The server has to calculate the MD5 digest of the entity so it will not be much harder to calculate the length. 
I guess the proxy better get the length right or the client better do its own length calculation and not trust the proxy. 
As for Date, I guess the only problem is servers with no clock. 
They don't send a Date header. 
Draft-v11-rev01 says A received message that does not have a Date header field MUST be assigned one by the recipient if the message will be cached by that recipient or gatewayed via a protocol which requires a Date. 
So it seems that it is fine for the proxy to forward the dateless response as long as it does not cache the entity. 
It is unlikely that an authenticated response should be cached anyway. 
Maybe there are problems I don't understand. 
John Franks john@math.nwu.edu 
Studying the specification some more I see there seems to be some ambiguity about the meaning of Content-length. 
Here are some quotes: 7.1 Entity Header Fields Entity-header fields define optional metainformation about the entity- body or, if no body is present, about the resource identified by the request. 
entity-header = ... Content-Length ; Section 14.14 7.2.2 Length The length of an entity-body is the length of the message-body after any transfer codings have been removed... But later we have 14.14 Content-Length The Content-Length entity-header field indicates the size of the message-body, in decimal number of OCTETs, sent to the recipient... 
These seem inconsistent. 
If Content-Length means the length after transfer encodings have been applied then it is hop-by-hop and not end-to-end. 
It also cannot be an entity header as described in 7.1. 
There probably is also a need for a header meaning entity-length. 
Personally I would like to see Content-Length remain an entity header. 
All the other Content-* headers are entity headers and apply to the entity before transfer encoding. 
One way to do this would be to introduce a new "Transfer-Length" header with the stipulation that its default value is the Content-Length. 
The Content-Length would be defined as it is now in section 7, i.e. the entity length. 
Thus the Transfer-Length header would only be needed when the message length and entity length differed. 
This would give us consistent terminology (Content-* for entity, Transfer-* for message). 
It would also not break any current of which I am aware. 
At present the only widely deployed TE is chunked and it needs neither header. 
If new TEs arise which need to have the message length specified they would have to use Transfer-length (or both). 
I see no alternative other than rewriting the specification to make Content-length a hop-by-hop general header and not an entity header. 
The authentication specification would also need to be modified since it is not possible to put Authentication-Info in a chunked trailer as it is currently defined if Content-length is the length of the chunked message. 
John Franks An alternate proposal, which I believe is simpler and requires less modification: Content-Length, if validly present, is the length of the entity-body (which is the message-body after transfer codings are removed). 
It is also the length of the message-body if no transfer-coding is used. 
Content-Length MUST NOT be present if a transfer coding is used. 
If it is present in such cases, it is invalid, and the robustness principle says it should be ignored. 
If we want, we can provide for the future existence of non-self-delimited transfer codings by adding Transfer-length: Transfer-Length, if validly present, is the length of the message-body. 
Transfer-Length MUST NOT be present on self-delimiting transfer codings. 
If it is present in this case, it is invalid, and the robustness principle says it should be ignored. 
Under these rules, Content-Length is still logically end-to-end -- the header may not physically be present, but its value if it is ever present is well-defined end-to-end and the same end-to-end. 
Note that the definition of Content-length is independent of the existence of Transfer-length, and we could omit it. 
Under these rules, proxies may delete Content-length, as long as the add Transfer-length or a self-delimited transfer coding. 
So, we remove it from the list of "must not modify" headers. 
Digest Auth should drop all reference to Content-Length and replaces it with "length of entity-body", which is always well-defined and always determinable given the above rules. 
Paul After thinking about this some more, I believe we've (almost) all missed an obvious solution. 
We're concerned about how the sender specifies the actual length of the message body when a transfer-coding is used. 
Dave Morris has been saying "redefine the encoding to include a header (within the output of the encoding process) which gives the length." 
Which I had been thinking of as a bad idea, because it means that HTTP would be in the business of specifying the format of the message bodies. 
But then I remembered something that Dave had written earlier: We introduced CHUNKed transfer encoding because it is difficult for some servers to know the content length prior to beginning to send data. 
(in this case, he was discussing trailers, but it's an important observation.) And we already have the ability to specify multiple transfer codings in the Transfer-Encoding header. 
So: Instead of defining a Transfer-Length header, we could simply add this rule: If a message is sent on a persistent connection using a transfer-coding that does not exactly preserve the length of the data being encoding, then the "chunked" transfer-coding MUST be used, and MUST be the last transfer-coding applied. 
I.e., instead of sending HTTP/1.1 200 OK Date: Thu, 11 Dec 1997 20:33:51 GMT Transfer-Encoding: compress Transfer-Length: 12345 ... compressed data ... a server could send HTTP/1.1 200 OK Date: Thu, 11 Dec 1997 20:33:51 GMT Transfer-Encoding: compress, chunked 3039 ... compressed data ... 0 [Note: 12345 = 0x3039] Since the chunked encoding by definition includes the "transfer length", we don't need another header field. 
And all HTTP/1.1 implementations are required to support ("receive and decode") the chunked transfer-coding, so we don't need to argue about whether this is interoperable. 
We don't have to require a chunked transfer-coding when the end-of-message is marked by end-of-connection (not that we want to encourage it, of course) since current HTTP/1.0 practice suggests that this basically works. 
Aside from that, I don't have much to complain about Paul Leach's "simplified" proposal, although I'm not sure it would really be any simpler once all the definitions are made consistent. 
Under these rules, Content-Length is still logically end-to-end -- the header may not physically be present, but its value if it is ever present is well-defined end-to-end and the same end-to-end. 
I'm not sure it makes sense to talk about a header being "end-to-end" if it isn't actually transmitted on some hops. 
The content-length *value* is certainly end-to-end (although this is really an entity-length value), but I'm not sure we should be calling a header end-to-end if it is possible to remove it. 
After all, the definition in 13.5.1 
says: . 
End-to-end headers, which must be transmitted to the ultimate recipient of a request or response. 
End-to-end headers in responses must be stored as part of a cache entry and transmitted in any response formed from a cache entry. 
Do you really want to change this definition? 
Or admit that Content-Length is another category? 
-Jeff JM After thinking about this some more, I believe we've (almost) all JM missed an obvious solution. 
JM If a message is sent on a persistent connection using JM a transfer-coding that does not exactly preserve the JM length of the data being encoding, then the "chunked" JM transfer-coding MUST be used, and MUST be the last JM transfer-coding applied. 
JM a server could send JM HTTP/1.1 200 OK JM Date: Thu, 11 Dec 1997 20:33:51 GMT JM Transfer-Encoding: compress, chunked JM 3039 JM ... compressed data ... JM 0 Simple and elegant. 
Let's make it so. 
Scott Lawrence EmWeb Embedded Server lawrence@agranat.com 
Agranat Systems, Inc. Engineering http://www.agranat.com/ 
