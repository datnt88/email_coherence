At the telecon meeting today we reached consensus on the following definition for objective. 
If 80% or more of people who have knowledge of the relevant tech and test methods would agree in their judgment This is what we would use to test our guidelines or components for objectivity. 
It is posted here for comment. 
Gregg Gregg C Vanderheiden Ph.D. Professor - Human Factors Dept of Ind. 
Engr. - U of Wis. 
Director - Trace R &amp; D Center Gv@trace.wisc.edu 
mailto:Gv@trace.wisc.edu , http://trace.wisc.edu/ 
For a list of our listserves send ?lists? to listproc@trace.wisc.edu 
So how do we define "knowledge of the relevant tech and test methods"? 
i.e. how do people qualify to have their opinions considered or not in weighing up whether 80% agree? 
cheers Charles At the telecon meeting today we reached consensus on the following definition for objective. 
If 80% or more of people who have knowledge of the relevant tech and test methods would agree in their judgment This is what we would use to test our guidelines or components for objectivity. 
It is posted here for comment. 
Gregg Gregg C Vanderheiden Ph.D. Professor - Human Factors Dept of Ind. 
Engr. - U of Wis. 
Director - Trace R &amp; D Center Gv@trace.wisc.edu 
mailto:Gv@trace.wisc.edu , http://trace.wisc.edu/ 
For a list of our listserves send ?lists? to listproc@trace.wisc.edu 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) There are two answers to this question which emerged from today's teleconference: 1. 
Initially, the question of whether 80% or more of informed evaluators would agree, is to be decided hypothetically by the working group: do we think that 8 out of 10 testers familiar with the relevant language/format specification(s), when given the content, would agree on whether or not the checkpoint had been met. 
2. Later, of course, the issue will be decided by assessing the results of usability testing, in which actual content evaluations will be carried out in application of the guidelines. 
Right, The purpose of those qualifiers is just to eliminate having people fail to agree because we picked people who don?t understand what they are doing. 
It is to make it easier to achieve the "objective" label. 
This isn't something that goes into the guidelines by the way. 
It is something we use as part of our testing to see if something does go in, or rather does go in as an objective measure or criteria. 
So we (whomever is doing the testing) will be picking the people. 
We/they just need to pick people who, if they disagree, disagree because the criteria or whatever is ambiguous, not because they don?t understand what the question is. 
Gregg Gregg C Vanderheiden Ph.D. Professor - Human Factors Dept of Ind. 
Engr. - U of Wis. 
Director - Trace R &amp; D Center Gv@trace.wisc.edu 
mailto:Gv@trace.wisc.edu , http://trace.wisc.edu/ 
For a list of our listserves send ?lists? to listproc@trace.wisc.edu 
Behalf Of Jason White methods"? 
i.e. weighing up There are two answers to this question which emerged from today's teleconference: 1. 
Initially, the question of whether 80% or more of informed evaluators would agree, is to be decided hypothetically by the working group: do we think that 8 out of 10 testers familiar with the relevant language/format specification(s), when given the content, would agree on whether or not the checkpoint had been met. 
2. Later, of course, the issue will be decided by assessing the results of usability testing, in which actual content evaluations will be carried out in application of the guidelines. 
"We" exclude "they". 
Did 80% "consense" on this? 
If after 80% agreed did anyone propose a "motion to make it unanimous" as happens in most centralized/elitist organizations. 
Did very many people with "developmental disabilities" get to vote? 
I've become so senile that I don't even know how to unsubscribe from this list as well as get my name off that of "active Working Group members." 
Count me out. 
Unsubscribe. 
Remove. 
Regrets. 
Love. 
EACH UN-INDEXED/ANNOTATED WEB POSTING WE MAKE IS TESTAMENT TO OUR HYPOCRISY Bill, Reading back in the thread and the minutes, yes it looks like that definition may need more discussion -- but I'm assuming that's why Gregg put it out there. 
Seems like a thorny issue to figure out. 
I'm not sure what other approach would work better, but suggestions are welcome. 
Looks like WCAG WG is trying to set up a re-usable process for making determinations on testing questions not yet on the table. 
The crux of the definition might be who's considered an expert. 
Charles suggested that, and so did you. 
Maybe that's the part of the definition that needs the most work -- in reading it, I assumed that it would include people with disabilities who have need for the accommodation in a specific technique -- plus some knowledge of how it can be tested for. 
But it's not spelled out, and other people might make different assumptions about who's intended. 
The wording in the draft definition says "people who have knowledge of the relevant tech and test methods" which is open to different interpretations. 
One commenter in the minutes suggests "professional" which sounds quite exclusive, but that didn't make it into the draft definition. 
Another flavor I get from the minutes is "technology experts" but again that didn't make it into the draft definition. 
What do you think about "including people with disabilities who have need for the accommodation in a specific technique -- and some knowledge of how it is tested for"? 
Or other ideas? 
Or a diferent approach to addressing this issue? 
If you want to unsubscribe, you can send a message to w3c-wai-gl-request@w3.org with "unsubscribe" as the subject line. 
Note that that's not the same address as the mailing list address. 
But your comments are still welcome. 
- Judy Judy Brewer jbrewer@w3.org +1.617.258.9741 http://www.w3.org/WAI Director, Web Accessibility Initiative (WAI), World Wide Web Consortium (W3C) MIT/LCS Room NE43-355, 200 Technology Square, Cambridge, MA, 02139, USA So 80% of people, who meet subjective criteria for inclusion, then make subjective determinations, and if they happen to agree, we label this "objective"? 
--Kynn Kynn Bartlett kynn@idyllmtn.com 
I don't think it is useful for us to do a hypothetical assessment - I would prefer that we assume for now that we don't know, but that we don't have sufficient grounds to accept or reject something based on how well the requirement is specified (as opposed to whether it deals with something that is an accessibility requirement) until we have done the testing. 
cheers Charles There are two answers to this question which emerged from today's teleconference: 1. 
Initially, the question of whether 80% or more of informed evaluators would agree, is to be decided hypothetically by the working group: do we think that 8 out of 10 testers familiar with the relevant language/format specification(s), when given the content, would agree on whether or not the checkpoint had been met. 
2. Later, of course, the issue will be decided by assessing the results of usability testing, in which actual content evaluations will be carried out in application of the guidelines. 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) 
