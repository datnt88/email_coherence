The 'problem' I was referring to was that of automatically mapping one ontology (written I assume by person or persons A) to another (written by persons B). People have asserted that there exist automatic tools for doing that. 
And I was pointing out some corner cases. 
For the record, I don't believe that I, personally, made such an assertion. 
Nor did I intend to. 
I didn't read anyone else in this thread as doing so. 
I believe someone from NI made such an assertion. 
Perhaps this thread started on another list and migrated. 
It doesn't belong here, as Drew McDermott already pointed out. 
-Evan Evan K. Wallace Manufacturing Systems Integration Division NIST ewallace@nist.gov 
The 'problem' I was referring to was that of automatically mapping one ontology (written I assume by person or persons A) to another (written by persons B). People have asserted that there exist automatic tools for doing that. 
And I was pointing out some corner cases. 
For the record, I don't believe that I, personally, made such an assertion. 
Nor did I intend to. 
I didn't read anyone else in this thread as doing so. 
[snip] Well, I see Jack's message: Followed by my message: Followed by Frank's admonishment: And, I'm sorry, I don't see that either Jack or me saying anything about automatically mapping entire ontologies into each other. 
We did respond about what sorts of reasoning one might try "cross ontologies". 
I didn't originally read Frank's reply as insinuating that either Jack or I were making the silly claims. 
There does seem to be some difference between "reasoning across ontologies" and "mapping between ontologies". 
But whatever. 
Ok, I do see some loose talk at the end of Jack's message, basically: You do it by establishing axioms that express equivalencies, sub-class, or other relationships between the two ontologies (or many more ontologies) and use a mechanism such as owl:import to provide a linkage. 
If you have an inferencing technology, then you can maintain logical consistency across these relationships. 
"Closeness" is a matter of interpretation and can be influenced somewhat by the form of the "bridge" axioms expressed. 
If ontologies are far apart -- ie different concepts -- the logic processor would not infer that they represent the same or similar things. 
Eh. 
I don't see that he's said anything remotely problematic here. 
Add some axioms. 
Smush together. 
The reasoner will tell you 1) if the result is consistent, and 2) some inferred equivalencies and subclassing (e.g., it will classify the result). 
But the axioms are all shlooped in by people. 
My pointers were talking, basically, about other sorts of axioms (and relationships supported by inference mechanisms) floating around (to simplify a bit). 
Cheers, Bijan Parsia. 
No, we never made the assertion that the bridge axiom creation was entirely automatic. 
We stated that with some axioms, the use of OWL with inferencing algorithms can maintain the logical consistency, and answer cross ontology queries (which was the original question). 
I think Jim &amp; Bijan did a good job discussing the issues. 
We do have tools that allow some use of algorithms that do indeed suggest bridging axioms and generate the appropriate OWL, but due to the different nature of these algorithms (probabilistic among other techniques) as opposed to OWL-DL (deductive logic), we advise some person-in-the loop work to ensure continuity. 
Again, once checked, cross ontology reasoning and querying works fine. 
In addition, the precision of these algorithms is greatly increased if constrained within domains (i.e, not trying to compare a bookselling webservice with a type of cancer, but only against other e-commerce services) Jack The 'problem' I was referring to was that of automatically mapping one ontology (written I assume by person or persons A) to another (written by persons B). People have asserted that there exist automatic tools for doing that. 
And I was pointing out some corner cases. 
For the record, I don't believe that I, personally, made such an assertion. 
Nor did I intend to. 
I didn't read anyone else in this thread as doing so. 
I suspect this is one of those familiar cases where the, er, professionals, or maybe old hands would be better, know that some problem is almost certainly unsolvable *in general* and they know many nasty examples which are guaranteed to wreck any attempt to solve the problem *in general*, but that in practice one can do very useful work that pays off in real terms quite effectively, particularly if one keeps away from the corner cases. 
This situation tends to lead to this kind of mutually recriminationory conversation where the practical guys are kind of pleased with what they can get done, and the old hands are offended by what seems to them to be empty boasting. 
"Works fine" for example is capable of many subtle nuances of meaning. 
To some, it sounds like 'never fails' , i.e. a claim to have solved the central theoretical problems. 
To others, no doubt, it merely indicates that, as we used to say in our final reports to funding agencies, useful results have been achieved in some cases. 
Pat The 'problem' I was referring to was that of automatically mapping one ontology (written I assume by person or persons A) to another (written by persons B). People have asserted that there exist automatic tools for doing that. 
And I was pointing out some corner cases. 
For the record, I don't believe that I, personally, made such an assertion. 
Nor did I intend to. 
I didn't read anyone else in this thread as doing so. 
IHMC (850)434 8903 or (650)494 3973 home 40 South Alcaniz St. (850)202 4416 office FL 32501 (850)291 0667 cell phayes@ihmc.us 
http://www.ihmc.us/users/phayes 
I'm new to this thread of discussion, so I don't know whether somebody already replied to this question. 
The question is the following. 
We all know that the problem of query answering cross ontology queries is a tough problem (in terms of devising a proper algorithm and in terms of data complexity). 
It is already very tough the problem of correctly answering queries by just having one ontology (even in OWL-LITE). 
Can you tell the details of the technology that you use to answer such queries (you mention deductive techniques)? 
Which theoretical results are you applying? 
Which are the assumptions on the query language, the ontology language, the mapping rules language and semantics, the nature of the data (OWA or CWA), sound or exact views over the data? 
If the query answering algorithm is not complete, which are the answers that it would miss, and which were the design decisions that led you to have an incomplete algorithm? 
cheers --e. 
Enrico Franconi - franconi@inf.unibz.it 
Free University of Bozen-Bolzano - http://www.inf.unibz.it/~franconi/ 
