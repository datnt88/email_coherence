In general, there hasn't been a major concern with preserving downlevel client interoperability given the rapid turnover of client software, and the small base of existing Web distributed authoring clients. 
In our web consulting practice, we deal primarily with Fortune 500 clients. 
Mostly large insurance companies, health care organizations and computer manufacturers. 
Believe it or not, Netscape 2.x series browsers is still a standard with some clients internally, who have deployed it on over 10,000+ workstations. 
Considering that the highest demand for distributed authoring environments will probably come frome large, complex organizations such as this, I don't feel it is a safe assumption that there is "rapid turnover of client software." 
(I'm sure the browser vendors will push for continuous turnover of browsers and will make claims that everyone is converting over rapidly, but I don't think this is the reality... why have WEBDAV technology take a year extra before installed bases can take advantage of it?) I think it is in the spirit of web browser technology to make it possible to deploy the solution via the web browser, and without the need for special client applications. 
This also preserves the ability of the Web to provide users the ability to perform their work from anywhere. 
At the very least, WEBDAV should include recommendations to browser vendors that permits them to support the extended HTTP protocols via HTML forms in a meaningful manner. 
Jon Document Management companies are doing a fine job of delivering web and java based applications that allow some kind of authoring and document management applications to run using standard web browsers. 
(There are dozens, I think; We don't need any additional standards work to allow such applications; as you say, they work with standard browsers. 
WEBDAV is trying to solve a different problem: a protocol level interface to sufficent capabilities for new versioning-based authoring applications to have direct access. 
Since we're looking for interoperability for a new set of functionality at the protocol level, we need standards for those new functions if we're really going to have interoperability. 
Larry Even if POST is used as a way of sending and processing version control specific commands, there is value in standardizing "that" special use of POST. 
This way different plug-ins or CGI bin scripts can provide version control services with existing HTTP servers. 
This should not be dismissed out of hand. 
The installed base of HTTP servers is quite large. 
Additionally, in the new effort, I would like to see a factorization of the DAV services such that they could be added incrementally and placed in different places. 
For example, in our system, we manage the shared state of documents on the server side and use an HTTP proxy to provide configuration/workspace management service. 
Others have used proxies for the same kind of service and also to support group annotations (a distributed authoring service I would think). 
This kind of factorization allows us to do a whole host of interesting things w.r.to version control and configuration management. 
I would like to see that the new protocol consider such architectural issues. 
This way we can scale up easily than the current protocol design seems to indicate. 
Adding methods to HTTP is the easy approach. 
We also need to keep in mind the other neat architectural features of HTTP 1.1 (proxies, gateways, and pipelines) and see how we can leverage them and deal with the problems introduced by such architectural features (e.g., how does one deal with caches kept by proxies). 
The recent discussion of dealing with security through an API does not address the problems introduced by these other architectural entities. 
ACLs or state based ACLs (as was suggested by somebody earlier) need to be in the protocol for proxies and gateways to deal with that information correctly. 
P.S: (newbie question) Has there been any discussion before on the range of services to be provided by the DAV effort and their inter-relationships. 
Is there a service model document other than the ones alluded to in the documents on the archive server. 
Don't we need something like this document in order to scope our effort? 
Sankar Virdhagriswaranp. 
no: 508 371 0404 I quite agree that updating web browsers in large organizations is not a small thing, but remember that only content developers would require browsers which support the new methods. 
The vast majority of corporate users would not require new or specialized browsers. 
Gregory Woodhouse gjw@wnetc.com 
/ http://www.wnetc.com/home.html 
If you're going to reinvent the wheel, at least try to come up with a better one. 
Depends on the user... 
In our Corporate Technical Memory, one explicit goal was that everyone should be an author. 
"Everyone's an expert at something!" Mark Leighton Fisher Thomson Consumer Electronics fisherm@indy.tce.com 
Indianapolis, IN "ViaCrypt? 
Vhy not!" 
As far as I can see, there is nothing within WEBDAV that can't be done already within the context of the HTTP server infrastructure. 
Therefore, I am at a loss as to why it is to anyone's advantage to produce a protocol level standard to provide the functionality. 
As you indicated, there are "dozens" of products that provide the functionality of remote document editing via the Web. 
Shouldn't we be concerned with producing standards for modularizing these capabilities and providing standard methods for making requests in to them while preserving the existing investment in underlying technology? 
I think any standard that simultaneously proposes to extend HTTP, require a rollout of a whole new generation of Web servers, and creates a new class of client application for such a narrow application as WEBDAV will fail as vendors continue to produce proprietary extensions in the form of server-API plug-ins (CGI, et al) and Java applets; customers are not willing to blindly leap to "standards" if it requires a significant change to the underlying infrastructure. 
In a very practical sense I think extensions to HTTP methods should be reserved for those things that are absolutely necessary - in other words, those things that just can't be done any other way. 
This could ultimately lead to having zillions of new HTTP method "standards" not just from WEBDAV but from all sorts of other standards efforts as well; some day you might have to go down a checklist of which HTTP methods your Web vendor supports. 
And of course, some vendors will start to make up their own methods. 
It just seems like there is an opportunity here to standardize on some robust, usable methods within the existing infrastructure and this talk of extending HTTP to support a whole new range of client apps and tools is like killing an ant with a sledgehammer. 
Some of the early discussion in this group appears to have been highly influenced by the Web server implementors who may see an advantage in doing it this way; I think for this to be a successful endeavor some of the ideas so far will need to be reevaluated with the input of other types of vendors as well as potential users. 
Jon You're right. 
I was tacitly assuming people followed an approach similar to ours. 
I suspect that the all or nothing approach to access control in many current environments ha been a big factor in restrivcting write access to the server. 
(Note that I'm talking as much of a policy/administrative limitation as a technical one). 
I suspect that a standardized method of access control would make the situation you describe (everyone an author) feasible (and probably desirable, but fo course, that isn't a technical issue). 
Gregory Woodhouse gjw@wnetc.com 
/ http://www.wnetc.com/home.html 
If you're going to reinvent the wheel, at least try to come up with a better one. 
We started here. 
Yes, FrontPage &amp; Navigator Gold are both shipping. 
However, they require different and incompatible server extensions. 
AOL Press had their own set. 
While everything COULD BE deployed with the current server infrastructure, there wasn't interoperability. 
The vendors wanted to create interoperability between different authoring tools and different back-ends, so thus we have a standards group to standardize those extensions. 
It seems they all currently require server extensions, and people install them, but of course you can't get your HTTP server to do something it doesn't do now ("versioning") without installing some software on it that actually implements the functionality. 
And this added functionality doesn't actually do you any good if you don't have clients that use it. 
However, the clients are there and implemented, and the servers are there and implemented, we just want some standards so that future releases of those clients and servers can be cross-functional. 
Yes, it will require deployment of new versions of some of those tools in order to get that to happen, but that's what always happens. 
I thought this was all in the charter. 
Maybe there's something in the latest drafts that doesn't do this the way you think it should be done? 
Larry This is an important part of the design, to separate out the different resources for 'control'. 
The IPP group seemed to run into this, in having a separate URLs for "job data submit", "job status" and "job cancel". 
I think we're moving along in this direction by using links, but perhaps there are some factorization that we've not addressed that you see in your application? 
Could you be more explicit about what needs to be factored? 
As soon as you allow that kind of factorization, you have to worry about access control which is currently realm-based. 
It might actually solve, rather than create, problems, though. 
Larry 
