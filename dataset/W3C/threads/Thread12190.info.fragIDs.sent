Hi, I am not sure where this goes in the techniques document exactly, but I thought I would outline some possiblities first: 1. Testing for accessibility: An author runs the "test accessibility" function on some content. 
There are some automatic checks done, to find things like different images with the same non-null alt-text (suspicious?), and a lack of any structure element beyond except paragraphs, suggesting a lack of proper structure use. 
The author is asked to confirm the text equivalents of each image as being sensible. 
The Tool also does an automatic test for reading level, but has no function for doing anything more. 
It records its own result. 
Each result is recorded as an EARL assertion. 
The author then edits the content, changing some of the text and images, and runs the test again. 
There are confirmations now for 90% of the text alternatives, recorded in EARL. 
So the author is only asked to confirm the remaining 10%. 
The reading level comes out as higher than before, and the author is notified of this. 
The tool also annotates the existing report of reading level with one that says the Tool is suspicious thatthe requirement has not been met. 
The author now opens the page in a different tool, which can help with some text editing. 
This tool runs an automatic process examining the styling properties, and suggests a potential structure model (based on size and weight of fonts, and positioning). 
The author confirms that this model is good, and the existing report of suspicious lack of structure is annotated with the updated report that the user has confirmed the structure model. 
The tool also runs a paragraph by paragraph check of the text. 
In each case it uses dictionary lookup and thesaurus substitution to suggest a replacement with a lower reading level requirement. 
The author agrees or disagrees at each paragraph level, and it is noted that the author has now made the judgement on a paragraph by paragraph basis. 
What are the specific techniques here? 
record the reading level of content at each check. 
If the level increases, a 
tool may record a suspicion that the relevant checkpoint is not met. 
If the 
author is asked explicitly, it may record that the author claims the checkpoint is met. 
For abetter implementation, only do this after providing 
some repair attempt. 
record author's satisfaction with text equivalents. 
Don't prompt the author 
to confirm these again if there is a recorded answer and the content has not 
changed. chaals 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) 
increases, a 
the 
providing 
author 
not 
I think the real technique is annotating or having the tool record that the author checked something and it should be remembered - using EARL as a language to record it. 
I'd call it "Annotating with EARL". 
In the example I would like to suggest examples that are more priority 1 and not controversial [more usability and less technical accessibility] ones like reading level. 
For example, using alt="" - null string for spacer and redundant images should be recorded as "valid" by the author. 
Another example would be tables used for layout and real data tables. 
The layout tables could be recorded as "layout" and not checked anymore for TH's, captions, or headers=. 
The tool could also record that the "layout table" was "linearized" and verified that it "looked" OK to the author. 
In other words all the P1's that require or could benefit from some human judgement annotations. 
Regards, Phill Jenkins, (512) 838-4517 IBM Research Division - Accessibility Center 11501 Burnet Rd, Austin TX 78758 http://www.ibm.com/able 
Well, the technique of recording an implementation is a further part of these techniques (and is part of meeting WCAG checkpoint 13.2 which applies in a few of the relative checkpoint cases), so I guess I should outline some techniques for it. 
I agree that it would be good to outline some other, less controversial checkpoint techniques too, but I was hoping that people could adapt what I have provided and do that - it seemed easy pickings. 
Herewith, a technique for recording an EARL evaluation in general: Record an EARL evaluation as a bookmark, by POSTing it to an annotea server. 
@@action Charles, provide a code sample for the POST body. 
This is something I need to check on, but it is basically a question of creating the right wrapper syntax around the EARL code. 
In an offline tool, or working offline, record the EARL locally as an annotation, and when the document is pubished, POST the EARL to an online server, adapting if necessary the URIs referred to. 
For example, a local annotations store might be kept in file:///user/charles/.earl 
(registering this in a system would be helpful to working offline with multiple tools, or publishing the location used by a tool would be helpful to allowing other tools to be compatible). 
Annotations might be made about file:///user/charles/documents/somesite/subarea/document1234567.xml in the process of editing this document. 
If the document is then published to stored locally to replace the local URI with the new one, and post the new version to an online server. 
Optionally, a tool may delete superseded EARL comments, or even do a complete merge, before publishing the EARL associated with a page. 
end example. 
Chaals 
increases, a 
the 
providing 
[snip] I think the real technique is annotating or having the tool record that the author checked something and it should be remembered - using EARL as a language to record it. 
I'd call it "Annotating with EARL". 
In the example I would like to suggest examples that are more priority 1 and not controversial [more usability and less technical accessibility] ones like reading level. 
For example, using alt="" - null string for spacer and redundant images should be recorded as "valid" by the author. 
Another example would be tables used for layout and real data tables. 
The layout tables could be recorded as "layout" and not checked anymore for TH's, captions, or headers=. 
The tool could also record that the "layout table" was "linearized" and verified that it "looked" OK to the author. 
In other words all the P1's that require or could benefit from some human judgement annotations. 
Regards, Phill Jenkins, (512) 838-4517 IBM Research Division - Accessibility Center 11501 Burnet Rd, Austin TX 78758 http://www.ibm.com/able 
Location: 21 Mitchell street FOOTSCRAY Vic 3011, Australia (or W3C INRIA, Route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex, France) 
