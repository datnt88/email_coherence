To get things moving for the proposed HTTP working group, I have set up a new mailing list (http-wg@cuckoo.hpl.hp.com) and added a few people (including yourself) who have shown interest in the working group. 
To unsubscribe send an e-mail messsage to: http-wg-request@cuckoo.hpl.hp.com 
I am writing to you to get your input on the proposed charter and workplan to be discussed at the HTTP Working Group BOF at WWWF'94 in Chicago. 
Early input will help to ensure that the BOF makes the best use of our limited time together, and I would like us to be able to get names against actions to start things rolling along. 
Our next chance to get together, after WWWF'94, will be the December IETF meeting in San Jose. 
Please email your comments on the proposal and thoughts as to how to get the best out of the WWWF'94 BOF to http-wg@cuckoo.hpl.hp.com or to me personally at dsr@hplb.hpl.hp.com 
Many thanks, Dave Raggett dsr@hplb.hpl.hp.com +44 272 228046 (United Kindom) Proposed Charter for the IETF Hypertext Transfer Protocol Working Group Right now HTTP is in a mess. 
The internet draft has expired and needs updating to bring into line with current practice. 
The performance is widely perceived to be poor, particularly for modem users, and various groups are working on disparate approaches for adding security and payment mechanisms. 
Left to itself, we will get fragmented de facto standards that inhibit interoperability. 
Even if one company wins out as the dominant supplier of servers and clients, it will act as a bottleneck for change with negative effects for end-users and value-adding niche suppliers. 
The role of W3O and the IETF should be to facilitate the development of open standards in which everyone can gain. 
To achieve this we will need to take advantage of the best work whether it is done at academic research centers or by commercial developers. 
Internet drafts and RFCs offer a route for nailing down an open framework for the protocol and for standard APIs for plug-in modules. 
This will facilitate interoperability by encouraging re-use of security modules rather than, for example, forcing every developer to separately negotiate with RSA for rights to use public key algorithms, or with the Department of Commerce for export licenses. 
Acting together, we can get better deals. 
Our suggested focus is on the short term. 
In particular, we want: a) to tap into the 30% of US homes with PCs/Macs and provide the incentives for them to connect to the Web b) to make it easy to pay for goods and services on the Web c) to protect the copyright interests of information providers To meet these objectives, we need to build on existing work and scale our action plan to what is feasible in the short term. 
Security and Electronic Payments The suggested timetable for this is to first concentrate on what is needed to securely send order details and receipts. 
Credit card payments can be authenticated offline by the credit card companies, but the next step is to provide support for authenticating servers, and subsequently clients. 
Basic authentication is possible using the IP address. 
Other mechanisms include Kerberos, and public key certificates. 
We shouldn't overlook encryption of arbitrary HTTP requests and responses. 
Smart cards have a bright future for payments based on credit/debit models and digital cash. 
In the short term, no one has card readers and we need to consider how to get things off the ground. 
In transitioning from here to there, we need to make it very simple for end-users and financial institutions. 
Some of the issues this raises include: how users register with an authentication server; if public key mechanisms are used, who generates the public key/secret key pairs and certificates; do the credit card companies store the certificate information as well; and are the transactions themselves are protected for both secrecy and integrity? 
Mosaic Communications, EIT, Spyglass and CERN all have different approaches to this! 
The working group will need to come up with an open framework that supports a range of different approaches. 
Could we agree on using a standard header to indicate which approach is in use, and to indicate acceptable alternatives? 
Can we agree on a high level set of security mechanisms and an API for implementing them with a range of cryptographic techniques? 
Is there any role here for the GSS API? 
Improved Performance It may prove worthwhile to extend MIME for use with an improved HTTP. 
Switching to a binary encoding of the protocol headers will not of its own give us the performance we desire, but many of the weak spots in the current protocol have been repeatedly discussed on the mailing lists. 
We would like to see one or more Internet Drafts covering: - MGET and multipart messages The ability to request several objects in the same request. 
The objects are then returned as a multipart message. 
- keep-alive and segmented transfers This gives us the ability to get an HTML file and then request the inlined images reusing the same connection. 
- encouraging deployment of transaction TCP Recent proposals cut out the slow start up times of conventional TCP protocol stacks. 
Can we coordinate our efforts to promote the widespread adoption of these extensions to TCP? - ways to avoid long lists of Accept headers and to better specify client capabilities Right now Mosaic sends out long lists of Accept headers which could easily be replaced by more compact identifiers for standard configurations. 
For home users with standard VGA and slow modems, it would be great if servers could take advantage of this to send more compact images. 
- consideration of an ASN.1 based format We need to look at the advantages of switching to a binary encoded format for protocol headers. 
Suggested Workplan October '94: We meet in Chicago and seek agreement that a common framework is needed for security and payment mechanisms, as well as brainstorming the problems/issues that the framework should address. 
We agree a numbering scheme for subsequent HTTP releases, and get interested people to sign-up to take an active role. 
November '94: Work starts on a revised Internet Draft covering HTTP as in current use. 
The http-wg mailing list may be appropriate for exchanging detailed comments on this document as it is written. 
We use the www-security mailing list to continue brainstorming ideas on the common security framework. 
One or more people nominated at the October BOF write this up as an initial draft. 
The objective for November is to finalize the charter and initial workplan for the IETF working group. 
The group uses the http-wg mailing list to work together on this document. 
December '94: IETF HTTP WG BOF - we present the charter and workplan. 
This meeting should be used to build the consensus and to look forward to the next set of actions and milestones. 
The work group is formally established, and people are signed up to write particular Internet Drafts. 
Spring '95: We present Internet Drafts for the revamped HTTP spec describing current practice; the framework for security; and for improved performance. 
This will coincide with the Internet Draft for HTML 3.0. 
WWW'95: Demonstrations of working implementations of these Internet Drafts. 
The HTTP working group starts looking at new issues such as the framework needed for digital cash, collaborative hypermedia, and scaling issues for information access and the implications for HTTP. 
IETF HTTP BOF in December I have reserved a slot at the December meeting of the IETF in San Jose. 
The Hypertext Transfer Protocol (http) BOF will be held on Tuesday, December 6: 1330-1530. 
Further info on IETF meetings is available from: http://www.ietf.cnri.reston.va.us/home.html 
Click on the link for "meetings" and you should find an entry for the San Jose meeting. 
Dave Raggett - 2nd October 1994 First off: thanks for taking the initiative to establish an HTTP working group. 
You've set a very ambitious timetable for getting things done. 
I have my doubts whether the timetable is feasible, but I'm more often a pessimist than an optimist. 
My comments appear below. 
Dave Kristol Bear in mind that the standards process usually ratifies existing practice. 
Therefore, updating the old Internet draft ought to be the first order of business. 
Addressing well-known problems, while important, should come afterward. 
[De facto, closed standards are a bad thing....] [W3O and IETF should facilitate open WWW standards.] 
[Use Internet drafts and RFCs to define open protocols and APIs.] 
I think this is possibly beyond our control. 
If there's interesting stuff, people will connect. 
Otherwise they won't. 
I agree we should have the technology for them to use the Web at acceptable speed and cost. 
Yes. 
Yes. 
But I doubt this can be solved in the short term. 
There are many ideas around, but I don't see any consensus on how to do this. 
IP address is dicey if you're trying to serve those folks with their Macs and PCs who get a new IP address each time they connect to their Internet provider. 
Some transactions will surely be protected. 
Otherwise an eavesdropper could capture for free what someone else bought. 
Don't forget privacy. 
I think it will be important for people to make requests anonymously and/or to feel comfortable that servers do not accumulate dossiers on their information buying habits. 
[Different security approaches being developed....] [Good proposals omitted.] 
The numbering scheme may be premature -- it may depend on who gets what done (and accepted by the community) first. 
My guess is that development will be breadth-first, which works against the usual numbering schemes. 
What I mean is this: after people agree on the state of current practice, folks will go off in different directions that, I hope, are largely orthogonal: performance improvement, security, payment. 
A linear numbering system won't accommodate that diversity well. 
You might have to say "HTTP 1.1 with performance improvements", or "HTTP 1.1 with security". 
Please please use www-buyinfo for discussions of commercial issues. 
(An interesting question is whether security and payment can be treated separately, or whether authentication connected to payment must be bundled with other kinds of authentication. 
I'm hoping for orthogonality, but I certainly haven't demonstrated it yet.) Yes. 
Yes. 
I would expect people to agree to the formation of a working group. 
Getting them to agree to a draft charter will perhaps be tougher. 
Who knows about a workplan? 
Describing current practice may be possible by Spring '95. 
The other two are less likely. 
It would be better to have developed working prototypes of security and improved performance features. 
Remember that the IETF expects working code in conjunction with paper specs. 
It will be hard to have both polished code and a polished draft ready in that timespan. 
(When is WWW '95? 
Where?) [Meeting placed on schedule.] 
Great! 
My reactions to Dave Kristol's comments: Agreed, although I think that we can work on problems in parallel. 
The goal is to unlock the business potential, the means to achieving this are asserted to be improving performance and simple payment mechanisms. 
I think there may be short term steps we should take, e.g. passing limited copyright information in the HTTP header, perhaps along with contractual restrictions on right to print/save local copies. 
A related idea is to allow publishers of "free" information to get some idea of how many people are accessing it via shared caches. 
Good point. 
None-the-less there is a need for a basic authentication mechanism in the absence of the infrastructure for a stronger solution. 
Agreed. 
The intent was to raise the distinction between secrecy and integrity. 
Can we do this in the short term? 
I am interested in exploiting the blinding techniques of David Chaum, but don't yet know enough to get a clear idea of how feasible it will be to support this widely on the Web in the short term. 
We agree a numbering scheme for subsequent HTTP releases, When we produce the revised Internet Draft that describes current practise we will almost certainly want to change the version number in some way. 
That would do for now! 
I was hoping that say HTTP 2.0 would support the performance improvements and a framework for plugging in security extensions in a modular way, e.g. HTTP 2.0 with Shen or HTTP 2.0 with digital cash. 
I was too prescriptive here. 
The intention was to keep the working group mailing list clear of rambling discussions that are better handled on a wider forum. 
You may be right, but I am hoping that we can build on existing work rather than having to start from scratch, e.g. EIT would write up how their modified S-HTTP proposal fits into the open framework, ditto for Spyglass and others. 
W3O would enhance the public domain libraries to demonstrate feasibility of the open framework approach, e.g. with a basic authentication module (Spyglass have volunteered to provide code for this) and a module for using Shen. 
Much of the work has already been done for handling multipart messages, and plans are in hand for work on reuse of transactions for follow-on requests. 
Best wishes, Dave Raggett United Kingdom Dave Raggett dsr@hplb.hpl.hp.com says: (   is Dave Raggett's original. 
are my comments on that. 
Okay. 
I was thinking of a different kind of copyright protection, namely a technological approach like document marking. 
(See I agree. 
My quibble was with the specific use of IP address, not a basic authentication mechanism. 
There are a couple of aspects: 1) Anonymous payment mechanisms help to preserve privacy: DigiCash, anonymous credit cards. 
I don't see how we can preserve privacy using a billing model for payment. 
2) Caching proxy servers help obscure identity (to the information service provider). 
3) I can imagine proxy TCP services that establish connections in a way analogous to the anonymous reEmailers in Finland. 
These would obscure the original requester. 
Note the effect that (2) and (3) have on IP address authentication! 
We agree a numbering scheme for subsequent HTTP releases, Umm. 
Let me be pedantic and note that digital cash is not a security extension (at least in my book) but a payment extension. 
That said, I agree that we should be working toward a framework that allows compatible extensions to HTTP. 
I support the use of existing work. 
I think you and I are agreeing that we want to define a framework in which all these things can co-exist. 
The WG would define the framework, not necessarily the specific extensions. 
David M. Kristol AT&amp;T Bell Laboratories Thanks for initiating this! 
My comments to this proposal are: It is not only GET - we also need a way to have multiple POST (and PUT). 
The reason for this is that a message is often to be posted to one or more mailing lists, one or more news groups and maybe a remote HTTP server. 
I have described how I would like the client interface to the Library of Common Code when building what I call a POST-Web at If the client is capable of talking directly to all the remote servers then this causes no problem for the HTTP protocol. 
However, if the POST request is going through a Proxy server, the current POST concept is inadequate. 
I think that MIME is an obvious tool to be considered to extend the HTTP protocol. 
I am currently testing my implementation of the multi-threaded version of the HTTP client in the Library of Common Code (The implementation is *platform independent* and does not require threads) When this is working then clients have a far more powerful tool to keep connection alive, not only for inlined images but also for HTTP sessions, video etc. 
In my opinion TTCP is a very nice way of enhancing TCP - I think we need something that is backwards compatible with TCP for a long time to come. 
YEP - why not use MIME types without sub-types? 
What is ASN.1??? - I agree that the protocol must turn into binary mode but I am not sure that this is the right time to do it. 
Maybe we can have an extension to the HTTP as TTCP is to the TCP - that is start binary - if failure then fall back to the text based HTTP? 
The workplan is _very_ ambitious - I think it is the right way to do it - so let's start :-) I am very interested in having an active role in the work! 
-- cheers -- Henrik Frystyk frystyk@dxcern.cern.ch 
+ 41 22 767 8265 World-Wide Web Project, CERN, CH-1211 Geneva 23, Switzerland It should be noted that one company's solution to the problem of time when loading html pages with lots of inlined images was to 1) grab the page 2) note the images needed for download 3) open up separate TCP connections for *each* image 4) find out the width and height of each image as it's coming down the pipe, laying out a box in which the image gets filled in as it arrives - thus allowing the page to be layed out perfectly before all images are received. 
We here would think that 1 x N is the same as N x 1, so opening 4 connections for 4 different things shouldn't be faster than one connection containing all elements, but aesthetically it is *much* more appealing. 
Could we get that same effect with one connection? 
Sure - the browser must be (simulated if not OS_supported) multithreaded, so it can be accepting data and rendering simultaneously, and when accessing inlined images a HEAD command should be sent for each image whose response shows how much screen acreage it'll take (which I *think* can be determined in the first couple of bytes of any GIF or JPEG). 
Brian There are 2 problems being discussed here: 1) how to minimize the number of TCP connections. 
2) how to help the client do the document layout before it has the images (or image sizes). 
I'd like to work on these independently. 
Problem 1 is pretty much obvious -- we need to work on it. 
Problem 2 can be solved with or with out Problem 1. 
I defintely do not like the *wasting* of network resources as is described above. 
I would like to discuss one or both of the following: a) extend HTML to include image size information for inline images. 
[This is a real pain for those of us with a lot of content and/or those of us who hand-code the HTML. 
But with the increasing availability of HTML authoring tool, this issue is less important.] 
[I realize that this suggestion does not necessarily belong within the domain of this mailing list, but it is a method of solving the problem and if adopted, we wouldn't need to do anything to HTTP....] b) have the server provide ''Image Size Hints'' in the document response header for all inlined images. 
This does not require any content to change, but does require some extra work from the server (eg, scanning HTML to find the images referenced and looking up their sizes (when local)). 
The nice thing about both of these is that if the information is absent (either not in the HTML or not provided by the server), then the client falls-back to the current method of document presentation. 
jeff 
